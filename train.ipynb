{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import keras_metrics # for recall and precision metrics\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 100 # the length of all sequences (number of words per sample)\n",
    "EMBEDDING_SIZE = 100  # Using 100-Dimensional GloVe embedding vectors\n",
    "TEST_SIZE = 0.25 # ratio of testing set\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 25 # number of epochs\n",
    "\n",
    "# to convert labels to integers and vice-versa\n",
    "label2int = {\"ham\": 0, \"spam\": 1}\n",
    "int2label = {0: \"ham\", 1: \"spam\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads SMS Spam Collection dataset\n",
    "    \"\"\"\n",
    "    texts, labels = [], []\n",
    "    with open(\"SMSSpamCollection\") as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            labels.append(split[0].strip())\n",
    "            texts.append(' '.join(split[1:]).strip())\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text tokenization\n",
    "# vectorizing text, turning each text into sequence of integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "# convert to sequence of integers\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "# pad sequences at the beginning of each sequence with 0's\n",
    "# for example if SEQUENCE_LENGTH=4:\n",
    "# [[5, 3, 2], [5, 1, 2, 3], [3, 4]]\n",
    "# will be transformed to:\n",
    "# [[0, 5, 3, 2], [5, 1, 2, 3], [0, 0, 3, 4]]\n",
    "X = pad_sequences(X, maxlen=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encoding labels\n",
    "# [spam, ham, spam, ham, ham] will be converted to:\n",
    "# [1, 0, 1, 0, 1] and then to:\n",
    "# [[0, 1], [1, 0], [0, 1], [1, 0], [0, 1]]\n",
    "\n",
    "y = [ label2int[label] for label in y ]\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and shuffle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vectors(tokenizer, dim=100):\n",
    "    embedding_index = {}\n",
    "    with open(f\"data/glove.6B.{dim}d.txt\", encoding='utf8') as f:\n",
    "        for line in tqdm.tqdm(f, \"Reading GloVe\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vectors = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_index[word] = vectors\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found will be 0s\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(tokenizer, lstm_units):\n",
    "    \"\"\"\n",
    "    Constructs the model,\n",
    "    Embedding vectors => LSTM => 2 output Fully-Connected neurons with softmax activation\n",
    "    \"\"\"\n",
    "    # get the GloVe embedding vectors\n",
    "    embedding_matrix = get_embedding_vectors(tokenizer)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(tokenizer.word_index)+1,\n",
    "              EMBEDDING_SIZE,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              input_length=SEQUENCE_LENGTH))\n",
    "\n",
    "    model.add(LSTM(lstm_units, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    # compile as rmsprop optimizer\n",
    "    # aswell as with recall metric\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\", keras_metrics.precision(), keras_metrics.recall()])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading GloVe: 400000it [00:30, 12949.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          901300    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 1,018,806\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 901,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# constructs the model with 128 LSTM units\n",
    "model = get_model(tokenizer=tokenizer, lstm_units=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (4180, 100)\n",
      "X_test.shape: (1394, 100)\n",
      "y_train.shape: (4180, 2)\n",
      "y_test.shape: (1394, 2)\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 4180 samples, validate on 1394 samples\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/25\n",
      "4180/4180 [==============================] - ETA: 1:43 - loss: 0.8105 - acc: 0.2188 - precision: 0.7778 - recall: 0.12 - ETA: 57s - loss: 0.5989 - acc: 0.5391 - precision: 0.8493 - recall: 0.5636 - ETA: 42s - loss: 0.5269 - acc: 0.6406 - precision: 0.8467 - recall: 0.707 - ETA: 34s - loss: 0.4829 - acc: 0.6914 - precision: 0.8458 - recall: 0.779 - ETA: 30s - loss: 0.4508 - acc: 0.7219 - precision: 0.8453 - recall: 0.823 - ETA: 26s - loss: 0.4220 - acc: 0.7500 - precision: 0.8537 - recall: 0.853 - ETA: 24s - loss: 0.3930 - acc: 0.7723 - precision: 0.8622 - recall: 0.875 - ETA: 22s - loss: 0.3721 - acc: 0.7871 - precision: 0.8659 - recall: 0.891 - ETA: 20s - loss: 0.3539 - acc: 0.8003 - precision: 0.8707 - recall: 0.903 - ETA: 18s - loss: 0.3480 - acc: 0.8141 - precision: 0.8772 - recall: 0.913 - ETA: 17s - loss: 0.3428 - acc: 0.8239 - precision: 0.8829 - recall: 0.917 - ETA: 16s - loss: 0.3354 - acc: 0.8268 - precision: 0.8807 - recall: 0.924 - ETA: 16s - loss: 0.3257 - acc: 0.8353 - precision: 0.8853 - recall: 0.928 - ETA: 15s - loss: 0.3207 - acc: 0.8415 - precision: 0.8889 - recall: 0.932 - ETA: 14s - loss: 0.3073 - acc: 0.8521 - precision: 0.8964 - recall: 0.937 - ETA: 14s - loss: 0.3007 - acc: 0.8574 - precision: 0.8991 - recall: 0.941 - ETA: 13s - loss: 0.2945 - acc: 0.8612 - precision: 0.9002 - recall: 0.945 - ETA: 12s - loss: 0.2856 - acc: 0.8672 - precision: 0.9041 - recall: 0.948 - ETA: 12s - loss: 0.2801 - acc: 0.8725 - precision: 0.9072 - recall: 0.950 - ETA: 11s - loss: 0.2773 - acc: 0.8719 - precision: 0.9061 - recall: 0.950 - ETA: 11s - loss: 0.2785 - acc: 0.8720 - precision: 0.9096 - recall: 0.945 - ETA: 10s - loss: 0.2738 - acc: 0.8757 - precision: 0.9115 - recall: 0.948 - ETA: 10s - loss: 0.2700 - acc: 0.8784 - precision: 0.9124 - recall: 0.950 - ETA: 9s - loss: 0.2637 - acc: 0.8828 - precision: 0.9158 - recall: 0.951 - ETA: 9s - loss: 0.2595 - acc: 0.8862 - precision: 0.9179 - recall: 0.95 - ETA: 9s - loss: 0.2561 - acc: 0.8888 - precision: 0.9198 - recall: 0.95 - ETA: 8s - loss: 0.2507 - acc: 0.8918 - precision: 0.9222 - recall: 0.95 - ETA: 8s - loss: 0.2471 - acc: 0.8934 - precision: 0.9236 - recall: 0.95 - ETA: 8s - loss: 0.2419 - acc: 0.8960 - precision: 0.9254 - recall: 0.95 - ETA: 7s - loss: 0.2368 - acc: 0.8984 - precision: 0.9276 - recall: 0.95 - ETA: 7s - loss: 0.2345 - acc: 0.8997 - precision: 0.9278 - recall: 0.95 - ETA: 7s - loss: 0.2321 - acc: 0.8999 - precision: 0.9293 - recall: 0.95 - ETA: 6s - loss: 0.2319 - acc: 0.9006 - precision: 0.9287 - recall: 0.95 - ETA: 6s - loss: 0.2299 - acc: 0.9026 - precision: 0.9302 - recall: 0.95 - ETA: 6s - loss: 0.2259 - acc: 0.9045 - precision: 0.9318 - recall: 0.95 - ETA: 6s - loss: 0.2238 - acc: 0.9058 - precision: 0.9322 - recall: 0.96 - ETA: 5s - loss: 0.2208 - acc: 0.9067 - precision: 0.9326 - recall: 0.96 - ETA: 5s - loss: 0.2177 - acc: 0.9083 - precision: 0.9339 - recall: 0.96 - ETA: 5s - loss: 0.2172 - acc: 0.9091 - precision: 0.9340 - recall: 0.96 - ETA: 5s - loss: 0.2164 - acc: 0.9098 - precision: 0.9356 - recall: 0.96 - ETA: 4s - loss: 0.2157 - acc: 0.9108 - precision: 0.9358 - recall: 0.96 - ETA: 4s - loss: 0.2116 - acc: 0.9129 - precision: 0.9373 - recall: 0.96 - ETA: 4s - loss: 0.2084 - acc: 0.9146 - precision: 0.9384 - recall: 0.96 - ETA: 4s - loss: 0.2092 - acc: 0.9151 - precision: 0.9384 - recall: 0.96 - ETA: 4s - loss: 0.2075 - acc: 0.9160 - precision: 0.9393 - recall: 0.96 - ETA: 3s - loss: 0.2050 - acc: 0.9171 - precision: 0.9398 - recall: 0.96 - ETA: 3s - loss: 0.2028 - acc: 0.9179 - precision: 0.9400 - recall: 0.96 - ETA: 3s - loss: 0.2013 - acc: 0.9189 - precision: 0.9413 - recall: 0.96 - ETA: 3s - loss: 0.2022 - acc: 0.9193 - precision: 0.9410 - recall: 0.96 - ETA: 3s - loss: 0.1991 - acc: 0.9209 - precision: 0.9423 - recall: 0.96 - ETA: 2s - loss: 0.1963 - acc: 0.9222 - precision: 0.9434 - recall: 0.96 - ETA: 2s - loss: 0.1941 - acc: 0.9234 - precision: 0.9444 - recall: 0.96 - ETA: 2s - loss: 0.1918 - acc: 0.9245 - precision: 0.9451 - recall: 0.96 - ETA: 2s - loss: 0.1914 - acc: 0.9245 - precision: 0.9445 - recall: 0.96 - ETA: 2s - loss: 0.1900 - acc: 0.9253 - precision: 0.9453 - recall: 0.96 - ETA: 1s - loss: 0.1876 - acc: 0.9263 - precision: 0.9459 - recall: 0.97 - ETA: 1s - loss: 0.1853 - acc: 0.9274 - precision: 0.9465 - recall: 0.97 - ETA: 1s - loss: 0.1832 - acc: 0.9283 - precision: 0.9475 - recall: 0.97 - ETA: 1s - loss: 0.1825 - acc: 0.9288 - precision: 0.9475 - recall: 0.97 - ETA: 1s - loss: 0.1813 - acc: 0.9292 - precision: 0.9483 - recall: 0.97 - ETA: 0s - loss: 0.1796 - acc: 0.9298 - precision: 0.9486 - recall: 0.97 - ETA: 0s - loss: 0.1781 - acc: 0.9304 - precision: 0.9494 - recall: 0.97 - ETA: 0s - loss: 0.1777 - acc: 0.9306 - precision: 0.9491 - recall: 0.97 - ETA: 0s - loss: 0.1758 - acc: 0.9316 - precision: 0.9499 - recall: 0.97 - ETA: 0s - loss: 0.1737 - acc: 0.9327 - precision: 0.9507 - recall: 0.97 - 14s 3ms/step - loss: 0.1729 - acc: 0.9330 - precision: 0.9510 - recall: 0.9730 - val_loss: 0.0997 - val_acc: 0.9677 - val_precision: 0.9729 - val_recall: 0.9900\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09973, saving model to results/spam_classifier_0.10\n",
      "Epoch 2/25\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 0.0391 - acc: 0.9844 - precision: 1.0000 - recall: 0.982 - ETA: 10s - loss: 0.0883 - acc: 0.9766 - precision: 0.9828 - recall: 0.991 - ETA: 10s - loss: 0.1008 - acc: 0.9792 - precision: 0.9826 - recall: 0.994 - ETA: 10s - loss: 0.0846 - acc: 0.9844 - precision: 0.9868 - recall: 0.995 - ETA: 11s - loss: 0.0875 - acc: 0.9812 - precision: 0.9862 - recall: 0.993 - ETA: 13s - loss: 0.0905 - acc: 0.9766 - precision: 0.9826 - recall: 0.991 - ETA: 13s - loss: 0.0959 - acc: 0.9732 - precision: 0.9777 - recall: 0.992 - ETA: 14s - loss: 0.0935 - acc: 0.9727 - precision: 0.9762 - recall: 0.993 - ETA: 13s - loss: 0.0953 - acc: 0.9705 - precision: 0.9785 - recall: 0.988 - ETA: 13s - loss: 0.0944 - acc: 0.9672 - precision: 0.9737 - recall: 0.989 - ETA: 12s - loss: 0.1012 - acc: 0.9631 - precision: 0.9713 - recall: 0.987 - ETA: 12s - loss: 0.0997 - acc: 0.9635 - precision: 0.9723 - recall: 0.986 - ETA: 11s - loss: 0.0999 - acc: 0.9651 - precision: 0.9733 - recall: 0.987 - ETA: 11s - loss: 0.1014 - acc: 0.9654 - precision: 0.9725 - recall: 0.988 - ETA: 11s - loss: 0.1009 - acc: 0.9656 - precision: 0.9742 - recall: 0.986 - ETA: 10s - loss: 0.0968 - acc: 0.9668 - precision: 0.9746 - recall: 0.987 - ETA: 10s - loss: 0.0965 - acc: 0.9669 - precision: 0.9761 - recall: 0.986 - ETA: 10s - loss: 0.0920 - acc: 0.9688 - precision: 0.9775 - recall: 0.987 - ETA: 9s - loss: 0.0936 - acc: 0.9688 - precision: 0.9769 - recall: 0.987 - ETA: 9s - loss: 0.0905 - acc: 0.9703 - precision: 0.9781 - recall: 0.98 - ETA: 9s - loss: 0.0890 - acc: 0.9710 - precision: 0.9783 - recall: 0.98 - ETA: 9s - loss: 0.0880 - acc: 0.9709 - precision: 0.9793 - recall: 0.98 - ETA: 9s - loss: 0.0897 - acc: 0.9701 - precision: 0.9787 - recall: 0.98 - ETA: 8s - loss: 0.0883 - acc: 0.9707 - precision: 0.9788 - recall: 0.98 - ETA: 8s - loss: 0.0872 - acc: 0.9706 - precision: 0.9783 - recall: 0.98 - ETA: 8s - loss: 0.0863 - acc: 0.9706 - precision: 0.9784 - recall: 0.98 - ETA: 8s - loss: 0.0871 - acc: 0.9711 - precision: 0.9786 - recall: 0.98 - ETA: 7s - loss: 0.0870 - acc: 0.9715 - precision: 0.9793 - recall: 0.98 - ETA: 7s - loss: 0.0882 - acc: 0.9709 - precision: 0.9783 - recall: 0.98 - ETA: 7s - loss: 0.0862 - acc: 0.9719 - precision: 0.9790 - recall: 0.98 - ETA: 7s - loss: 0.0844 - acc: 0.9728 - precision: 0.9797 - recall: 0.98 - ETA: 6s - loss: 0.0830 - acc: 0.9731 - precision: 0.9803 - recall: 0.98 - ETA: 6s - loss: 0.0949 - acc: 0.9711 - precision: 0.9778 - recall: 0.98 - ETA: 6s - loss: 0.0940 - acc: 0.9710 - precision: 0.9784 - recall: 0.98 - ETA: 6s - loss: 0.0945 - acc: 0.9705 - precision: 0.9780 - recall: 0.98 - ETA: 6s - loss: 0.0956 - acc: 0.9696 - precision: 0.9771 - recall: 0.98 - ETA: 5s - loss: 0.0941 - acc: 0.9700 - precision: 0.9773 - recall: 0.98 - ETA: 5s - loss: 0.0924 - acc: 0.9708 - precision: 0.9778 - recall: 0.98 - ETA: 5s - loss: 0.0930 - acc: 0.9700 - precision: 0.9775 - recall: 0.98 - ETA: 5s - loss: 0.0929 - acc: 0.9691 - precision: 0.9763 - recall: 0.98 - ETA: 5s - loss: 0.0925 - acc: 0.9691 - precision: 0.9769 - recall: 0.98 - ETA: 4s - loss: 0.0942 - acc: 0.9688 - precision: 0.9762 - recall: 0.98 - ETA: 4s - loss: 0.0933 - acc: 0.9691 - precision: 0.9763 - recall: 0.98 - ETA: 4s - loss: 0.0931 - acc: 0.9695 - precision: 0.9764 - recall: 0.98 - ETA: 4s - loss: 0.0943 - acc: 0.9684 - precision: 0.9757 - recall: 0.98 - ETA: 3s - loss: 0.0959 - acc: 0.9681 - precision: 0.9751 - recall: 0.98 - ETA: 3s - loss: 0.0975 - acc: 0.9678 - precision: 0.9756 - recall: 0.98 - ETA: 3s - loss: 0.0966 - acc: 0.9681 - precision: 0.9757 - recall: 0.98 - ETA: 3s - loss: 0.0977 - acc: 0.9678 - precision: 0.9755 - recall: 0.98 - ETA: 3s - loss: 0.0978 - acc: 0.9681 - precision: 0.9756 - recall: 0.98 - ETA: 2s - loss: 0.0967 - acc: 0.9684 - precision: 0.9760 - recall: 0.98 - ETA: 2s - loss: 0.0952 - acc: 0.9691 - precision: 0.9765 - recall: 0.98 - ETA: 2s - loss: 0.0943 - acc: 0.9690 - precision: 0.9766 - recall: 0.98 - ETA: 2s - loss: 0.0941 - acc: 0.9693 - precision: 0.9771 - recall: 0.98 - ETA: 2s - loss: 0.0961 - acc: 0.9682 - precision: 0.9759 - recall: 0.98 - ETA: 1s - loss: 0.0955 - acc: 0.9688 - precision: 0.9763 - recall: 0.98 - ETA: 1s - loss: 0.0946 - acc: 0.9693 - precision: 0.9767 - recall: 0.98 - ETA: 1s - loss: 0.0938 - acc: 0.9696 - precision: 0.9771 - recall: 0.98 - ETA: 1s - loss: 0.0939 - acc: 0.9690 - precision: 0.9766 - recall: 0.98 - ETA: 1s - loss: 0.0955 - acc: 0.9688 - precision: 0.9766 - recall: 0.98 - ETA: 0s - loss: 0.0958 - acc: 0.9685 - precision: 0.9761 - recall: 0.98 - ETA: 0s - loss: 0.0951 - acc: 0.9688 - precision: 0.9765 - recall: 0.98 - ETA: 0s - loss: 0.0943 - acc: 0.9690 - precision: 0.9766 - recall: 0.98 - ETA: 0s - loss: 0.0937 - acc: 0.9692 - precision: 0.9767 - recall: 0.98 - ETA: 0s - loss: 0.0937 - acc: 0.9692 - precision: 0.9765 - recall: 0.98 - 14s 3ms/step - loss: 0.0933 - acc: 0.9694 - precision: 0.9766 - recall: 0.9884 - val_loss: 0.0881 - val_acc: 0.9692 - val_precision: 0.9857 - val_recall: 0.9783\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09973 to 0.08809, saving model to results/spam_classifier_0.09\n",
      "Epoch 3/25\n",
      "4180/4180 [==============================] - ETA: 13s - loss: 0.0326 - acc: 0.9844 - precision: 1.0000 - recall: 0.981 - ETA: 12s - loss: 0.0531 - acc: 0.9766 - precision: 0.9907 - recall: 0.981 - ETA: 13s - loss: 0.0548 - acc: 0.9740 - precision: 0.9936 - recall: 0.975 - ETA: 13s - loss: 0.0486 - acc: 0.9805 - precision: 0.9953 - recall: 0.981 - ETA: 12s - loss: 0.0526 - acc: 0.9812 - precision: 0.9925 - recall: 0.985 - ETA: 12s - loss: 0.0521 - acc: 0.9792 - precision: 0.9905 - recall: 0.984 - ETA: 11s - loss: 0.0533 - acc: 0.9777 - precision: 0.9866 - recall: 0.986 - ETA: 11s - loss: 0.0539 - acc: 0.9785 - precision: 0.9883 - recall: 0.985 - ETA: 11s - loss: 0.0628 - acc: 0.9774 - precision: 0.9856 - recall: 0.987 - ETA: 10s - loss: 0.0659 - acc: 0.9781 - precision: 0.9852 - recall: 0.988 - ETA: 10s - loss: 0.0696 - acc: 0.9787 - precision: 0.9849 - recall: 0.989 - ETA: 10s - loss: 0.0675 - acc: 0.9792 - precision: 0.9846 - recall: 0.990 - ETA: 10s - loss: 0.0644 - acc: 0.9808 - precision: 0.9858 - recall: 0.991 - ETA: 10s - loss: 0.0624 - acc: 0.9810 - precision: 0.9855 - recall: 0.992 - ETA: 9s - loss: 0.0639 - acc: 0.9812 - precision: 0.9866 - recall: 0.991 - ETA: 9s - loss: 0.0616 - acc: 0.9814 - precision: 0.9863 - recall: 0.99 - ETA: 9s - loss: 0.0614 - acc: 0.9807 - precision: 0.9860 - recall: 0.99 - ETA: 9s - loss: 0.0637 - acc: 0.9792 - precision: 0.9838 - recall: 0.99 - ETA: 9s - loss: 0.0616 - acc: 0.9803 - precision: 0.9847 - recall: 0.99 - ETA: 8s - loss: 0.0595 - acc: 0.9805 - precision: 0.9845 - recall: 0.99 - ETA: 8s - loss: 0.0665 - acc: 0.9784 - precision: 0.9819 - recall: 0.99 - ETA: 8s - loss: 0.0761 - acc: 0.9737 - precision: 0.9826 - recall: 0.98 - ETA: 8s - loss: 0.0740 - acc: 0.9749 - precision: 0.9834 - recall: 0.98 - ETA: 8s - loss: 0.0741 - acc: 0.9753 - precision: 0.9833 - recall: 0.98 - ETA: 7s - loss: 0.0720 - acc: 0.9762 - precision: 0.9839 - recall: 0.98 - ETA: 7s - loss: 0.0733 - acc: 0.9754 - precision: 0.9832 - recall: 0.98 - ETA: 7s - loss: 0.0755 - acc: 0.9740 - precision: 0.9811 - recall: 0.98 - ETA: 7s - loss: 0.0757 - acc: 0.9732 - precision: 0.9818 - recall: 0.98 - ETA: 7s - loss: 0.0794 - acc: 0.9714 - precision: 0.9799 - recall: 0.98 - ETA: 7s - loss: 0.0816 - acc: 0.9708 - precision: 0.9799 - recall: 0.98 - ETA: 6s - loss: 0.0798 - acc: 0.9718 - precision: 0.9806 - recall: 0.98 - ETA: 6s - loss: 0.0797 - acc: 0.9712 - precision: 0.9801 - recall: 0.98 - ETA: 6s - loss: 0.0808 - acc: 0.9711 - precision: 0.9801 - recall: 0.98 - ETA: 6s - loss: 0.0793 - acc: 0.9720 - precision: 0.9807 - recall: 0.98 - ETA: 6s - loss: 0.0808 - acc: 0.9714 - precision: 0.9802 - recall: 0.98 - ETA: 5s - loss: 0.0794 - acc: 0.9722 - precision: 0.9808 - recall: 0.98 - ETA: 5s - loss: 0.0778 - acc: 0.9730 - precision: 0.9814 - recall: 0.98 - ETA: 5s - loss: 0.0765 - acc: 0.9733 - precision: 0.9818 - recall: 0.98 - ETA: 5s - loss: 0.0760 - acc: 0.9728 - precision: 0.9809 - recall: 0.98 - ETA: 5s - loss: 0.0757 - acc: 0.9723 - precision: 0.9814 - recall: 0.98 - ETA: 4s - loss: 0.0773 - acc: 0.9714 - precision: 0.9805 - recall: 0.98 - ETA: 4s - loss: 0.0757 - acc: 0.9721 - precision: 0.9811 - recall: 0.98 - ETA: 4s - loss: 0.0760 - acc: 0.9717 - precision: 0.9803 - recall: 0.98 - ETA: 4s - loss: 0.0749 - acc: 0.9723 - precision: 0.9807 - recall: 0.98 - ETA: 4s - loss: 0.0750 - acc: 0.9726 - precision: 0.9808 - recall: 0.98 - ETA: 3s - loss: 0.0744 - acc: 0.9725 - precision: 0.9804 - recall: 0.98 - ETA: 3s - loss: 0.0742 - acc: 0.9721 - precision: 0.9797 - recall: 0.98 - ETA: 3s - loss: 0.0825 - acc: 0.9694 - precision: 0.9794 - recall: 0.98 - ETA: 3s - loss: 0.0815 - acc: 0.9697 - precision: 0.9794 - recall: 0.98 - ETA: 3s - loss: 0.0802 - acc: 0.9703 - precision: 0.9799 - recall: 0.98 - ETA: 2s - loss: 0.0809 - acc: 0.9700 - precision: 0.9792 - recall: 0.98 - ETA: 2s - loss: 0.0799 - acc: 0.9706 - precision: 0.9797 - recall: 0.98 - ETA: 2s - loss: 0.0794 - acc: 0.9708 - precision: 0.9800 - recall: 0.98 - ETA: 2s - loss: 0.0804 - acc: 0.9708 - precision: 0.9798 - recall: 0.98 - ETA: 2s - loss: 0.0806 - acc: 0.9707 - precision: 0.9798 - recall: 0.98 - ETA: 1s - loss: 0.0803 - acc: 0.9707 - precision: 0.9799 - recall: 0.98 - ETA: 1s - loss: 0.0795 - acc: 0.9712 - precision: 0.9802 - recall: 0.98 - ETA: 1s - loss: 0.0790 - acc: 0.9714 - precision: 0.9806 - recall: 0.98 - ETA: 1s - loss: 0.0785 - acc: 0.9714 - precision: 0.9803 - recall: 0.98 - ETA: 1s - loss: 0.0775 - acc: 0.9719 - precision: 0.9806 - recall: 0.98 - ETA: 0s - loss: 0.0771 - acc: 0.9718 - precision: 0.9809 - recall: 0.98 - ETA: 0s - loss: 0.0781 - acc: 0.9720 - precision: 0.9810 - recall: 0.98 - ETA: 0s - loss: 0.0780 - acc: 0.9720 - precision: 0.9807 - recall: 0.98 - ETA: 0s - loss: 0.0777 - acc: 0.9719 - precision: 0.9810 - recall: 0.98 - ETA: 0s - loss: 0.0787 - acc: 0.9719 - precision: 0.9807 - recall: 0.98 - 14s 3ms/step - loss: 0.0787 - acc: 0.9718 - precision: 0.9806 - recall: 0.9870 - val_loss: 0.0730 - val_acc: 0.9749 - val_precision: 0.9834 - val_recall: 0.9875\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08809 to 0.07303, saving model to results/spam_classifier_0.07\n",
      "Epoch 4/25\n",
      "4180/4180 [==============================] - ETA: 8s - loss: 0.0393 - acc: 0.9844 - precision: 0.9815 - recall: 1.00 - ETA: 8s - loss: 0.0286 - acc: 0.9922 - precision: 0.9912 - recall: 1.00 - ETA: 8s - loss: 0.0303 - acc: 0.9896 - precision: 0.9881 - recall: 1.00 - ETA: 8s - loss: 0.0416 - acc: 0.9805 - precision: 0.9867 - recall: 0.99 - ETA: 8s - loss: 0.0573 - acc: 0.9750 - precision: 0.9786 - recall: 0.99 - ETA: 8s - loss: 0.0603 - acc: 0.9740 - precision: 0.9823 - recall: 0.98 - ETA: 8s - loss: 0.0573 - acc: 0.9754 - precision: 0.9848 - recall: 0.98 - ETA: 8s - loss: 0.0559 - acc: 0.9766 - precision: 0.9844 - recall: 0.98 - ETA: 8s - loss: 0.0675 - acc: 0.9757 - precision: 0.9841 - recall: 0.98 - ETA: 8s - loss: 0.0670 - acc: 0.9750 - precision: 0.9839 - recall: 0.98 - ETA: 8s - loss: 0.0659 - acc: 0.9759 - precision: 0.9837 - recall: 0.98 - ETA: 7s - loss: 0.0697 - acc: 0.9753 - precision: 0.9822 - recall: 0.98 - ETA: 7s - loss: 0.0691 - acc: 0.9760 - precision: 0.9835 - recall: 0.98 - ETA: 7s - loss: 0.0654 - acc: 0.9777 - precision: 0.9847 - recall: 0.98 - ETA: 7s - loss: 0.0660 - acc: 0.9771 - precision: 0.9844 - recall: 0.98 - ETA: 7s - loss: 0.0664 - acc: 0.9766 - precision: 0.9854 - recall: 0.98 - ETA: 7s - loss: 0.0656 - acc: 0.9761 - precision: 0.9842 - recall: 0.98 - ETA: 7s - loss: 0.0626 - acc: 0.9774 - precision: 0.9852 - recall: 0.98 - ETA: 6s - loss: 0.0601 - acc: 0.9786 - precision: 0.9860 - recall: 0.98 - ETA: 6s - loss: 0.0641 - acc: 0.9773 - precision: 0.9841 - recall: 0.99 - ETA: 6s - loss: 0.0626 - acc: 0.9784 - precision: 0.9848 - recall: 0.99 - ETA: 6s - loss: 0.0624 - acc: 0.9780 - precision: 0.9847 - recall: 0.99 - ETA: 6s - loss: 0.0613 - acc: 0.9783 - precision: 0.9853 - recall: 0.98 - ETA: 6s - loss: 0.0598 - acc: 0.9792 - precision: 0.9860 - recall: 0.99 - ETA: 6s - loss: 0.0585 - acc: 0.9800 - precision: 0.9864 - recall: 0.99 - ETA: 6s - loss: 0.0564 - acc: 0.9808 - precision: 0.9869 - recall: 0.99 - ETA: 6s - loss: 0.0563 - acc: 0.9803 - precision: 0.9861 - recall: 0.99 - ETA: 5s - loss: 0.0603 - acc: 0.9782 - precision: 0.9860 - recall: 0.98 - ETA: 5s - loss: 0.0586 - acc: 0.9790 - precision: 0.9865 - recall: 0.98 - ETA: 5s - loss: 0.0569 - acc: 0.9797 - precision: 0.9869 - recall: 0.98 - ETA: 5s - loss: 0.0563 - acc: 0.9798 - precision: 0.9873 - recall: 0.98 - ETA: 5s - loss: 0.0582 - acc: 0.9795 - precision: 0.9866 - recall: 0.98 - ETA: 5s - loss: 0.0574 - acc: 0.9796 - precision: 0.9870 - recall: 0.98 - ETA: 5s - loss: 0.0581 - acc: 0.9798 - precision: 0.9874 - recall: 0.98 - ETA: 4s - loss: 0.0567 - acc: 0.9804 - precision: 0.9878 - recall: 0.98 - ETA: 4s - loss: 0.0574 - acc: 0.9796 - precision: 0.9866 - recall: 0.99 - ETA: 4s - loss: 0.0574 - acc: 0.9797 - precision: 0.9870 - recall: 0.98 - ETA: 4s - loss: 0.0563 - acc: 0.9803 - precision: 0.9873 - recall: 0.99 - ETA: 4s - loss: 0.0554 - acc: 0.9808 - precision: 0.9877 - recall: 0.99 - ETA: 4s - loss: 0.0565 - acc: 0.9809 - precision: 0.9875 - recall: 0.99 - ETA: 3s - loss: 0.0557 - acc: 0.9813 - precision: 0.9878 - recall: 0.99 - ETA: 3s - loss: 0.0556 - acc: 0.9810 - precision: 0.9873 - recall: 0.99 - ETA: 3s - loss: 0.0547 - acc: 0.9815 - precision: 0.9876 - recall: 0.99 - ETA: 3s - loss: 0.0554 - acc: 0.9815 - precision: 0.9878 - recall: 0.99 - ETA: 3s - loss: 0.0565 - acc: 0.9812 - precision: 0.9873 - recall: 0.99 - ETA: 3s - loss: 0.0579 - acc: 0.9803 - precision: 0.9871 - recall: 0.99 - ETA: 2s - loss: 0.0590 - acc: 0.9801 - precision: 0.9866 - recall: 0.99 - ETA: 2s - loss: 0.0609 - acc: 0.9798 - precision: 0.9862 - recall: 0.99 - ETA: 2s - loss: 0.0605 - acc: 0.9799 - precision: 0.9865 - recall: 0.99 - ETA: 2s - loss: 0.0598 - acc: 0.9800 - precision: 0.9867 - recall: 0.99 - ETA: 2s - loss: 0.0589 - acc: 0.9804 - precision: 0.9870 - recall: 0.99 - ETA: 2s - loss: 0.0583 - acc: 0.9808 - precision: 0.9872 - recall: 0.99 - ETA: 2s - loss: 0.0581 - acc: 0.9805 - precision: 0.9868 - recall: 0.99 - ETA: 1s - loss: 0.0587 - acc: 0.9803 - precision: 0.9864 - recall: 0.99 - ETA: 1s - loss: 0.0583 - acc: 0.9804 - precision: 0.9864 - recall: 0.99 - ETA: 1s - loss: 0.0581 - acc: 0.9805 - precision: 0.9863 - recall: 0.99 - ETA: 1s - loss: 0.0577 - acc: 0.9805 - precision: 0.9862 - recall: 0.99 - ETA: 1s - loss: 0.0569 - acc: 0.9809 - precision: 0.9865 - recall: 0.99 - ETA: 1s - loss: 0.0581 - acc: 0.9807 - precision: 0.9861 - recall: 0.99 - ETA: 0s - loss: 0.0586 - acc: 0.9807 - precision: 0.9860 - recall: 0.99 - ETA: 0s - loss: 0.0583 - acc: 0.9810 - precision: 0.9863 - recall: 0.99 - ETA: 0s - loss: 0.0578 - acc: 0.9814 - precision: 0.9865 - recall: 0.99 - ETA: 0s - loss: 0.0585 - acc: 0.9814 - precision: 0.9864 - recall: 0.99 - ETA: 0s - loss: 0.0587 - acc: 0.9814 - precision: 0.9866 - recall: 0.99 - ETA: 0s - loss: 0.0587 - acc: 0.9815 - precision: 0.9865 - recall: 0.99 - 12s 3ms/step - loss: 0.0593 - acc: 0.9811 - precision: 0.9860 - recall: 0.9923 - val_loss: 0.1602 - val_acc: 0.9397 - val_precision: 0.9938 - val_recall: 0.9357\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.07303\n",
      "Epoch 5/25\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 0.2036 - acc: 0.9219 - precision: 0.9808 - recall: 0.927 - ETA: 10s - loss: 0.1699 - acc: 0.9375 - precision: 0.9732 - recall: 0.956 - ETA: 9s - loss: 0.1196 - acc: 0.9583 - precision: 0.9817 - recall: 0.969 - ETA: 9s - loss: 0.0977 - acc: 0.9648 - precision: 0.9863 - recall: 0.97 - ETA: 9s - loss: 0.1017 - acc: 0.9656 - precision: 0.9817 - recall: 0.97 - ETA: 9s - loss: 0.0882 - acc: 0.9714 - precision: 0.9847 - recall: 0.98 - ETA: 9s - loss: 0.0920 - acc: 0.9732 - precision: 0.9845 - recall: 0.98 - ETA: 8s - loss: 0.0878 - acc: 0.9727 - precision: 0.9819 - recall: 0.98 - ETA: 8s - loss: 0.0800 - acc: 0.9757 - precision: 0.9840 - recall: 0.98 - ETA: 8s - loss: 0.0763 - acc: 0.9766 - precision: 0.9855 - recall: 0.98 - ETA: 8s - loss: 0.0782 - acc: 0.9773 - precision: 0.9853 - recall: 0.98 - ETA: 8s - loss: 0.0752 - acc: 0.9779 - precision: 0.9851 - recall: 0.98 - ETA: 8s - loss: 0.0736 - acc: 0.9784 - precision: 0.9861 - recall: 0.98 - ETA: 7s - loss: 0.0700 - acc: 0.9799 - precision: 0.9870 - recall: 0.98 - ETA: 7s - loss: 0.0665 - acc: 0.9812 - precision: 0.9879 - recall: 0.99 - ETA: 7s - loss: 0.0666 - acc: 0.9805 - precision: 0.9875 - recall: 0.98 - ETA: 7s - loss: 0.0656 - acc: 0.9807 - precision: 0.9872 - recall: 0.99 - ETA: 7s - loss: 0.0652 - acc: 0.9809 - precision: 0.9879 - recall: 0.98 - ETA: 7s - loss: 0.0635 - acc: 0.9811 - precision: 0.9876 - recall: 0.99 - ETA: 6s - loss: 0.0630 - acc: 0.9812 - precision: 0.9883 - recall: 0.99 - ETA: 6s - loss: 0.0622 - acc: 0.9814 - precision: 0.9880 - recall: 0.99 - ETA: 6s - loss: 0.0603 - acc: 0.9815 - precision: 0.9886 - recall: 0.99 - ETA: 6s - loss: 0.0578 - acc: 0.9823 - precision: 0.9891 - recall: 0.99 - ETA: 6s - loss: 0.0599 - acc: 0.9811 - precision: 0.9873 - recall: 0.99 - ETA: 6s - loss: 0.0623 - acc: 0.9800 - precision: 0.9878 - recall: 0.98 - ETA: 6s - loss: 0.0635 - acc: 0.9790 - precision: 0.9862 - recall: 0.98 - ETA: 6s - loss: 0.0630 - acc: 0.9792 - precision: 0.9861 - recall: 0.99 - ETA: 5s - loss: 0.0612 - acc: 0.9799 - precision: 0.9866 - recall: 0.99 - ETA: 5s - loss: 0.0600 - acc: 0.9806 - precision: 0.9871 - recall: 0.99 - ETA: 5s - loss: 0.0629 - acc: 0.9802 - precision: 0.9863 - recall: 0.99 - ETA: 5s - loss: 0.0621 - acc: 0.9803 - precision: 0.9862 - recall: 0.99 - ETA: 5s - loss: 0.0617 - acc: 0.9805 - precision: 0.9860 - recall: 0.99 - ETA: 5s - loss: 0.0623 - acc: 0.9801 - precision: 0.9864 - recall: 0.99 - ETA: 5s - loss: 0.0634 - acc: 0.9802 - precision: 0.9863 - recall: 0.99 - ETA: 5s - loss: 0.0636 - acc: 0.9804 - precision: 0.9867 - recall: 0.99 - ETA: 4s - loss: 0.0621 - acc: 0.9809 - precision: 0.9871 - recall: 0.99 - ETA: 4s - loss: 0.0606 - acc: 0.9814 - precision: 0.9875 - recall: 0.99 - ETA: 4s - loss: 0.0592 - acc: 0.9819 - precision: 0.9878 - recall: 0.99 - ETA: 4s - loss: 0.0595 - acc: 0.9820 - precision: 0.9876 - recall: 0.99 - ETA: 4s - loss: 0.0601 - acc: 0.9812 - precision: 0.9866 - recall: 0.99 - ETA: 4s - loss: 0.0605 - acc: 0.9809 - precision: 0.9869 - recall: 0.99 - ETA: 4s - loss: 0.0597 - acc: 0.9814 - precision: 0.9872 - recall: 0.99 - ETA: 3s - loss: 0.0596 - acc: 0.9811 - precision: 0.9871 - recall: 0.99 - ETA: 3s - loss: 0.0584 - acc: 0.9815 - precision: 0.9874 - recall: 0.99 - ETA: 3s - loss: 0.0625 - acc: 0.9802 - precision: 0.9857 - recall: 0.99 - ETA: 3s - loss: 0.0626 - acc: 0.9803 - precision: 0.9860 - recall: 0.99 - ETA: 3s - loss: 0.0617 - acc: 0.9807 - precision: 0.9863 - recall: 0.99 - ETA: 3s - loss: 0.0609 - acc: 0.9811 - precision: 0.9866 - recall: 0.99 - ETA: 2s - loss: 0.0602 - acc: 0.9812 - precision: 0.9869 - recall: 0.99 - ETA: 2s - loss: 0.0606 - acc: 0.9812 - precision: 0.9868 - recall: 0.99 - ETA: 2s - loss: 0.0600 - acc: 0.9816 - precision: 0.9870 - recall: 0.99 - ETA: 2s - loss: 0.0596 - acc: 0.9817 - precision: 0.9873 - recall: 0.99 - ETA: 2s - loss: 0.0588 - acc: 0.9820 - precision: 0.9875 - recall: 0.99 - ETA: 2s - loss: 0.0581 - acc: 0.9823 - precision: 0.9877 - recall: 0.99 - ETA: 1s - loss: 0.0585 - acc: 0.9821 - precision: 0.9873 - recall: 0.99 - ETA: 1s - loss: 0.0593 - acc: 0.9816 - precision: 0.9875 - recall: 0.99 - ETA: 1s - loss: 0.0602 - acc: 0.9808 - precision: 0.9868 - recall: 0.99 - ETA: 1s - loss: 0.0595 - acc: 0.9811 - precision: 0.9870 - recall: 0.99 - ETA: 1s - loss: 0.0601 - acc: 0.9812 - precision: 0.9869 - recall: 0.99 - ETA: 0s - loss: 0.0595 - acc: 0.9815 - precision: 0.9872 - recall: 0.99 - ETA: 0s - loss: 0.0589 - acc: 0.9818 - precision: 0.9874 - recall: 0.99 - ETA: 0s - loss: 0.0584 - acc: 0.9821 - precision: 0.9876 - recall: 0.99 - ETA: 0s - loss: 0.0594 - acc: 0.9816 - precision: 0.9869 - recall: 0.99 - ETA: 0s - loss: 0.0597 - acc: 0.9814 - precision: 0.9871 - recall: 0.99 - ETA: 0s - loss: 0.0591 - acc: 0.9815 - precision: 0.9871 - recall: 0.99 - 13s 3ms/step - loss: 0.0592 - acc: 0.9813 - precision: 0.9868 - recall: 0.9917 - val_loss: 0.0703 - val_acc: 0.9735 - val_precision: 0.9882 - val_recall: 0.9808\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.07303 to 0.07027, saving model to results/spam_classifier_0.07\n",
      "Epoch 6/25\n",
      "4180/4180 [==============================] - ETA: 18s - loss: 0.0098 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 14s - loss: 0.0100 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 13s - loss: 0.0170 - acc: 0.9948 - precision: 0.9939 - recall: 1.000 - ETA: 12s - loss: 0.0320 - acc: 0.9922 - precision: 0.9953 - recall: 0.995 - ETA: 12s - loss: 0.0285 - acc: 0.9938 - precision: 0.9963 - recall: 0.996 - ETA: 11s - loss: 0.0351 - acc: 0.9922 - precision: 0.9938 - recall: 0.996 - ETA: 11s - loss: 0.0315 - acc: 0.9933 - precision: 0.9947 - recall: 0.997 - ETA: 11s - loss: 0.0329 - acc: 0.9922 - precision: 0.9953 - recall: 0.995 - ETA: 11s - loss: 0.0345 - acc: 0.9913 - precision: 0.9938 - recall: 0.995 - ETA: 11s - loss: 0.0332 - acc: 0.9922 - precision: 0.9944 - recall: 0.996 - ETA: 11s - loss: 0.0318 - acc: 0.9915 - precision: 0.9933 - recall: 0.996 - ETA: 10s - loss: 0.0392 - acc: 0.9883 - precision: 0.9924 - recall: 0.993 - ETA: 10s - loss: 0.0430 - acc: 0.9856 - precision: 0.9888 - recall: 0.994 - ETA: 10s - loss: 0.0406 - acc: 0.9866 - precision: 0.9896 - recall: 0.994 - ETA: 9s - loss: 0.0388 - acc: 0.9875 - precision: 0.9903 - recall: 0.995 - ETA: 9s - loss: 0.0424 - acc: 0.9873 - precision: 0.9897 - recall: 0.99 - ETA: 9s - loss: 0.0407 - acc: 0.9881 - precision: 0.9904 - recall: 0.99 - ETA: 9s - loss: 0.0399 - acc: 0.9878 - precision: 0.9899 - recall: 0.99 - ETA: 8s - loss: 0.0396 - acc: 0.9877 - precision: 0.9896 - recall: 0.99 - ETA: 8s - loss: 0.0422 - acc: 0.9867 - precision: 0.9883 - recall: 0.99 - ETA: 8s - loss: 0.0410 - acc: 0.9874 - precision: 0.9888 - recall: 0.99 - ETA: 8s - loss: 0.0404 - acc: 0.9879 - precision: 0.9893 - recall: 0.99 - ETA: 8s - loss: 0.0392 - acc: 0.9885 - precision: 0.9898 - recall: 0.99 - ETA: 7s - loss: 0.0440 - acc: 0.9870 - precision: 0.9880 - recall: 0.99 - ETA: 7s - loss: 0.0439 - acc: 0.9869 - precision: 0.9885 - recall: 0.99 - ETA: 7s - loss: 0.0458 - acc: 0.9862 - precision: 0.9876 - recall: 0.99 - ETA: 7s - loss: 0.0455 - acc: 0.9861 - precision: 0.9874 - recall: 0.99 - ETA: 7s - loss: 0.0446 - acc: 0.9866 - precision: 0.9878 - recall: 0.99 - ETA: 6s - loss: 0.0443 - acc: 0.9865 - precision: 0.9876 - recall: 0.99 - ETA: 6s - loss: 0.0434 - acc: 0.9870 - precision: 0.9880 - recall: 0.99 - ETA: 6s - loss: 0.0424 - acc: 0.9874 - precision: 0.9884 - recall: 0.99 - ETA: 6s - loss: 0.0420 - acc: 0.9873 - precision: 0.9888 - recall: 0.99 - ETA: 6s - loss: 0.0414 - acc: 0.9877 - precision: 0.9891 - recall: 0.99 - ETA: 5s - loss: 0.0404 - acc: 0.9881 - precision: 0.9894 - recall: 0.99 - ETA: 5s - loss: 0.0399 - acc: 0.9879 - precision: 0.9892 - recall: 0.99 - ETA: 5s - loss: 0.0398 - acc: 0.9878 - precision: 0.9895 - recall: 0.99 - ETA: 5s - loss: 0.0443 - acc: 0.9861 - precision: 0.9874 - recall: 0.99 - ETA: 5s - loss: 0.0438 - acc: 0.9860 - precision: 0.9877 - recall: 0.99 - ETA: 4s - loss: 0.0439 - acc: 0.9860 - precision: 0.9880 - recall: 0.99 - ETA: 4s - loss: 0.0431 - acc: 0.9863 - precision: 0.9883 - recall: 0.99 - ETA: 4s - loss: 0.0426 - acc: 0.9863 - precision: 0.9882 - recall: 0.99 - ETA: 4s - loss: 0.0420 - acc: 0.9866 - precision: 0.9885 - recall: 0.99 - ETA: 4s - loss: 0.0420 - acc: 0.9866 - precision: 0.9884 - recall: 0.99 - ETA: 3s - loss: 0.0429 - acc: 0.9865 - precision: 0.9882 - recall: 0.99 - ETA: 3s - loss: 0.0439 - acc: 0.9865 - precision: 0.9881 - recall: 0.99 - ETA: 3s - loss: 0.0434 - acc: 0.9868 - precision: 0.9884 - recall: 0.99 - ETA: 3s - loss: 0.0431 - acc: 0.9867 - precision: 0.9882 - recall: 0.99 - ETA: 3s - loss: 0.0426 - acc: 0.9870 - precision: 0.9885 - recall: 0.99 - ETA: 3s - loss: 0.0428 - acc: 0.9869 - precision: 0.9887 - recall: 0.99 - ETA: 2s - loss: 0.0423 - acc: 0.9872 - precision: 0.9889 - recall: 0.99 - ETA: 2s - loss: 0.0431 - acc: 0.9868 - precision: 0.9885 - recall: 0.99 - ETA: 2s - loss: 0.0427 - acc: 0.9871 - precision: 0.9887 - recall: 0.99 - ETA: 2s - loss: 0.0444 - acc: 0.9867 - precision: 0.9882 - recall: 0.99 - ETA: 2s - loss: 0.0441 - acc: 0.9870 - precision: 0.9884 - recall: 0.99 - ETA: 1s - loss: 0.0446 - acc: 0.9869 - precision: 0.9883 - recall: 0.99 - ETA: 1s - loss: 0.0441 - acc: 0.9872 - precision: 0.9885 - recall: 0.99 - ETA: 1s - loss: 0.0439 - acc: 0.9871 - precision: 0.9887 - recall: 0.99 - ETA: 1s - loss: 0.0443 - acc: 0.9871 - precision: 0.9886 - recall: 0.99 - ETA: 1s - loss: 0.0438 - acc: 0.9873 - precision: 0.9888 - recall: 0.99 - ETA: 0s - loss: 0.0443 - acc: 0.9872 - precision: 0.9887 - recall: 0.99 - ETA: 0s - loss: 0.0437 - acc: 0.9874 - precision: 0.9889 - recall: 0.99 - ETA: 0s - loss: 0.0455 - acc: 0.9871 - precision: 0.9888 - recall: 0.99 - ETA: 0s - loss: 0.0450 - acc: 0.9874 - precision: 0.9890 - recall: 0.99 - ETA: 0s - loss: 0.0449 - acc: 0.9873 - precision: 0.9889 - recall: 0.99 - ETA: 0s - loss: 0.0451 - acc: 0.9873 - precision: 0.9890 - recall: 0.99 - 13s 3ms/step - loss: 0.0461 - acc: 0.9871 - precision: 0.9888 - recall: 0.9964 - val_loss: 0.0869 - val_acc: 0.9684 - val_precision: 0.9661 - val_recall: 0.9983\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07027\n",
      "Epoch 7/25\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 0.0770 - acc: 0.9688 - precision: 0.9630 - recall: 1.000 - ETA: 9s - loss: 0.0566 - acc: 0.9844 - precision: 0.9818 - recall: 1.000 - ETA: 10s - loss: 0.0421 - acc: 0.9896 - precision: 0.9882 - recall: 1.000 - ETA: 9s - loss: 0.0345 - acc: 0.9922 - precision: 0.9910 - recall: 1.000 - ETA: 9s - loss: 0.0304 - acc: 0.9938 - precision: 0.9927 - recall: 1.00 - ETA: 9s - loss: 0.0403 - acc: 0.9922 - precision: 0.9910 - recall: 1.00 - ETA: 9s - loss: 0.0387 - acc: 0.9911 - precision: 0.9898 - recall: 1.00 - ETA: 8s - loss: 0.0419 - acc: 0.9902 - precision: 0.9888 - recall: 1.00 - ETA: 8s - loss: 0.0412 - acc: 0.9896 - precision: 0.9900 - recall: 0.99 - ETA: 8s - loss: 0.0379 - acc: 0.9906 - precision: 0.9909 - recall: 0.99 - ETA: 8s - loss: 0.0348 - acc: 0.9915 - precision: 0.9918 - recall: 0.99 - ETA: 8s - loss: 0.0347 - acc: 0.9909 - precision: 0.9909 - recall: 0.99 - ETA: 8s - loss: 0.0335 - acc: 0.9916 - precision: 0.9916 - recall: 0.99 - ETA: 7s - loss: 0.0319 - acc: 0.9922 - precision: 0.9922 - recall: 0.99 - ETA: 7s - loss: 0.0306 - acc: 0.9927 - precision: 0.9928 - recall: 0.99 - ETA: 7s - loss: 0.0337 - acc: 0.9912 - precision: 0.9910 - recall: 0.99 - ETA: 7s - loss: 0.0379 - acc: 0.9890 - precision: 0.9894 - recall: 0.99 - ETA: 7s - loss: 0.0367 - acc: 0.9896 - precision: 0.9900 - recall: 0.99 - ETA: 7s - loss: 0.0367 - acc: 0.9893 - precision: 0.9906 - recall: 0.99 - ETA: 6s - loss: 0.0396 - acc: 0.9891 - precision: 0.9901 - recall: 0.99 - ETA: 6s - loss: 0.0383 - acc: 0.9896 - precision: 0.9906 - recall: 0.99 - ETA: 6s - loss: 0.0368 - acc: 0.9901 - precision: 0.9910 - recall: 0.99 - ETA: 6s - loss: 0.0355 - acc: 0.9905 - precision: 0.9915 - recall: 0.99 - ETA: 6s - loss: 0.0343 - acc: 0.9909 - precision: 0.9918 - recall: 0.99 - ETA: 6s - loss: 0.0332 - acc: 0.9912 - precision: 0.9922 - recall: 0.99 - ETA: 5s - loss: 0.0330 - acc: 0.9910 - precision: 0.9918 - recall: 0.99 - ETA: 5s - loss: 0.0412 - acc: 0.9890 - precision: 0.9921 - recall: 0.99 - ETA: 5s - loss: 0.0462 - acc: 0.9872 - precision: 0.9898 - recall: 0.99 - ETA: 5s - loss: 0.0472 - acc: 0.9865 - precision: 0.9895 - recall: 0.99 - ETA: 5s - loss: 0.0462 - acc: 0.9870 - precision: 0.9899 - recall: 0.99 - ETA: 5s - loss: 0.0452 - acc: 0.9874 - precision: 0.9902 - recall: 0.99 - ETA: 4s - loss: 0.0440 - acc: 0.9878 - precision: 0.9905 - recall: 0.99 - ETA: 4s - loss: 0.0432 - acc: 0.9882 - precision: 0.9908 - recall: 0.99 - ETA: 4s - loss: 0.0422 - acc: 0.9885 - precision: 0.9910 - recall: 0.99 - ETA: 4s - loss: 0.0415 - acc: 0.9888 - precision: 0.9913 - recall: 0.99 - ETA: 4s - loss: 0.0412 - acc: 0.9887 - precision: 0.9910 - recall: 0.99 - ETA: 4s - loss: 0.0410 - acc: 0.9886 - precision: 0.9908 - recall: 0.99 - ETA: 4s - loss: 0.0404 - acc: 0.9889 - precision: 0.9910 - recall: 0.99 - ETA: 3s - loss: 0.0400 - acc: 0.9888 - precision: 0.9908 - recall: 0.99 - ETA: 3s - loss: 0.0413 - acc: 0.9887 - precision: 0.9906 - recall: 0.99 - ETA: 3s - loss: 0.0407 - acc: 0.9889 - precision: 0.9908 - recall: 0.99 - ETA: 3s - loss: 0.0404 - acc: 0.9892 - precision: 0.9910 - recall: 0.99 - ETA: 3s - loss: 0.0420 - acc: 0.9884 - precision: 0.9904 - recall: 0.99 - ETA: 3s - loss: 0.0417 - acc: 0.9883 - precision: 0.9902 - recall: 0.99 - ETA: 3s - loss: 0.0411 - acc: 0.9885 - precision: 0.9904 - recall: 0.99 - ETA: 3s - loss: 0.0405 - acc: 0.9888 - precision: 0.9906 - recall: 0.99 - ETA: 2s - loss: 0.0401 - acc: 0.9890 - precision: 0.9908 - recall: 0.99 - ETA: 2s - loss: 0.0421 - acc: 0.9886 - precision: 0.9903 - recall: 0.99 - ETA: 2s - loss: 0.0422 - acc: 0.9885 - precision: 0.9905 - recall: 0.99 - ETA: 2s - loss: 0.0431 - acc: 0.9884 - precision: 0.9903 - recall: 0.99 - ETA: 2s - loss: 0.0433 - acc: 0.9881 - precision: 0.9901 - recall: 0.99 - ETA: 2s - loss: 0.0434 - acc: 0.9880 - precision: 0.9903 - recall: 0.99 - ETA: 1s - loss: 0.0428 - acc: 0.9882 - precision: 0.9905 - recall: 0.99 - ETA: 1s - loss: 0.0433 - acc: 0.9881 - precision: 0.9904 - recall: 0.99 - ETA: 1s - loss: 0.0428 - acc: 0.9884 - precision: 0.9906 - recall: 0.99 - ETA: 1s - loss: 0.0427 - acc: 0.9883 - precision: 0.9904 - recall: 0.99 - ETA: 1s - loss: 0.0422 - acc: 0.9885 - precision: 0.9906 - recall: 0.99 - ETA: 1s - loss: 0.0422 - acc: 0.9884 - precision: 0.9904 - recall: 0.99 - ETA: 0s - loss: 0.0432 - acc: 0.9881 - precision: 0.9900 - recall: 0.99 - ETA: 0s - loss: 0.0431 - acc: 0.9880 - precision: 0.9902 - recall: 0.99 - ETA: 0s - loss: 0.0427 - acc: 0.9882 - precision: 0.9903 - recall: 0.99 - ETA: 0s - loss: 0.0425 - acc: 0.9882 - precision: 0.9905 - recall: 0.99 - ETA: 0s - loss: 0.0426 - acc: 0.9881 - precision: 0.9904 - recall: 0.99 - ETA: 0s - loss: 0.0424 - acc: 0.9880 - precision: 0.9902 - recall: 0.99 - ETA: 0s - loss: 0.0420 - acc: 0.9882 - precision: 0.9904 - recall: 0.99 - 11s 3ms/step - loss: 0.0420 - acc: 0.9883 - precision: 0.9904 - recall: 0.9961 - val_loss: 0.0757 - val_acc: 0.9699 - val_precision: 0.9722 - val_recall: 0.9933\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.07027\n",
      "Epoch 8/25\n",
      "4180/4180 [==============================] - ETA: 12s - loss: 0.0058 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0954 - acc: 0.9766 - precision: 0.9725 - recall: 1.000 - ETA: 11s - loss: 0.0760 - acc: 0.9792 - precision: 0.9817 - recall: 0.993 - ETA: 10s - loss: 0.0594 - acc: 0.9844 - precision: 0.9864 - recall: 0.995 - ETA: 10s - loss: 0.0553 - acc: 0.9812 - precision: 0.9892 - recall: 0.989 - ETA: 9s - loss: 0.0582 - acc: 0.9792 - precision: 0.9851 - recall: 0.991 - ETA: 9s - loss: 0.0538 - acc: 0.9799 - precision: 0.9872 - recall: 0.98 - ETA: 9s - loss: 0.0480 - acc: 0.9824 - precision: 0.9889 - recall: 0.99 - ETA: 8s - loss: 0.0504 - acc: 0.9826 - precision: 0.9882 - recall: 0.99 - ETA: 8s - loss: 0.0488 - acc: 0.9828 - precision: 0.9877 - recall: 0.99 - ETA: 8s - loss: 0.0465 - acc: 0.9844 - precision: 0.9889 - recall: 0.99 - ETA: 8s - loss: 0.0436 - acc: 0.9857 - precision: 0.9898 - recall: 0.99 - ETA: 8s - loss: 0.0419 - acc: 0.9856 - precision: 0.9892 - recall: 0.99 - ETA: 7s - loss: 0.0400 - acc: 0.9866 - precision: 0.9900 - recall: 0.99 - ETA: 7s - loss: 0.0410 - acc: 0.9865 - precision: 0.9907 - recall: 0.99 - ETA: 7s - loss: 0.0387 - acc: 0.9873 - precision: 0.9912 - recall: 0.99 - ETA: 7s - loss: 0.0421 - acc: 0.9862 - precision: 0.9897 - recall: 0.99 - ETA: 7s - loss: 0.0412 - acc: 0.9861 - precision: 0.9893 - recall: 0.99 - ETA: 7s - loss: 0.0398 - acc: 0.9868 - precision: 0.9899 - recall: 0.99 - ETA: 7s - loss: 0.0392 - acc: 0.9867 - precision: 0.9904 - recall: 0.99 - ETA: 6s - loss: 0.0449 - acc: 0.9851 - precision: 0.9883 - recall: 0.99 - ETA: 6s - loss: 0.0439 - acc: 0.9851 - precision: 0.9889 - recall: 0.99 - ETA: 6s - loss: 0.0421 - acc: 0.9857 - precision: 0.9894 - recall: 0.99 - ETA: 6s - loss: 0.0439 - acc: 0.9850 - precision: 0.9884 - recall: 0.99 - ETA: 6s - loss: 0.0425 - acc: 0.9856 - precision: 0.9888 - recall: 0.99 - ETA: 6s - loss: 0.0415 - acc: 0.9862 - precision: 0.9892 - recall: 0.99 - ETA: 6s - loss: 0.0438 - acc: 0.9861 - precision: 0.9890 - recall: 0.99 - ETA: 6s - loss: 0.0433 - acc: 0.9860 - precision: 0.9887 - recall: 0.99 - ETA: 6s - loss: 0.0469 - acc: 0.9844 - precision: 0.9891 - recall: 0.99 - ETA: 5s - loss: 0.0461 - acc: 0.9844 - precision: 0.9888 - recall: 0.99 - ETA: 5s - loss: 0.0454 - acc: 0.9844 - precision: 0.9886 - recall: 0.99 - ETA: 5s - loss: 0.0467 - acc: 0.9844 - precision: 0.9885 - recall: 0.99 - ETA: 5s - loss: 0.0456 - acc: 0.9848 - precision: 0.9888 - recall: 0.99 - ETA: 5s - loss: 0.0446 - acc: 0.9853 - precision: 0.9891 - recall: 0.99 - ETA: 5s - loss: 0.0441 - acc: 0.9857 - precision: 0.9894 - recall: 0.99 - ETA: 4s - loss: 0.0431 - acc: 0.9861 - precision: 0.9897 - recall: 0.99 - ETA: 4s - loss: 0.0427 - acc: 0.9861 - precision: 0.9895 - recall: 0.99 - ETA: 4s - loss: 0.0418 - acc: 0.9864 - precision: 0.9898 - recall: 0.99 - ETA: 4s - loss: 0.0421 - acc: 0.9860 - precision: 0.9892 - recall: 0.99 - ETA: 4s - loss: 0.0421 - acc: 0.9859 - precision: 0.9890 - recall: 0.99 - ETA: 4s - loss: 0.0413 - acc: 0.9863 - precision: 0.9892 - recall: 0.99 - ETA: 4s - loss: 0.0406 - acc: 0.9866 - precision: 0.9895 - recall: 0.99 - ETA: 3s - loss: 0.0407 - acc: 0.9866 - precision: 0.9894 - recall: 0.99 - ETA: 3s - loss: 0.0406 - acc: 0.9865 - precision: 0.9896 - recall: 0.99 - ETA: 3s - loss: 0.0411 - acc: 0.9865 - precision: 0.9894 - recall: 0.99 - ETA: 3s - loss: 0.0407 - acc: 0.9868 - precision: 0.9896 - recall: 0.99 - ETA: 3s - loss: 0.0417 - acc: 0.9864 - precision: 0.9891 - recall: 0.99 - ETA: 3s - loss: 0.0415 - acc: 0.9863 - precision: 0.9889 - recall: 0.99 - ETA: 2s - loss: 0.0417 - acc: 0.9863 - precision: 0.9888 - recall: 0.99 - ETA: 2s - loss: 0.0413 - acc: 0.9866 - precision: 0.9890 - recall: 0.99 - ETA: 2s - loss: 0.0408 - acc: 0.9868 - precision: 0.9892 - recall: 0.99 - ETA: 2s - loss: 0.0403 - acc: 0.9871 - precision: 0.9894 - recall: 0.99 - ETA: 2s - loss: 0.0408 - acc: 0.9867 - precision: 0.9889 - recall: 0.99 - ETA: 2s - loss: 0.0408 - acc: 0.9867 - precision: 0.9891 - recall: 0.99 - ETA: 1s - loss: 0.0413 - acc: 0.9866 - precision: 0.9890 - recall: 0.99 - ETA: 1s - loss: 0.0409 - acc: 0.9869 - precision: 0.9892 - recall: 0.99 - ETA: 1s - loss: 0.0406 - acc: 0.9871 - precision: 0.9894 - recall: 0.99 - ETA: 1s - loss: 0.0400 - acc: 0.9873 - precision: 0.9895 - recall: 0.99 - ETA: 1s - loss: 0.0416 - acc: 0.9868 - precision: 0.9888 - recall: 0.99 - ETA: 0s - loss: 0.0411 - acc: 0.9870 - precision: 0.9890 - recall: 0.99 - ETA: 0s - loss: 0.0407 - acc: 0.9872 - precision: 0.9892 - recall: 0.99 - ETA: 0s - loss: 0.0408 - acc: 0.9869 - precision: 0.9893 - recall: 0.99 - ETA: 0s - loss: 0.0403 - acc: 0.9871 - precision: 0.9895 - recall: 0.99 - ETA: 0s - loss: 0.0398 - acc: 0.9873 - precision: 0.9897 - recall: 0.99 - ETA: 0s - loss: 0.0396 - acc: 0.9873 - precision: 0.9895 - recall: 0.99 - 13s 3ms/step - loss: 0.0403 - acc: 0.9871 - precision: 0.9896 - recall: 0.9956 - val_loss: 0.1182 - val_acc: 0.9627 - val_precision: 0.9584 - val_recall: 1.0000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.07027\n",
      "Epoch 9/25\n",
      "4180/4180 [==============================] - ETA: 20s - loss: 0.0123 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 16s - loss: 0.0937 - acc: 0.9766 - precision: 0.9735 - recall: 1.000 - ETA: 14s - loss: 0.0746 - acc: 0.9792 - precision: 0.9765 - recall: 1.000 - ETA: 13s - loss: 0.0609 - acc: 0.9844 - precision: 0.9825 - recall: 1.000 - ETA: 12s - loss: 0.0591 - acc: 0.9844 - precision: 0.9827 - recall: 1.000 - ETA: 12s - loss: 0.0632 - acc: 0.9844 - precision: 0.9827 - recall: 1.000 - ETA: 12s - loss: 0.0559 - acc: 0.9866 - precision: 0.9851 - recall: 1.000 - ETA: 12s - loss: 0.0505 - acc: 0.9883 - precision: 0.9869 - recall: 1.000 - ETA: 12s - loss: 0.0504 - acc: 0.9861 - precision: 0.9844 - recall: 1.000 - ETA: 11s - loss: 0.0471 - acc: 0.9875 - precision: 0.9859 - recall: 1.000 - ETA: 11s - loss: 0.0441 - acc: 0.9886 - precision: 0.9871 - recall: 1.000 - ETA: 11s - loss: 0.0417 - acc: 0.9896 - precision: 0.9882 - recall: 1.000 - ETA: 11s - loss: 0.0443 - acc: 0.9892 - precision: 0.9878 - recall: 1.000 - ETA: 10s - loss: 0.0422 - acc: 0.9900 - precision: 0.9887 - recall: 1.000 - ETA: 10s - loss: 0.0404 - acc: 0.9906 - precision: 0.9894 - recall: 1.000 - ETA: 10s - loss: 0.0395 - acc: 0.9902 - precision: 0.9889 - recall: 1.000 - ETA: 10s - loss: 0.0377 - acc: 0.9908 - precision: 0.9896 - recall: 1.000 - ETA: 9s - loss: 0.0362 - acc: 0.9913 - precision: 0.9901 - recall: 1.000 - ETA: 9s - loss: 0.0355 - acc: 0.9910 - precision: 0.9897 - recall: 1.00 - ETA: 9s - loss: 0.0353 - acc: 0.9906 - precision: 0.9902 - recall: 0.99 - ETA: 9s - loss: 0.0415 - acc: 0.9896 - precision: 0.9890 - recall: 0.99 - ETA: 8s - loss: 0.0412 - acc: 0.9893 - precision: 0.9887 - recall: 0.99 - ETA: 8s - loss: 0.0401 - acc: 0.9898 - precision: 0.9892 - recall: 0.99 - ETA: 8s - loss: 0.0395 - acc: 0.9896 - precision: 0.9889 - recall: 0.99 - ETA: 7s - loss: 0.0383 - acc: 0.9900 - precision: 0.9893 - recall: 0.99 - ETA: 7s - loss: 0.0382 - acc: 0.9898 - precision: 0.9890 - recall: 0.99 - ETA: 7s - loss: 0.0383 - acc: 0.9890 - precision: 0.9894 - recall: 0.99 - ETA: 7s - loss: 0.0374 - acc: 0.9894 - precision: 0.9898 - recall: 0.99 - ETA: 7s - loss: 0.0377 - acc: 0.9892 - precision: 0.9896 - recall: 0.99 - ETA: 6s - loss: 0.0368 - acc: 0.9896 - precision: 0.9899 - recall: 0.99 - ETA: 6s - loss: 0.0358 - acc: 0.9899 - precision: 0.9902 - recall: 0.99 - ETA: 6s - loss: 0.0350 - acc: 0.9902 - precision: 0.9905 - recall: 0.99 - ETA: 6s - loss: 0.0341 - acc: 0.9905 - precision: 0.9908 - recall: 0.99 - ETA: 6s - loss: 0.0334 - acc: 0.9908 - precision: 0.9911 - recall: 0.99 - ETA: 5s - loss: 0.0371 - acc: 0.9902 - precision: 0.9903 - recall: 0.99 - ETA: 5s - loss: 0.0366 - acc: 0.9905 - precision: 0.9906 - recall: 0.99 - ETA: 5s - loss: 0.0361 - acc: 0.9907 - precision: 0.9908 - recall: 0.99 - ETA: 5s - loss: 0.0353 - acc: 0.9910 - precision: 0.9911 - recall: 0.99 - ETA: 5s - loss: 0.0347 - acc: 0.9912 - precision: 0.9913 - recall: 0.99 - ETA: 4s - loss: 0.0340 - acc: 0.9914 - precision: 0.9915 - recall: 0.99 - ETA: 4s - loss: 0.0334 - acc: 0.9916 - precision: 0.9917 - recall: 0.99 - ETA: 4s - loss: 0.0342 - acc: 0.9911 - precision: 0.9911 - recall: 0.99 - ETA: 4s - loss: 0.0348 - acc: 0.9909 - precision: 0.9909 - recall: 0.99 - ETA: 4s - loss: 0.0359 - acc: 0.9904 - precision: 0.9911 - recall: 0.99 - ETA: 3s - loss: 0.0351 - acc: 0.9906 - precision: 0.9913 - recall: 0.99 - ETA: 3s - loss: 0.0363 - acc: 0.9901 - precision: 0.9907 - recall: 0.99 - ETA: 3s - loss: 0.0356 - acc: 0.9904 - precision: 0.9909 - recall: 0.99 - ETA: 3s - loss: 0.0350 - acc: 0.9906 - precision: 0.9911 - recall: 0.99 - ETA: 3s - loss: 0.0343 - acc: 0.9908 - precision: 0.9912 - recall: 0.99 - ETA: 2s - loss: 0.0356 - acc: 0.9903 - precision: 0.9907 - recall: 0.99 - ETA: 2s - loss: 0.0367 - acc: 0.9899 - precision: 0.9902 - recall: 0.99 - ETA: 2s - loss: 0.0365 - acc: 0.9901 - precision: 0.9904 - recall: 0.99 - ETA: 2s - loss: 0.0359 - acc: 0.9903 - precision: 0.9906 - recall: 0.99 - ETA: 2s - loss: 0.0357 - acc: 0.9902 - precision: 0.9904 - recall: 0.99 - ETA: 1s - loss: 0.0364 - acc: 0.9901 - precision: 0.9903 - recall: 0.99 - ETA: 1s - loss: 0.0360 - acc: 0.9902 - precision: 0.9904 - recall: 0.99 - ETA: 1s - loss: 0.0356 - acc: 0.9904 - precision: 0.9906 - recall: 0.99 - ETA: 1s - loss: 0.0353 - acc: 0.9906 - precision: 0.9908 - recall: 0.99 - ETA: 1s - loss: 0.0357 - acc: 0.9905 - precision: 0.9906 - recall: 0.99 - ETA: 1s - loss: 0.0355 - acc: 0.9904 - precision: 0.9905 - recall: 0.99 - ETA: 0s - loss: 0.0350 - acc: 0.9905 - precision: 0.9906 - recall: 0.99 - ETA: 0s - loss: 0.0344 - acc: 0.9907 - precision: 0.9908 - recall: 0.99 - ETA: 0s - loss: 0.0341 - acc: 0.9908 - precision: 0.9909 - recall: 0.99 - ETA: 0s - loss: 0.0336 - acc: 0.9910 - precision: 0.9911 - recall: 0.99 - ETA: 0s - loss: 0.0339 - acc: 0.9909 - precision: 0.9912 - recall: 0.99 - 13s 3ms/step - loss: 0.0337 - acc: 0.9909 - precision: 0.9912 - recall: 0.9983 - val_loss: 0.1017 - val_acc: 0.9699 - val_precision: 0.9676 - val_recall: 0.9983\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.07027\n",
      "Epoch 10/25\n",
      "4180/4180 [==============================] - ETA: 15s - loss: 0.0436 - acc: 0.9844 - precision: 0.9815 - recall: 1.000 - ETA: 13s - loss: 0.0245 - acc: 0.9922 - precision: 0.9908 - recall: 1.000 - ETA: 12s - loss: 0.0302 - acc: 0.9896 - precision: 0.9877 - recall: 1.000 - ETA: 12s - loss: 0.0245 - acc: 0.9922 - precision: 0.9910 - recall: 1.000 - ETA: 12s - loss: 0.0242 - acc: 0.9906 - precision: 0.9892 - recall: 1.000 - ETA: 11s - loss: 0.0222 - acc: 0.9922 - precision: 0.9910 - recall: 1.000 - ETA: 11s - loss: 0.0217 - acc: 0.9933 - precision: 0.9923 - recall: 1.000 - ETA: 11s - loss: 0.0236 - acc: 0.9922 - precision: 0.9910 - recall: 1.000 - ETA: 11s - loss: 0.0263 - acc: 0.9913 - precision: 0.9900 - recall: 1.000 - ETA: 10s - loss: 0.0257 - acc: 0.9922 - precision: 0.9910 - recall: 1.000 - ETA: 10s - loss: 0.0240 - acc: 0.9929 - precision: 0.9919 - recall: 1.000 - ETA: 10s - loss: 0.0243 - acc: 0.9922 - precision: 0.9910 - recall: 1.000 - ETA: 10s - loss: 0.0298 - acc: 0.9904 - precision: 0.9903 - recall: 0.998 - ETA: 10s - loss: 0.0284 - acc: 0.9911 - precision: 0.9909 - recall: 0.998 - ETA: 9s - loss: 0.0292 - acc: 0.9906 - precision: 0.9903 - recall: 0.998 - ETA: 9s - loss: 0.0278 - acc: 0.9912 - precision: 0.9909 - recall: 0.99 - ETA: 9s - loss: 0.0306 - acc: 0.9908 - precision: 0.9904 - recall: 0.99 - ETA: 9s - loss: 0.0295 - acc: 0.9913 - precision: 0.9909 - recall: 0.99 - ETA: 9s - loss: 0.0283 - acc: 0.9918 - precision: 0.9914 - recall: 0.99 - ETA: 9s - loss: 0.0280 - acc: 0.9922 - precision: 0.9918 - recall: 0.99 - ETA: 8s - loss: 0.0278 - acc: 0.9926 - precision: 0.9922 - recall: 0.99 - ETA: 8s - loss: 0.0273 - acc: 0.9929 - precision: 0.9926 - recall: 0.99 - ETA: 8s - loss: 0.0264 - acc: 0.9932 - precision: 0.9929 - recall: 0.99 - ETA: 8s - loss: 0.0254 - acc: 0.9935 - precision: 0.9932 - recall: 0.99 - ETA: 8s - loss: 0.0273 - acc: 0.9931 - precision: 0.9928 - recall: 0.99 - ETA: 7s - loss: 0.0274 - acc: 0.9928 - precision: 0.9931 - recall: 0.99 - ETA: 7s - loss: 0.0287 - acc: 0.9925 - precision: 0.9927 - recall: 0.99 - ETA: 7s - loss: 0.0294 - acc: 0.9922 - precision: 0.9923 - recall: 0.99 - ETA: 7s - loss: 0.0287 - acc: 0.9925 - precision: 0.9926 - recall: 0.99 - ETA: 6s - loss: 0.0280 - acc: 0.9927 - precision: 0.9928 - recall: 0.99 - ETA: 6s - loss: 0.0274 - acc: 0.9929 - precision: 0.9931 - recall: 0.99 - ETA: 6s - loss: 0.0267 - acc: 0.9932 - precision: 0.9933 - recall: 0.99 - ETA: 6s - loss: 0.0264 - acc: 0.9929 - precision: 0.9929 - recall: 0.99 - ETA: 6s - loss: 0.0261 - acc: 0.9926 - precision: 0.9931 - recall: 0.99 - ETA: 5s - loss: 0.0254 - acc: 0.9929 - precision: 0.9934 - recall: 0.99 - ETA: 5s - loss: 0.0247 - acc: 0.9931 - precision: 0.9935 - recall: 0.99 - ETA: 5s - loss: 0.0255 - acc: 0.9924 - precision: 0.9928 - recall: 0.99 - ETA: 5s - loss: 0.0314 - acc: 0.9905 - precision: 0.9924 - recall: 0.99 - ETA: 5s - loss: 0.0330 - acc: 0.9896 - precision: 0.9913 - recall: 0.99 - ETA: 4s - loss: 0.0326 - acc: 0.9898 - precision: 0.9915 - recall: 0.99 - ETA: 4s - loss: 0.0332 - acc: 0.9897 - precision: 0.9913 - recall: 0.99 - ETA: 4s - loss: 0.0331 - acc: 0.9900 - precision: 0.9915 - recall: 0.99 - ETA: 4s - loss: 0.0329 - acc: 0.9902 - precision: 0.9917 - recall: 0.99 - ETA: 4s - loss: 0.0332 - acc: 0.9901 - precision: 0.9919 - recall: 0.99 - ETA: 3s - loss: 0.0347 - acc: 0.9899 - precision: 0.9917 - recall: 0.99 - ETA: 3s - loss: 0.0340 - acc: 0.9901 - precision: 0.9918 - recall: 0.99 - ETA: 3s - loss: 0.0340 - acc: 0.9900 - precision: 0.9916 - recall: 0.99 - ETA: 3s - loss: 0.0338 - acc: 0.9899 - precision: 0.9918 - recall: 0.99 - ETA: 3s - loss: 0.0333 - acc: 0.9901 - precision: 0.9920 - recall: 0.99 - ETA: 3s - loss: 0.0327 - acc: 0.9903 - precision: 0.9921 - recall: 0.99 - ETA: 2s - loss: 0.0322 - acc: 0.9905 - precision: 0.9923 - recall: 0.99 - ETA: 2s - loss: 0.0335 - acc: 0.9904 - precision: 0.9921 - recall: 0.99 - ETA: 2s - loss: 0.0332 - acc: 0.9903 - precision: 0.9919 - recall: 0.99 - ETA: 2s - loss: 0.0327 - acc: 0.9905 - precision: 0.9920 - recall: 0.99 - ETA: 2s - loss: 0.0324 - acc: 0.9906 - precision: 0.9922 - recall: 0.99 - ETA: 1s - loss: 0.0320 - acc: 0.9908 - precision: 0.9923 - recall: 0.99 - ETA: 1s - loss: 0.0332 - acc: 0.9907 - precision: 0.9921 - recall: 0.99 - ETA: 1s - loss: 0.0330 - acc: 0.9906 - precision: 0.9920 - recall: 0.99 - ETA: 1s - loss: 0.0328 - acc: 0.9907 - precision: 0.9921 - recall: 0.99 - ETA: 1s - loss: 0.0323 - acc: 0.9909 - precision: 0.9922 - recall: 0.99 - ETA: 0s - loss: 0.0323 - acc: 0.9910 - precision: 0.9924 - recall: 0.99 - ETA: 0s - loss: 0.0319 - acc: 0.9912 - precision: 0.9925 - recall: 0.99 - ETA: 0s - loss: 0.0322 - acc: 0.9911 - precision: 0.9923 - recall: 0.99 - ETA: 0s - loss: 0.0318 - acc: 0.9912 - precision: 0.9924 - recall: 0.99 - ETA: 0s - loss: 0.0314 - acc: 0.9913 - precision: 0.9926 - recall: 0.99 - 14s 3ms/step - loss: 0.0313 - acc: 0.9914 - precision: 0.9926 - recall: 0.9975 - val_loss: 0.0756 - val_acc: 0.9785 - val_precision: 0.9795 - val_recall: 0.9958\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07027\n",
      "Epoch 11/25\n",
      "4180/4180 [==============================] - ETA: 12s - loss: 0.0067 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0059 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0050 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0231 - acc: 0.9961 - precision: 0.9954 - recall: 1.000 - ETA: 11s - loss: 0.0240 - acc: 0.9969 - precision: 0.9963 - recall: 1.000 - ETA: 11s - loss: 0.0206 - acc: 0.9974 - precision: 0.9970 - recall: 1.000 - ETA: 11s - loss: 0.0181 - acc: 0.9978 - precision: 0.9974 - recall: 1.000 - ETA: 11s - loss: 0.0164 - acc: 0.9980 - precision: 0.9977 - recall: 1.000 - ETA: 11s - loss: 0.0167 - acc: 0.9965 - precision: 0.9959 - recall: 1.000 - ETA: 10s - loss: 0.0162 - acc: 0.9969 - precision: 0.9964 - recall: 1.000 - ETA: 10s - loss: 0.0151 - acc: 0.9972 - precision: 0.9967 - recall: 1.000 - ETA: 10s - loss: 0.0140 - acc: 0.9974 - precision: 0.9970 - recall: 1.000 - ETA: 9s - loss: 0.0147 - acc: 0.9964 - precision: 0.9958 - recall: 1.000 - ETA: 9s - loss: 0.0139 - acc: 0.9967 - precision: 0.9961 - recall: 1.00 - ETA: 9s - loss: 0.0167 - acc: 0.9958 - precision: 0.9964 - recall: 0.99 - ETA: 8s - loss: 0.0162 - acc: 0.9961 - precision: 0.9966 - recall: 0.99 - ETA: 8s - loss: 0.0167 - acc: 0.9954 - precision: 0.9958 - recall: 0.99 - ETA: 8s - loss: 0.0168 - acc: 0.9957 - precision: 0.9960 - recall: 0.99 - ETA: 8s - loss: 0.0199 - acc: 0.9951 - precision: 0.9953 - recall: 0.99 - ETA: 8s - loss: 0.0190 - acc: 0.9953 - precision: 0.9955 - recall: 0.99 - ETA: 8s - loss: 0.0183 - acc: 0.9955 - precision: 0.9957 - recall: 0.99 - ETA: 7s - loss: 0.0176 - acc: 0.9957 - precision: 0.9959 - recall: 0.99 - ETA: 7s - loss: 0.0172 - acc: 0.9959 - precision: 0.9961 - recall: 0.99 - ETA: 7s - loss: 0.0167 - acc: 0.9961 - precision: 0.9963 - recall: 0.99 - ETA: 7s - loss: 0.0161 - acc: 0.9962 - precision: 0.9964 - recall: 0.99 - ETA: 7s - loss: 0.0157 - acc: 0.9964 - precision: 0.9965 - recall: 0.99 - ETA: 6s - loss: 0.0161 - acc: 0.9959 - precision: 0.9960 - recall: 0.99 - ETA: 6s - loss: 0.0157 - acc: 0.9961 - precision: 0.9962 - recall: 0.99 - ETA: 6s - loss: 0.0155 - acc: 0.9962 - precision: 0.9963 - recall: 0.99 - ETA: 6s - loss: 0.0151 - acc: 0.9964 - precision: 0.9964 - recall: 0.99 - ETA: 5s - loss: 0.0147 - acc: 0.9965 - precision: 0.9965 - recall: 0.99 - ETA: 5s - loss: 0.0147 - acc: 0.9961 - precision: 0.9961 - recall: 0.99 - ETA: 5s - loss: 0.0169 - acc: 0.9948 - precision: 0.9951 - recall: 0.99 - ETA: 5s - loss: 0.0187 - acc: 0.9945 - precision: 0.9947 - recall: 0.99 - ETA: 5s - loss: 0.0185 - acc: 0.9946 - precision: 0.9948 - recall: 0.99 - ETA: 5s - loss: 0.0192 - acc: 0.9944 - precision: 0.9945 - recall: 0.99 - ETA: 4s - loss: 0.0188 - acc: 0.9945 - precision: 0.9946 - recall: 0.99 - ETA: 4s - loss: 0.0186 - acc: 0.9947 - precision: 0.9948 - recall: 0.99 - ETA: 4s - loss: 0.0182 - acc: 0.9948 - precision: 0.9949 - recall: 0.99 - ETA: 4s - loss: 0.0178 - acc: 0.9949 - precision: 0.9950 - recall: 0.99 - ETA: 4s - loss: 0.0174 - acc: 0.9950 - precision: 0.9952 - recall: 0.99 - ETA: 3s - loss: 0.0175 - acc: 0.9948 - precision: 0.9949 - recall: 0.99 - ETA: 3s - loss: 0.0179 - acc: 0.9945 - precision: 0.9945 - recall: 0.99 - ETA: 3s - loss: 0.0177 - acc: 0.9947 - precision: 0.9947 - recall: 0.99 - ETA: 3s - loss: 0.0176 - acc: 0.9948 - precision: 0.9948 - recall: 0.99 - ETA: 3s - loss: 0.0174 - acc: 0.9949 - precision: 0.9949 - recall: 0.99 - ETA: 3s - loss: 0.0174 - acc: 0.9950 - precision: 0.9950 - recall: 0.99 - ETA: 2s - loss: 0.0172 - acc: 0.9951 - precision: 0.9951 - recall: 0.99 - ETA: 2s - loss: 0.0183 - acc: 0.9949 - precision: 0.9949 - recall: 0.99 - ETA: 2s - loss: 0.0186 - acc: 0.9947 - precision: 0.9950 - recall: 0.99 - ETA: 2s - loss: 0.0183 - acc: 0.9948 - precision: 0.9951 - recall: 0.99 - ETA: 2s - loss: 0.0179 - acc: 0.9949 - precision: 0.9952 - recall: 0.99 - ETA: 2s - loss: 0.0177 - acc: 0.9950 - precision: 0.9953 - recall: 0.99 - ETA: 1s - loss: 0.0178 - acc: 0.9948 - precision: 0.9950 - recall: 0.99 - ETA: 1s - loss: 0.0193 - acc: 0.9940 - precision: 0.9948 - recall: 0.99 - ETA: 1s - loss: 0.0210 - acc: 0.9933 - precision: 0.9939 - recall: 0.99 - ETA: 1s - loss: 0.0209 - acc: 0.9934 - precision: 0.9940 - recall: 0.99 - ETA: 1s - loss: 0.0206 - acc: 0.9935 - precision: 0.9941 - recall: 0.99 - ETA: 1s - loss: 0.0204 - acc: 0.9936 - precision: 0.9942 - recall: 0.99 - ETA: 0s - loss: 0.0203 - acc: 0.9938 - precision: 0.9943 - recall: 0.99 - ETA: 0s - loss: 0.0201 - acc: 0.9939 - precision: 0.9944 - recall: 0.99 - ETA: 0s - loss: 0.0202 - acc: 0.9937 - precision: 0.9942 - recall: 0.99 - ETA: 0s - loss: 0.0200 - acc: 0.9938 - precision: 0.9943 - recall: 0.99 - ETA: 0s - loss: 0.0197 - acc: 0.9939 - precision: 0.9944 - recall: 0.99 - ETA: 0s - loss: 0.0210 - acc: 0.9938 - precision: 0.9942 - recall: 0.99 - 12s 3ms/step - loss: 0.0209 - acc: 0.9938 - precision: 0.9942 - recall: 0.9986 - val_loss: 0.0609 - val_acc: 0.9813 - val_precision: 0.9900 - val_recall: 0.9883\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.07027 to 0.06094, saving model to results/spam_classifier_0.06\n",
      "Epoch 12/25\n",
      "4180/4180 [==============================] - ETA: 14s - loss: 0.0076 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0060 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0049 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0115 - acc: 0.9961 - precision: 0.9955 - recall: 1.000 - ETA: 12s - loss: 0.0106 - acc: 0.9969 - precision: 0.9964 - recall: 1.000 - ETA: 12s - loss: 0.0252 - acc: 0.9922 - precision: 0.9969 - recall: 0.993 - ETA: 12s - loss: 0.0220 - acc: 0.9933 - precision: 0.9974 - recall: 0.994 - ETA: 12s - loss: 0.0265 - acc: 0.9922 - precision: 0.9954 - recall: 0.995 - ETA: 12s - loss: 0.0246 - acc: 0.9931 - precision: 0.9959 - recall: 0.995 - ETA: 11s - loss: 0.0226 - acc: 0.9938 - precision: 0.9963 - recall: 0.996 - ETA: 11s - loss: 0.0216 - acc: 0.9943 - precision: 0.9966 - recall: 0.996 - ETA: 11s - loss: 0.0211 - acc: 0.9948 - precision: 0.9969 - recall: 0.996 - ETA: 10s - loss: 0.0198 - acc: 0.9952 - precision: 0.9972 - recall: 0.997 - ETA: 10s - loss: 0.0205 - acc: 0.9944 - precision: 0.9961 - recall: 0.997 - ETA: 10s - loss: 0.0250 - acc: 0.9927 - precision: 0.9951 - recall: 0.996 - ETA: 10s - loss: 0.0270 - acc: 0.9922 - precision: 0.9943 - recall: 0.996 - ETA: 10s - loss: 0.0263 - acc: 0.9926 - precision: 0.9946 - recall: 0.996 - ETA: 9s - loss: 0.0252 - acc: 0.9931 - precision: 0.9949 - recall: 0.997 - ETA: 9s - loss: 0.0241 - acc: 0.9934 - precision: 0.9952 - recall: 0.99 - ETA: 9s - loss: 0.0231 - acc: 0.9938 - precision: 0.9955 - recall: 0.99 - ETA: 9s - loss: 0.0230 - acc: 0.9940 - precision: 0.9957 - recall: 0.99 - ETA: 8s - loss: 0.0222 - acc: 0.9943 - precision: 0.9959 - recall: 0.99 - ETA: 8s - loss: 0.0247 - acc: 0.9939 - precision: 0.9953 - recall: 0.99 - ETA: 8s - loss: 0.0245 - acc: 0.9935 - precision: 0.9947 - recall: 0.99 - ETA: 8s - loss: 0.0238 - acc: 0.9938 - precision: 0.9949 - recall: 0.99 - ETA: 8s - loss: 0.0232 - acc: 0.9940 - precision: 0.9951 - recall: 0.99 - ETA: 7s - loss: 0.0225 - acc: 0.9942 - precision: 0.9953 - recall: 0.99 - ETA: 7s - loss: 0.0227 - acc: 0.9939 - precision: 0.9948 - recall: 0.99 - ETA: 7s - loss: 0.0221 - acc: 0.9941 - precision: 0.9950 - recall: 0.99 - ETA: 7s - loss: 0.0221 - acc: 0.9938 - precision: 0.9946 - recall: 0.99 - ETA: 6s - loss: 0.0214 - acc: 0.9940 - precision: 0.9948 - recall: 0.99 - ETA: 6s - loss: 0.0214 - acc: 0.9937 - precision: 0.9949 - recall: 0.99 - ETA: 6s - loss: 0.0208 - acc: 0.9938 - precision: 0.9951 - recall: 0.99 - ETA: 6s - loss: 0.0205 - acc: 0.9940 - precision: 0.9952 - recall: 0.99 - ETA: 6s - loss: 0.0199 - acc: 0.9942 - precision: 0.9953 - recall: 0.99 - ETA: 5s - loss: 0.0195 - acc: 0.9944 - precision: 0.9955 - recall: 0.99 - ETA: 5s - loss: 0.0194 - acc: 0.9945 - precision: 0.9956 - recall: 0.99 - ETA: 5s - loss: 0.0194 - acc: 0.9942 - precision: 0.9953 - recall: 0.99 - ETA: 5s - loss: 0.0189 - acc: 0.9944 - precision: 0.9954 - recall: 0.99 - ETA: 5s - loss: 0.0198 - acc: 0.9941 - precision: 0.9950 - recall: 0.99 - ETA: 4s - loss: 0.0197 - acc: 0.9943 - precision: 0.9952 - recall: 0.99 - ETA: 4s - loss: 0.0194 - acc: 0.9944 - precision: 0.9953 - recall: 0.99 - ETA: 4s - loss: 0.0199 - acc: 0.9942 - precision: 0.9950 - recall: 0.99 - ETA: 4s - loss: 0.0206 - acc: 0.9936 - precision: 0.9951 - recall: 0.99 - ETA: 3s - loss: 0.0203 - acc: 0.9938 - precision: 0.9952 - recall: 0.99 - ETA: 3s - loss: 0.0202 - acc: 0.9939 - precision: 0.9953 - recall: 0.99 - ETA: 3s - loss: 0.0199 - acc: 0.9940 - precision: 0.9954 - recall: 0.99 - ETA: 3s - loss: 0.0212 - acc: 0.9938 - precision: 0.9951 - recall: 0.99 - ETA: 3s - loss: 0.0211 - acc: 0.9939 - precision: 0.9952 - recall: 0.99 - ETA: 3s - loss: 0.0245 - acc: 0.9931 - precision: 0.9942 - recall: 0.99 - ETA: 2s - loss: 0.0243 - acc: 0.9933 - precision: 0.9944 - recall: 0.99 - ETA: 2s - loss: 0.0240 - acc: 0.9934 - precision: 0.9945 - recall: 0.99 - ETA: 2s - loss: 0.0236 - acc: 0.9935 - precision: 0.9946 - recall: 0.99 - ETA: 2s - loss: 0.0234 - acc: 0.9936 - precision: 0.9947 - recall: 0.99 - ETA: 2s - loss: 0.0232 - acc: 0.9938 - precision: 0.9948 - recall: 0.99 - ETA: 1s - loss: 0.0230 - acc: 0.9939 - precision: 0.9949 - recall: 0.99 - ETA: 1s - loss: 0.0226 - acc: 0.9940 - precision: 0.9950 - recall: 0.99 - ETA: 1s - loss: 0.0248 - acc: 0.9933 - precision: 0.9941 - recall: 0.99 - ETA: 1s - loss: 0.0256 - acc: 0.9928 - precision: 0.9939 - recall: 0.99 - ETA: 1s - loss: 0.0254 - acc: 0.9927 - precision: 0.9940 - recall: 0.99 - ETA: 0s - loss: 0.0251 - acc: 0.9928 - precision: 0.9941 - recall: 0.99 - ETA: 0s - loss: 0.0248 - acc: 0.9929 - precision: 0.9942 - recall: 0.99 - ETA: 0s - loss: 0.0246 - acc: 0.9931 - precision: 0.9943 - recall: 0.99 - ETA: 0s - loss: 0.0242 - acc: 0.9932 - precision: 0.9944 - recall: 0.99 - ETA: 0s - loss: 0.0239 - acc: 0.9933 - precision: 0.9945 - recall: 0.99 - 14s 3ms/step - loss: 0.0238 - acc: 0.9933 - precision: 0.9945 - recall: 0.9978 - val_loss: 0.0604 - val_acc: 0.9821 - val_precision: 0.9867 - val_recall: 0.9925\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.06094 to 0.06038, saving model to results/spam_classifier_0.06\n",
      "Epoch 13/25\n",
      "4180/4180 [==============================] - ETA: 11s - loss: 0.0024 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0063 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0400 - acc: 0.9896 - precision: 0.9878 - recall: 1.000 - ETA: 10s - loss: 0.0327 - acc: 0.9922 - precision: 0.9910 - recall: 1.000 - ETA: 10s - loss: 0.0287 - acc: 0.9938 - precision: 0.9929 - recall: 1.000 - ETA: 10s - loss: 0.0249 - acc: 0.9948 - precision: 0.9940 - recall: 1.000 - ETA: 9s - loss: 0.0218 - acc: 0.9955 - precision: 0.9949 - recall: 1.000 - ETA: 9s - loss: 0.0198 - acc: 0.9961 - precision: 0.9955 - recall: 1.00 - ETA: 9s - loss: 0.0178 - acc: 0.9965 - precision: 0.9960 - recall: 1.00 - ETA: 9s - loss: 0.0163 - acc: 0.9969 - precision: 0.9964 - recall: 1.00 - ETA: 9s - loss: 0.0163 - acc: 0.9957 - precision: 0.9951 - recall: 1.00 - ETA: 8s - loss: 0.0158 - acc: 0.9961 - precision: 0.9955 - recall: 1.00 - ETA: 8s - loss: 0.0194 - acc: 0.9952 - precision: 0.9945 - recall: 1.00 - ETA: 8s - loss: 0.0206 - acc: 0.9933 - precision: 0.9923 - recall: 1.00 - ETA: 8s - loss: 0.0196 - acc: 0.9938 - precision: 0.9928 - recall: 1.00 - ETA: 8s - loss: 0.0188 - acc: 0.9941 - precision: 0.9933 - recall: 1.00 - ETA: 8s - loss: 0.0198 - acc: 0.9936 - precision: 0.9926 - recall: 1.00 - ETA: 7s - loss: 0.0194 - acc: 0.9939 - precision: 0.9930 - recall: 1.00 - ETA: 7s - loss: 0.0199 - acc: 0.9934 - precision: 0.9933 - recall: 0.99 - ETA: 7s - loss: 0.0191 - acc: 0.9938 - precision: 0.9937 - recall: 0.99 - ETA: 7s - loss: 0.0183 - acc: 0.9940 - precision: 0.9940 - recall: 0.99 - ETA: 7s - loss: 0.0178 - acc: 0.9943 - precision: 0.9942 - recall: 0.99 - ETA: 7s - loss: 0.0172 - acc: 0.9946 - precision: 0.9945 - recall: 0.99 - ETA: 7s - loss: 0.0165 - acc: 0.9948 - precision: 0.9947 - recall: 0.99 - ETA: 6s - loss: 0.0170 - acc: 0.9944 - precision: 0.9942 - recall: 0.99 - ETA: 6s - loss: 0.0167 - acc: 0.9946 - precision: 0.9944 - recall: 0.99 - ETA: 6s - loss: 0.0163 - acc: 0.9948 - precision: 0.9946 - recall: 0.99 - ETA: 6s - loss: 0.0160 - acc: 0.9950 - precision: 0.9948 - recall: 0.99 - ETA: 6s - loss: 0.0159 - acc: 0.9946 - precision: 0.9944 - recall: 0.99 - ETA: 6s - loss: 0.0154 - acc: 0.9948 - precision: 0.9946 - recall: 0.99 - ETA: 5s - loss: 0.0155 - acc: 0.9950 - precision: 0.9947 - recall: 0.99 - ETA: 5s - loss: 0.0201 - acc: 0.9937 - precision: 0.9932 - recall: 0.99 - ETA: 5s - loss: 0.0208 - acc: 0.9929 - precision: 0.9929 - recall: 0.99 - ETA: 5s - loss: 0.0205 - acc: 0.9931 - precision: 0.9931 - recall: 0.99 - ETA: 5s - loss: 0.0215 - acc: 0.9929 - precision: 0.9928 - recall: 0.99 - ETA: 5s - loss: 0.0210 - acc: 0.9931 - precision: 0.9930 - recall: 0.99 - ETA: 5s - loss: 0.0207 - acc: 0.9932 - precision: 0.9932 - recall: 0.99 - ETA: 4s - loss: 0.0203 - acc: 0.9934 - precision: 0.9934 - recall: 0.99 - ETA: 4s - loss: 0.0199 - acc: 0.9936 - precision: 0.9936 - recall: 0.99 - ETA: 4s - loss: 0.0196 - acc: 0.9938 - precision: 0.9937 - recall: 0.99 - ETA: 4s - loss: 0.0203 - acc: 0.9935 - precision: 0.9934 - recall: 0.99 - ETA: 4s - loss: 0.0200 - acc: 0.9937 - precision: 0.9936 - recall: 0.99 - ETA: 4s - loss: 0.0199 - acc: 0.9938 - precision: 0.9938 - recall: 0.99 - ETA: 3s - loss: 0.0195 - acc: 0.9940 - precision: 0.9939 - recall: 0.99 - ETA: 3s - loss: 0.0196 - acc: 0.9938 - precision: 0.9936 - recall: 0.99 - ETA: 3s - loss: 0.0193 - acc: 0.9939 - precision: 0.9938 - recall: 0.99 - ETA: 3s - loss: 0.0189 - acc: 0.9940 - precision: 0.9939 - recall: 0.99 - ETA: 3s - loss: 0.0195 - acc: 0.9938 - precision: 0.9937 - recall: 0.99 - ETA: 2s - loss: 0.0197 - acc: 0.9936 - precision: 0.9938 - recall: 0.99 - ETA: 2s - loss: 0.0226 - acc: 0.9928 - precision: 0.9928 - recall: 0.99 - ETA: 2s - loss: 0.0226 - acc: 0.9930 - precision: 0.9930 - recall: 0.99 - ETA: 2s - loss: 0.0223 - acc: 0.9931 - precision: 0.9931 - recall: 0.99 - ETA: 2s - loss: 0.0221 - acc: 0.9932 - precision: 0.9933 - recall: 0.99 - ETA: 2s - loss: 0.0225 - acc: 0.9931 - precision: 0.9930 - recall: 0.99 - ETA: 1s - loss: 0.0222 - acc: 0.9932 - precision: 0.9932 - recall: 0.99 - ETA: 1s - loss: 0.0219 - acc: 0.9933 - precision: 0.9933 - recall: 0.99 - ETA: 1s - loss: 0.0218 - acc: 0.9931 - precision: 0.9934 - recall: 0.99 - ETA: 1s - loss: 0.0224 - acc: 0.9927 - precision: 0.9929 - recall: 0.99 - ETA: 1s - loss: 0.0221 - acc: 0.9928 - precision: 0.9930 - recall: 0.99 - ETA: 0s - loss: 0.0226 - acc: 0.9927 - precision: 0.9928 - recall: 0.99 - ETA: 0s - loss: 0.0224 - acc: 0.9928 - precision: 0.9930 - recall: 0.99 - ETA: 0s - loss: 0.0221 - acc: 0.9929 - precision: 0.9931 - recall: 0.99 - ETA: 0s - loss: 0.0219 - acc: 0.9931 - precision: 0.9932 - recall: 0.99 - ETA: 0s - loss: 0.0217 - acc: 0.9929 - precision: 0.9930 - recall: 0.99 - ETA: 0s - loss: 0.0215 - acc: 0.9930 - precision: 0.9931 - recall: 0.99 - 13s 3ms/step - loss: 0.0216 - acc: 0.9928 - precision: 0.9931 - recall: 0.9986 - val_loss: 0.0939 - val_acc: 0.9720 - val_precision: 0.9685 - val_recall: 1.0000\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.06038\n",
      "Epoch 14/25\n",
      "4180/4180 [==============================] - ETA: 13s - loss: 0.0403 - acc: 0.9844 - precision: 0.9825 - recall: 1.000 - ETA: 12s - loss: 0.0211 - acc: 0.9922 - precision: 0.9913 - recall: 1.000 - ETA: 12s - loss: 0.0149 - acc: 0.9948 - precision: 0.9942 - recall: 1.000 - ETA: 12s - loss: 0.0126 - acc: 0.9961 - precision: 0.9956 - recall: 1.000 - ETA: 12s - loss: 0.0108 - acc: 0.9969 - precision: 0.9964 - recall: 1.000 - ETA: 12s - loss: 0.0099 - acc: 0.9974 - precision: 0.9971 - recall: 1.000 - ETA: 11s - loss: 0.0150 - acc: 0.9955 - precision: 0.9949 - recall: 1.000 - ETA: 11s - loss: 0.0136 - acc: 0.9961 - precision: 0.9956 - recall: 1.000 - ETA: 11s - loss: 0.0124 - acc: 0.9965 - precision: 0.9961 - recall: 1.000 - ETA: 10s - loss: 0.0113 - acc: 0.9969 - precision: 0.9965 - recall: 1.000 - ETA: 10s - loss: 0.0105 - acc: 0.9972 - precision: 0.9968 - recall: 1.000 - ETA: 10s - loss: 0.0099 - acc: 0.9974 - precision: 0.9971 - recall: 1.000 - ETA: 10s - loss: 0.0092 - acc: 0.9976 - precision: 0.9973 - recall: 1.000 - ETA: 10s - loss: 0.0088 - acc: 0.9978 - precision: 0.9975 - recall: 1.000 - ETA: 10s - loss: 0.0084 - acc: 0.9979 - precision: 0.9976 - recall: 1.000 - ETA: 9s - loss: 0.0080 - acc: 0.9980 - precision: 0.9978 - recall: 1.000 - ETA: 9s - loss: 0.0128 - acc: 0.9972 - precision: 0.9969 - recall: 1.00 - ETA: 9s - loss: 0.0123 - acc: 0.9974 - precision: 0.9970 - recall: 1.00 - ETA: 9s - loss: 0.0119 - acc: 0.9975 - precision: 0.9972 - recall: 1.00 - ETA: 9s - loss: 0.0114 - acc: 0.9977 - precision: 0.9973 - recall: 1.00 - ETA: 8s - loss: 0.0115 - acc: 0.9978 - precision: 0.9975 - recall: 1.00 - ETA: 8s - loss: 0.0111 - acc: 0.9979 - precision: 0.9976 - recall: 1.00 - ETA: 8s - loss: 0.0160 - acc: 0.9966 - precision: 0.9961 - recall: 1.00 - ETA: 8s - loss: 0.0162 - acc: 0.9967 - precision: 0.9963 - recall: 1.00 - ETA: 8s - loss: 0.0163 - acc: 0.9969 - precision: 0.9964 - recall: 1.00 - ETA: 7s - loss: 0.0159 - acc: 0.9970 - precision: 0.9966 - recall: 1.00 - ETA: 7s - loss: 0.0158 - acc: 0.9965 - precision: 0.9960 - recall: 1.00 - ETA: 7s - loss: 0.0163 - acc: 0.9961 - precision: 0.9955 - recall: 1.00 - ETA: 7s - loss: 0.0159 - acc: 0.9962 - precision: 0.9957 - recall: 1.00 - ETA: 7s - loss: 0.0165 - acc: 0.9958 - precision: 0.9958 - recall: 0.99 - ETA: 6s - loss: 0.0177 - acc: 0.9955 - precision: 0.9954 - recall: 0.99 - ETA: 6s - loss: 0.0173 - acc: 0.9956 - precision: 0.9955 - recall: 0.99 - ETA: 6s - loss: 0.0184 - acc: 0.9953 - precision: 0.9951 - recall: 0.99 - ETA: 6s - loss: 0.0182 - acc: 0.9954 - precision: 0.9953 - recall: 0.99 - ETA: 6s - loss: 0.0180 - acc: 0.9955 - precision: 0.9954 - recall: 0.99 - ETA: 5s - loss: 0.0189 - acc: 0.9948 - precision: 0.9945 - recall: 0.99 - ETA: 5s - loss: 0.0185 - acc: 0.9949 - precision: 0.9947 - recall: 0.99 - ETA: 5s - loss: 0.0181 - acc: 0.9951 - precision: 0.9948 - recall: 0.99 - ETA: 5s - loss: 0.0177 - acc: 0.9952 - precision: 0.9949 - recall: 0.99 - ETA: 5s - loss: 0.0182 - acc: 0.9949 - precision: 0.9946 - recall: 0.99 - ETA: 4s - loss: 0.0180 - acc: 0.9950 - precision: 0.9947 - recall: 0.99 - ETA: 4s - loss: 0.0177 - acc: 0.9952 - precision: 0.9949 - recall: 0.99 - ETA: 4s - loss: 0.0173 - acc: 0.9953 - precision: 0.9950 - recall: 0.99 - ETA: 4s - loss: 0.0182 - acc: 0.9950 - precision: 0.9947 - recall: 0.99 - ETA: 4s - loss: 0.0184 - acc: 0.9948 - precision: 0.9944 - recall: 0.99 - ETA: 3s - loss: 0.0190 - acc: 0.9946 - precision: 0.9942 - recall: 0.99 - ETA: 3s - loss: 0.0188 - acc: 0.9947 - precision: 0.9943 - recall: 0.99 - ETA: 3s - loss: 0.0185 - acc: 0.9948 - precision: 0.9944 - recall: 0.99 - ETA: 3s - loss: 0.0184 - acc: 0.9949 - precision: 0.9945 - recall: 0.99 - ETA: 3s - loss: 0.0182 - acc: 0.9950 - precision: 0.9946 - recall: 0.99 - ETA: 2s - loss: 0.0185 - acc: 0.9948 - precision: 0.9944 - recall: 0.99 - ETA: 2s - loss: 0.0189 - acc: 0.9946 - precision: 0.9945 - recall: 0.99 - ETA: 2s - loss: 0.0186 - acc: 0.9947 - precision: 0.9946 - recall: 0.99 - ETA: 2s - loss: 0.0183 - acc: 0.9948 - precision: 0.9947 - recall: 0.99 - ETA: 2s - loss: 0.0180 - acc: 0.9949 - precision: 0.9948 - recall: 0.99 - ETA: 1s - loss: 0.0178 - acc: 0.9950 - precision: 0.9949 - recall: 0.99 - ETA: 1s - loss: 0.0176 - acc: 0.9951 - precision: 0.9950 - recall: 0.99 - ETA: 1s - loss: 0.0173 - acc: 0.9952 - precision: 0.9950 - recall: 0.99 - ETA: 1s - loss: 0.0171 - acc: 0.9952 - precision: 0.9951 - recall: 0.99 - ETA: 1s - loss: 0.0168 - acc: 0.9953 - precision: 0.9952 - recall: 0.99 - ETA: 0s - loss: 0.0166 - acc: 0.9954 - precision: 0.9953 - recall: 0.99 - ETA: 0s - loss: 0.0164 - acc: 0.9955 - precision: 0.9954 - recall: 0.99 - ETA: 0s - loss: 0.0170 - acc: 0.9953 - precision: 0.9952 - recall: 0.99 - ETA: 0s - loss: 0.0169 - acc: 0.9954 - precision: 0.9952 - recall: 0.99 - ETA: 0s - loss: 0.0167 - acc: 0.9954 - precision: 0.9953 - recall: 0.99 - 14s 3ms/step - loss: 0.0167 - acc: 0.9955 - precision: 0.9953 - recall: 0.9994 - val_loss: 0.0829 - val_acc: 0.9799 - val_precision: 0.9851 - val_recall: 0.9917\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.06038\n",
      "Epoch 15/25\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 0.0049 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0028 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0029 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0042 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0039 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0100 - acc: 0.9974 - precision: 0.9971 - recall: 1.000 - ETA: 10s - loss: 0.0093 - acc: 0.9978 - precision: 0.9975 - recall: 1.000 - ETA: 9s - loss: 0.0083 - acc: 0.9980 - precision: 0.9978 - recall: 1.000 - ETA: 9s - loss: 0.0104 - acc: 0.9965 - precision: 0.9961 - recall: 1.00 - ETA: 9s - loss: 0.0096 - acc: 0.9969 - precision: 0.9965 - recall: 1.00 - ETA: 9s - loss: 0.0163 - acc: 0.9957 - precision: 0.9967 - recall: 0.99 - ETA: 9s - loss: 0.0337 - acc: 0.9922 - precision: 0.9925 - recall: 0.99 - ETA: 9s - loss: 0.0324 - acc: 0.9928 - precision: 0.9931 - recall: 0.99 - ETA: 8s - loss: 0.0306 - acc: 0.9933 - precision: 0.9937 - recall: 0.99 - ETA: 8s - loss: 0.0286 - acc: 0.9938 - precision: 0.9940 - recall: 0.99 - ETA: 8s - loss: 0.0272 - acc: 0.9941 - precision: 0.9944 - recall: 0.99 - ETA: 8s - loss: 0.0259 - acc: 0.9945 - precision: 0.9947 - recall: 0.99 - ETA: 8s - loss: 0.0287 - acc: 0.9939 - precision: 0.9939 - recall: 0.99 - ETA: 7s - loss: 0.0275 - acc: 0.9942 - precision: 0.9943 - recall: 0.99 - ETA: 7s - loss: 0.0267 - acc: 0.9945 - precision: 0.9946 - recall: 0.99 - ETA: 7s - loss: 0.0262 - acc: 0.9948 - precision: 0.9948 - recall: 0.99 - ETA: 7s - loss: 0.0251 - acc: 0.9950 - precision: 0.9950 - recall: 0.99 - ETA: 7s - loss: 0.0243 - acc: 0.9952 - precision: 0.9953 - recall: 0.99 - ETA: 7s - loss: 0.0234 - acc: 0.9954 - precision: 0.9955 - recall: 0.99 - ETA: 7s - loss: 0.0226 - acc: 0.9956 - precision: 0.9957 - recall: 0.99 - ETA: 7s - loss: 0.0221 - acc: 0.9958 - precision: 0.9958 - recall: 0.99 - ETA: 6s - loss: 0.0215 - acc: 0.9959 - precision: 0.9960 - recall: 0.99 - ETA: 6s - loss: 0.0209 - acc: 0.9961 - precision: 0.9961 - recall: 0.99 - ETA: 6s - loss: 0.0202 - acc: 0.9962 - precision: 0.9962 - recall: 0.99 - ETA: 6s - loss: 0.0195 - acc: 0.9964 - precision: 0.9964 - recall: 0.99 - ETA: 6s - loss: 0.0190 - acc: 0.9965 - precision: 0.9965 - recall: 0.99 - ETA: 6s - loss: 0.0192 - acc: 0.9961 - precision: 0.9960 - recall: 0.99 - ETA: 5s - loss: 0.0187 - acc: 0.9962 - precision: 0.9962 - recall: 0.99 - ETA: 5s - loss: 0.0182 - acc: 0.9963 - precision: 0.9963 - recall: 0.99 - ETA: 5s - loss: 0.0177 - acc: 0.9964 - precision: 0.9964 - recall: 0.99 - ETA: 5s - loss: 0.0200 - acc: 0.9957 - precision: 0.9955 - recall: 0.99 - ETA: 5s - loss: 0.0200 - acc: 0.9958 - precision: 0.9956 - recall: 0.99 - ETA: 4s - loss: 0.0196 - acc: 0.9959 - precision: 0.9957 - recall: 0.99 - ETA: 4s - loss: 0.0195 - acc: 0.9956 - precision: 0.9958 - recall: 0.99 - ETA: 4s - loss: 0.0191 - acc: 0.9957 - precision: 0.9959 - recall: 0.99 - ETA: 4s - loss: 0.0187 - acc: 0.9958 - precision: 0.9960 - recall: 0.99 - ETA: 4s - loss: 0.0195 - acc: 0.9955 - precision: 0.9957 - recall: 0.99 - ETA: 3s - loss: 0.0193 - acc: 0.9956 - precision: 0.9958 - recall: 0.99 - ETA: 3s - loss: 0.0193 - acc: 0.9954 - precision: 0.9955 - recall: 0.99 - ETA: 3s - loss: 0.0192 - acc: 0.9955 - precision: 0.9956 - recall: 0.99 - ETA: 3s - loss: 0.0188 - acc: 0.9956 - precision: 0.9957 - recall: 0.99 - ETA: 3s - loss: 0.0189 - acc: 0.9953 - precision: 0.9954 - recall: 0.99 - ETA: 3s - loss: 0.0186 - acc: 0.9954 - precision: 0.9955 - recall: 0.99 - ETA: 2s - loss: 0.0200 - acc: 0.9949 - precision: 0.9952 - recall: 0.99 - ETA: 2s - loss: 0.0197 - acc: 0.9950 - precision: 0.9953 - recall: 0.99 - ETA: 2s - loss: 0.0193 - acc: 0.9951 - precision: 0.9954 - recall: 0.99 - ETA: 2s - loss: 0.0190 - acc: 0.9952 - precision: 0.9955 - recall: 0.99 - ETA: 2s - loss: 0.0186 - acc: 0.9953 - precision: 0.9956 - recall: 0.99 - ETA: 2s - loss: 0.0183 - acc: 0.9954 - precision: 0.9957 - recall: 0.99 - ETA: 1s - loss: 0.0187 - acc: 0.9952 - precision: 0.9954 - recall: 0.99 - ETA: 1s - loss: 0.0184 - acc: 0.9953 - precision: 0.9955 - recall: 0.99 - ETA: 1s - loss: 0.0181 - acc: 0.9953 - precision: 0.9956 - recall: 0.99 - ETA: 1s - loss: 0.0186 - acc: 0.9949 - precision: 0.9954 - recall: 0.99 - ETA: 1s - loss: 0.0186 - acc: 0.9947 - precision: 0.9951 - recall: 0.99 - ETA: 0s - loss: 0.0185 - acc: 0.9948 - precision: 0.9952 - recall: 0.99 - ETA: 0s - loss: 0.0183 - acc: 0.9949 - precision: 0.9953 - recall: 0.99 - ETA: 0s - loss: 0.0181 - acc: 0.9950 - precision: 0.9954 - recall: 0.99 - ETA: 0s - loss: 0.0183 - acc: 0.9948 - precision: 0.9952 - recall: 0.99 - ETA: 0s - loss: 0.0181 - acc: 0.9949 - precision: 0.9952 - recall: 0.99 - ETA: 0s - loss: 0.0180 - acc: 0.9950 - precision: 0.9953 - recall: 0.99 - 13s 3ms/step - loss: 0.0179 - acc: 0.9950 - precision: 0.9953 - recall: 0.9989 - val_loss: 0.0806 - val_acc: 0.9842 - val_precision: 0.9900 - val_recall: 0.9917\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.06038\n",
      "Epoch 16/25\n",
      "4180/4180 [==============================] - ETA: 12s - loss: 4.3385e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0014 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000    - ETA: 12s - loss: 0.0012 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0239 - acc: 0.9961 - precision: 0.9956 - recall: 1.000 - ETA: 11s - loss: 0.0224 - acc: 0.9969 - precision: 0.9965 - recall: 1.000 - ETA: 11s - loss: 0.0265 - acc: 0.9948 - precision: 0.9941 - recall: 1.000 - ETA: 11s - loss: 0.0266 - acc: 0.9933 - precision: 0.9950 - recall: 0.997 - ETA: 11s - loss: 0.0240 - acc: 0.9941 - precision: 0.9956 - recall: 0.997 - ETA: 11s - loss: 0.0216 - acc: 0.9948 - precision: 0.9961 - recall: 0.998 - ETA: 11s - loss: 0.0197 - acc: 0.9953 - precision: 0.9965 - recall: 0.998 - ETA: 11s - loss: 0.0180 - acc: 0.9957 - precision: 0.9968 - recall: 0.998 - ETA: 10s - loss: 0.0185 - acc: 0.9948 - precision: 0.9956 - recall: 0.998 - ETA: 10s - loss: 0.0174 - acc: 0.9952 - precision: 0.9959 - recall: 0.998 - ETA: 10s - loss: 0.0170 - acc: 0.9955 - precision: 0.9962 - recall: 0.998 - ETA: 10s - loss: 0.0163 - acc: 0.9958 - precision: 0.9964 - recall: 0.998 - ETA: 10s - loss: 0.0168 - acc: 0.9951 - precision: 0.9955 - recall: 0.998 - ETA: 9s - loss: 0.0160 - acc: 0.9954 - precision: 0.9958 - recall: 0.998 - ETA: 9s - loss: 0.0162 - acc: 0.9957 - precision: 0.9960 - recall: 0.99 - ETA: 9s - loss: 0.0155 - acc: 0.9959 - precision: 0.9962 - recall: 0.99 - ETA: 9s - loss: 0.0153 - acc: 0.9961 - precision: 0.9964 - recall: 0.99 - ETA: 8s - loss: 0.0148 - acc: 0.9963 - precision: 0.9966 - recall: 0.99 - ETA: 8s - loss: 0.0142 - acc: 0.9964 - precision: 0.9967 - recall: 0.99 - ETA: 8s - loss: 0.0136 - acc: 0.9966 - precision: 0.9969 - recall: 0.99 - ETA: 7s - loss: 0.0142 - acc: 0.9961 - precision: 0.9963 - recall: 0.99 - ETA: 7s - loss: 0.0216 - acc: 0.9944 - precision: 0.9964 - recall: 0.99 - ETA: 7s - loss: 0.0208 - acc: 0.9946 - precision: 0.9965 - recall: 0.99 - ETA: 7s - loss: 0.0201 - acc: 0.9948 - precision: 0.9967 - recall: 0.99 - ETA: 7s - loss: 0.0194 - acc: 0.9950 - precision: 0.9968 - recall: 0.99 - ETA: 6s - loss: 0.0188 - acc: 0.9952 - precision: 0.9969 - recall: 0.99 - ETA: 6s - loss: 0.0189 - acc: 0.9948 - precision: 0.9964 - recall: 0.99 - ETA: 6s - loss: 0.0184 - acc: 0.9950 - precision: 0.9965 - recall: 0.99 - ETA: 6s - loss: 0.0180 - acc: 0.9951 - precision: 0.9967 - recall: 0.99 - ETA: 6s - loss: 0.0175 - acc: 0.9953 - precision: 0.9968 - recall: 0.99 - ETA: 6s - loss: 0.0170 - acc: 0.9954 - precision: 0.9969 - recall: 0.99 - ETA: 5s - loss: 0.0166 - acc: 0.9955 - precision: 0.9969 - recall: 0.99 - ETA: 5s - loss: 0.0173 - acc: 0.9952 - precision: 0.9965 - recall: 0.99 - ETA: 5s - loss: 0.0171 - acc: 0.9954 - precision: 0.9966 - recall: 0.99 - ETA: 5s - loss: 0.0167 - acc: 0.9955 - precision: 0.9967 - recall: 0.99 - ETA: 4s - loss: 0.0164 - acc: 0.9956 - precision: 0.9968 - recall: 0.99 - ETA: 4s - loss: 0.0174 - acc: 0.9953 - precision: 0.9964 - recall: 0.99 - ETA: 4s - loss: 0.0171 - acc: 0.9954 - precision: 0.9965 - recall: 0.99 - ETA: 4s - loss: 0.0169 - acc: 0.9955 - precision: 0.9966 - recall: 0.99 - ETA: 4s - loss: 0.0165 - acc: 0.9956 - precision: 0.9967 - recall: 0.99 - ETA: 4s - loss: 0.0165 - acc: 0.9954 - precision: 0.9967 - recall: 0.99 - ETA: 3s - loss: 0.0162 - acc: 0.9955 - precision: 0.9968 - recall: 0.99 - ETA: 3s - loss: 0.0159 - acc: 0.9956 - precision: 0.9969 - recall: 0.99 - ETA: 3s - loss: 0.0157 - acc: 0.9957 - precision: 0.9970 - recall: 0.99 - ETA: 3s - loss: 0.0156 - acc: 0.9954 - precision: 0.9966 - recall: 0.99 - ETA: 3s - loss: 0.0154 - acc: 0.9955 - precision: 0.9967 - recall: 0.99 - ETA: 2s - loss: 0.0151 - acc: 0.9956 - precision: 0.9968 - recall: 0.99 - ETA: 2s - loss: 0.0149 - acc: 0.9957 - precision: 0.9968 - recall: 0.99 - ETA: 2s - loss: 0.0149 - acc: 0.9955 - precision: 0.9966 - recall: 0.99 - ETA: 2s - loss: 0.0156 - acc: 0.9953 - precision: 0.9966 - recall: 0.99 - ETA: 2s - loss: 0.0162 - acc: 0.9951 - precision: 0.9963 - recall: 0.99 - ETA: 1s - loss: 0.0160 - acc: 0.9952 - precision: 0.9964 - recall: 0.99 - ETA: 1s - loss: 0.0157 - acc: 0.9953 - precision: 0.9965 - recall: 0.99 - ETA: 1s - loss: 0.0155 - acc: 0.9953 - precision: 0.9965 - recall: 0.99 - ETA: 1s - loss: 0.0152 - acc: 0.9954 - precision: 0.9966 - recall: 0.99 - ETA: 1s - loss: 0.0150 - acc: 0.9955 - precision: 0.9967 - recall: 0.99 - ETA: 1s - loss: 0.0165 - acc: 0.9953 - precision: 0.9964 - recall: 0.99 - ETA: 0s - loss: 0.0163 - acc: 0.9954 - precision: 0.9965 - recall: 0.99 - ETA: 0s - loss: 0.0161 - acc: 0.9955 - precision: 0.9965 - recall: 0.99 - ETA: 0s - loss: 0.0160 - acc: 0.9955 - precision: 0.9966 - recall: 0.99 - ETA: 0s - loss: 0.0158 - acc: 0.9956 - precision: 0.9966 - recall: 0.99 - ETA: 0s - loss: 0.0156 - acc: 0.9957 - precision: 0.9967 - recall: 0.99 - 14s 3ms/step - loss: 0.0155 - acc: 0.9957 - precision: 0.9967 - recall: 0.9983 - val_loss: 0.0612 - val_acc: 0.9821 - val_precision: 0.9908 - val_recall: 0.9883\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.06038\n",
      "Epoch 17/25\n",
      "4180/4180 [==============================] - ETA: 14s - loss: 0.0020 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 13s - loss: 0.0028 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0038 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0031 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0085 - acc: 0.9969 - precision: 0.9964 - recall: 1.000 - ETA: 11s - loss: 0.0081 - acc: 0.9974 - precision: 0.9970 - recall: 1.000 - ETA: 10s - loss: 0.0084 - acc: 0.9978 - precision: 0.9974 - recall: 1.000 - ETA: 10s - loss: 0.0078 - acc: 0.9980 - precision: 0.9978 - recall: 1.000 - ETA: 10s - loss: 0.0077 - acc: 0.9983 - precision: 0.9980 - recall: 1.000 - ETA: 10s - loss: 0.0072 - acc: 0.9984 - precision: 0.9982 - recall: 1.000 - ETA: 10s - loss: 0.0068 - acc: 0.9986 - precision: 0.9984 - recall: 1.000 - ETA: 9s - loss: 0.0064 - acc: 0.9987 - precision: 0.9985 - recall: 1.000 - ETA: 9s - loss: 0.0085 - acc: 0.9976 - precision: 0.9972 - recall: 1.00 - ETA: 9s - loss: 0.0082 - acc: 0.9978 - precision: 0.9974 - recall: 1.00 - ETA: 9s - loss: 0.0086 - acc: 0.9979 - precision: 0.9976 - recall: 1.00 - ETA: 9s - loss: 0.0087 - acc: 0.9980 - precision: 0.9977 - recall: 1.00 - ETA: 9s - loss: 0.0085 - acc: 0.9982 - precision: 0.9979 - recall: 1.00 - ETA: 9s - loss: 0.0081 - acc: 0.9983 - precision: 0.9980 - recall: 1.00 - ETA: 8s - loss: 0.0077 - acc: 0.9984 - precision: 0.9981 - recall: 1.00 - ETA: 8s - loss: 0.0074 - acc: 0.9984 - precision: 0.9982 - recall: 1.00 - ETA: 8s - loss: 0.0073 - acc: 0.9985 - precision: 0.9983 - recall: 1.00 - ETA: 8s - loss: 0.0070 - acc: 0.9986 - precision: 0.9984 - recall: 1.00 - ETA: 7s - loss: 0.0082 - acc: 0.9980 - precision: 0.9977 - recall: 1.00 - ETA: 7s - loss: 0.0079 - acc: 0.9980 - precision: 0.9977 - recall: 1.00 - ETA: 7s - loss: 0.0079 - acc: 0.9981 - precision: 0.9978 - recall: 1.00 - ETA: 7s - loss: 0.0077 - acc: 0.9982 - precision: 0.9979 - recall: 1.00 - ETA: 7s - loss: 0.0095 - acc: 0.9971 - precision: 0.9980 - recall: 0.99 - ETA: 6s - loss: 0.0098 - acc: 0.9967 - precision: 0.9974 - recall: 0.99 - ETA: 6s - loss: 0.0115 - acc: 0.9957 - precision: 0.9963 - recall: 0.99 - ETA: 6s - loss: 0.0113 - acc: 0.9958 - precision: 0.9964 - recall: 0.99 - ETA: 6s - loss: 0.0110 - acc: 0.9960 - precision: 0.9965 - recall: 0.99 - ETA: 5s - loss: 0.0108 - acc: 0.9961 - precision: 0.9966 - recall: 0.99 - ETA: 5s - loss: 0.0106 - acc: 0.9962 - precision: 0.9967 - recall: 0.99 - ETA: 5s - loss: 0.0106 - acc: 0.9963 - precision: 0.9968 - recall: 0.99 - ETA: 5s - loss: 0.0105 - acc: 0.9964 - precision: 0.9969 - recall: 0.99 - ETA: 5s - loss: 0.0125 - acc: 0.9961 - precision: 0.9965 - recall: 0.99 - ETA: 5s - loss: 0.0123 - acc: 0.9962 - precision: 0.9966 - recall: 0.99 - ETA: 4s - loss: 0.0121 - acc: 0.9963 - precision: 0.9967 - recall: 0.99 - ETA: 4s - loss: 0.0118 - acc: 0.9964 - precision: 0.9968 - recall: 0.99 - ETA: 4s - loss: 0.0115 - acc: 0.9965 - precision: 0.9968 - recall: 0.99 - ETA: 4s - loss: 0.0113 - acc: 0.9966 - precision: 0.9969 - recall: 0.99 - ETA: 4s - loss: 0.0110 - acc: 0.9967 - precision: 0.9970 - recall: 0.99 - ETA: 3s - loss: 0.0108 - acc: 0.9967 - precision: 0.9971 - recall: 0.99 - ETA: 3s - loss: 0.0106 - acc: 0.9968 - precision: 0.9971 - recall: 0.99 - ETA: 3s - loss: 0.0104 - acc: 0.9969 - precision: 0.9972 - recall: 0.99 - ETA: 3s - loss: 0.0102 - acc: 0.9969 - precision: 0.9973 - recall: 0.99 - ETA: 3s - loss: 0.0100 - acc: 0.9970 - precision: 0.9973 - recall: 0.99 - ETA: 3s - loss: 0.0098 - acc: 0.9971 - precision: 0.9974 - recall: 0.99 - ETA: 2s - loss: 0.0097 - acc: 0.9971 - precision: 0.9974 - recall: 0.99 - ETA: 2s - loss: 0.0095 - acc: 0.9972 - precision: 0.9975 - recall: 0.99 - ETA: 2s - loss: 0.0093 - acc: 0.9972 - precision: 0.9975 - recall: 0.99 - ETA: 2s - loss: 0.0092 - acc: 0.9973 - precision: 0.9976 - recall: 0.99 - ETA: 2s - loss: 0.0094 - acc: 0.9971 - precision: 0.9973 - recall: 0.99 - ETA: 1s - loss: 0.0093 - acc: 0.9971 - precision: 0.9973 - recall: 0.99 - ETA: 1s - loss: 0.0129 - acc: 0.9966 - precision: 0.9974 - recall: 0.99 - ETA: 1s - loss: 0.0145 - acc: 0.9961 - precision: 0.9968 - recall: 0.99 - ETA: 1s - loss: 0.0144 - acc: 0.9962 - precision: 0.9968 - recall: 0.99 - ETA: 1s - loss: 0.0149 - acc: 0.9960 - precision: 0.9966 - recall: 0.99 - ETA: 1s - loss: 0.0147 - acc: 0.9960 - precision: 0.9966 - recall: 0.99 - ETA: 0s - loss: 0.0145 - acc: 0.9961 - precision: 0.9967 - recall: 0.99 - ETA: 0s - loss: 0.0145 - acc: 0.9959 - precision: 0.9965 - recall: 0.99 - ETA: 0s - loss: 0.0143 - acc: 0.9960 - precision: 0.9965 - recall: 0.99 - ETA: 0s - loss: 0.0142 - acc: 0.9960 - precision: 0.9966 - recall: 0.99 - ETA: 0s - loss: 0.0143 - acc: 0.9958 - precision: 0.9966 - recall: 0.99 - ETA: 0s - loss: 0.0141 - acc: 0.9959 - precision: 0.9967 - recall: 0.99 - 12s 3ms/step - loss: 0.0141 - acc: 0.9959 - precision: 0.9967 - recall: 0.9986 - val_loss: 0.0766 - val_acc: 0.9828 - val_precision: 0.9811 - val_recall: 0.9992\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.06038\n",
      "Epoch 18/25\n",
      "4180/4180 [==============================] - ETA: 9s - loss: 2.5931e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0115 - acc: 0.9922 - precision: 0.9915 - recall: 1.0000   - ETA: 9s - loss: 0.0082 - acc: 0.9948 - precision: 0.9944 - recall: 1.00 - ETA: 9s - loss: 0.0071 - acc: 0.9961 - precision: 0.9957 - recall: 1.00 - ETA: 9s - loss: 0.0121 - acc: 0.9938 - precision: 0.9931 - recall: 1.00 - ETA: 9s - loss: 0.0105 - acc: 0.9948 - precision: 0.9943 - recall: 1.00 - ETA: 9s - loss: 0.0093 - acc: 0.9955 - precision: 0.9951 - recall: 1.00 - ETA: 9s - loss: 0.0126 - acc: 0.9941 - precision: 0.9935 - recall: 1.00 - ETA: 9s - loss: 0.0116 - acc: 0.9948 - precision: 0.9942 - recall: 1.00 - ETA: 9s - loss: 0.0116 - acc: 0.9953 - precision: 0.9947 - recall: 1.00 - ETA: 9s - loss: 0.0109 - acc: 0.9957 - precision: 0.9952 - recall: 1.00 - ETA: 8s - loss: 0.0101 - acc: 0.9961 - precision: 0.9956 - recall: 1.00 - ETA: 8s - loss: 0.0098 - acc: 0.9964 - precision: 0.9960 - recall: 1.00 - ETA: 8s - loss: 0.0092 - acc: 0.9967 - precision: 0.9962 - recall: 1.00 - ETA: 8s - loss: 0.0087 - acc: 0.9969 - precision: 0.9965 - recall: 1.00 - ETA: 8s - loss: 0.0085 - acc: 0.9971 - precision: 0.9967 - recall: 1.00 - ETA: 7s - loss: 0.0080 - acc: 0.9972 - precision: 0.9969 - recall: 1.00 - ETA: 7s - loss: 0.0077 - acc: 0.9974 - precision: 0.9971 - recall: 1.00 - ETA: 7s - loss: 0.0075 - acc: 0.9975 - precision: 0.9972 - recall: 1.00 - ETA: 7s - loss: 0.0072 - acc: 0.9977 - precision: 0.9974 - recall: 1.00 - ETA: 7s - loss: 0.0070 - acc: 0.9978 - precision: 0.9975 - recall: 1.00 - ETA: 7s - loss: 0.0067 - acc: 0.9979 - precision: 0.9976 - recall: 1.00 - ETA: 6s - loss: 0.0066 - acc: 0.9980 - precision: 0.9977 - recall: 1.00 - ETA: 6s - loss: 0.0064 - acc: 0.9980 - precision: 0.9978 - recall: 1.00 - ETA: 6s - loss: 0.0062 - acc: 0.9981 - precision: 0.9979 - recall: 1.00 - ETA: 6s - loss: 0.0060 - acc: 0.9982 - precision: 0.9980 - recall: 1.00 - ETA: 6s - loss: 0.0058 - acc: 0.9983 - precision: 0.9980 - recall: 1.00 - ETA: 5s - loss: 0.0068 - acc: 0.9978 - precision: 0.9975 - recall: 1.00 - ETA: 5s - loss: 0.0066 - acc: 0.9978 - precision: 0.9975 - recall: 1.00 - ETA: 5s - loss: 0.0066 - acc: 0.9979 - precision: 0.9976 - recall: 1.00 - ETA: 5s - loss: 0.0065 - acc: 0.9980 - precision: 0.9977 - recall: 1.00 - ETA: 5s - loss: 0.0064 - acc: 0.9980 - precision: 0.9978 - recall: 1.00 - ETA: 5s - loss: 0.0063 - acc: 0.9981 - precision: 0.9978 - recall: 1.00 - ETA: 4s - loss: 0.0062 - acc: 0.9982 - precision: 0.9979 - recall: 1.00 - ETA: 4s - loss: 0.0060 - acc: 0.9982 - precision: 0.9980 - recall: 1.00 - ETA: 4s - loss: 0.0058 - acc: 0.9983 - precision: 0.9980 - recall: 1.00 - ETA: 4s - loss: 0.0057 - acc: 0.9983 - precision: 0.9981 - recall: 1.00 - ETA: 4s - loss: 0.0056 - acc: 0.9984 - precision: 0.9981 - recall: 1.00 - ETA: 4s - loss: 0.0068 - acc: 0.9980 - precision: 0.9977 - recall: 1.00 - ETA: 4s - loss: 0.0081 - acc: 0.9977 - precision: 0.9978 - recall: 0.99 - ETA: 3s - loss: 0.0081 - acc: 0.9977 - precision: 0.9978 - recall: 0.99 - ETA: 3s - loss: 0.0080 - acc: 0.9978 - precision: 0.9979 - recall: 0.99 - ETA: 3s - loss: 0.0095 - acc: 0.9971 - precision: 0.9971 - recall: 0.99 - ETA: 3s - loss: 0.0101 - acc: 0.9968 - precision: 0.9972 - recall: 0.99 - ETA: 3s - loss: 0.0100 - acc: 0.9969 - precision: 0.9972 - recall: 0.99 - ETA: 3s - loss: 0.0098 - acc: 0.9969 - precision: 0.9973 - recall: 0.99 - ETA: 3s - loss: 0.0099 - acc: 0.9967 - precision: 0.9970 - recall: 0.99 - ETA: 2s - loss: 0.0098 - acc: 0.9967 - precision: 0.9970 - recall: 0.99 - ETA: 2s - loss: 0.0098 - acc: 0.9968 - precision: 0.9971 - recall: 0.99 - ETA: 2s - loss: 0.0100 - acc: 0.9966 - precision: 0.9971 - recall: 0.99 - ETA: 2s - loss: 0.0098 - acc: 0.9966 - precision: 0.9972 - recall: 0.99 - ETA: 2s - loss: 0.0097 - acc: 0.9967 - precision: 0.9972 - recall: 0.99 - ETA: 2s - loss: 0.0096 - acc: 0.9968 - precision: 0.9973 - recall: 0.99 - ETA: 1s - loss: 0.0095 - acc: 0.9968 - precision: 0.9973 - recall: 0.99 - ETA: 1s - loss: 0.0097 - acc: 0.9966 - precision: 0.9971 - recall: 0.99 - ETA: 1s - loss: 0.0095 - acc: 0.9967 - precision: 0.9971 - recall: 0.99 - ETA: 1s - loss: 0.0094 - acc: 0.9967 - precision: 0.9972 - recall: 0.99 - ETA: 1s - loss: 0.0093 - acc: 0.9968 - precision: 0.9972 - recall: 0.99 - ETA: 1s - loss: 0.0091 - acc: 0.9968 - precision: 0.9973 - recall: 0.99 - ETA: 0s - loss: 0.0090 - acc: 0.9969 - precision: 0.9973 - recall: 0.99 - ETA: 0s - loss: 0.0089 - acc: 0.9969 - precision: 0.9973 - recall: 0.99 - ETA: 0s - loss: 0.0088 - acc: 0.9970 - precision: 0.9974 - recall: 0.99 - ETA: 0s - loss: 0.0095 - acc: 0.9968 - precision: 0.9971 - recall: 0.99 - ETA: 0s - loss: 0.0094 - acc: 0.9968 - precision: 0.9972 - recall: 0.99 - ETA: 0s - loss: 0.0093 - acc: 0.9969 - precision: 0.9972 - recall: 0.99 - 12s 3ms/step - loss: 0.0092 - acc: 0.9969 - precision: 0.9972 - recall: 0.9992 - val_loss: 0.0679 - val_acc: 0.9835 - val_precision: 0.9884 - val_recall: 0.9925\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.06038\n",
      "Epoch 19/25\n",
      "4180/4180 [==============================] - ETA: 9s - loss: 1.8971e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0122 - acc: 0.9922 - precision: 0.9907 - recall: 1.0000   - ETA: 9s - loss: 0.0103 - acc: 0.9948 - precision: 0.9939 - recall: 1.00 - ETA: 9s - loss: 0.0097 - acc: 0.9961 - precision: 0.9955 - recall: 1.00 - ETA: 9s - loss: 0.0083 - acc: 0.9969 - precision: 0.9964 - recall: 1.00 - ETA: 9s - loss: 0.0102 - acc: 0.9948 - precision: 0.9970 - recall: 0.99 - ETA: 9s - loss: 0.0088 - acc: 0.9955 - precision: 0.9974 - recall: 0.99 - ETA: 9s - loss: 0.0083 - acc: 0.9961 - precision: 0.9977 - recall: 0.99 - ETA: 9s - loss: 0.0080 - acc: 0.9965 - precision: 0.9980 - recall: 0.99 - ETA: 8s - loss: 0.0076 - acc: 0.9969 - precision: 0.9982 - recall: 0.99 - ETA: 8s - loss: 0.0144 - acc: 0.9929 - precision: 0.9934 - recall: 0.99 - ETA: 8s - loss: 0.0139 - acc: 0.9935 - precision: 0.9939 - recall: 0.99 - ETA: 8s - loss: 0.0131 - acc: 0.9940 - precision: 0.9944 - recall: 0.99 - ETA: 8s - loss: 0.0125 - acc: 0.9944 - precision: 0.9948 - recall: 0.99 - ETA: 7s - loss: 0.0118 - acc: 0.9948 - precision: 0.9952 - recall: 0.99 - ETA: 7s - loss: 0.0114 - acc: 0.9951 - precision: 0.9955 - recall: 0.99 - ETA: 7s - loss: 0.0128 - acc: 0.9945 - precision: 0.9947 - recall: 0.99 - ETA: 7s - loss: 0.0122 - acc: 0.9948 - precision: 0.9950 - recall: 0.99 - ETA: 7s - loss: 0.0119 - acc: 0.9951 - precision: 0.9952 - recall: 0.99 - ETA: 7s - loss: 0.0116 - acc: 0.9953 - precision: 0.9955 - recall: 0.99 - ETA: 7s - loss: 0.0153 - acc: 0.9933 - precision: 0.9940 - recall: 0.99 - ETA: 6s - loss: 0.0149 - acc: 0.9936 - precision: 0.9943 - recall: 0.99 - ETA: 6s - loss: 0.0145 - acc: 0.9939 - precision: 0.9945 - recall: 0.99 - ETA: 6s - loss: 0.0141 - acc: 0.9941 - precision: 0.9947 - recall: 0.99 - ETA: 6s - loss: 0.0136 - acc: 0.9944 - precision: 0.9950 - recall: 0.99 - ETA: 6s - loss: 0.0132 - acc: 0.9946 - precision: 0.9951 - recall: 0.99 - ETA: 6s - loss: 0.0129 - acc: 0.9948 - precision: 0.9953 - recall: 0.99 - ETA: 6s - loss: 0.0125 - acc: 0.9950 - precision: 0.9955 - recall: 0.99 - ETA: 5s - loss: 0.0123 - acc: 0.9952 - precision: 0.9957 - recall: 0.99 - ETA: 5s - loss: 0.0120 - acc: 0.9953 - precision: 0.9958 - recall: 0.99 - ETA: 5s - loss: 0.0118 - acc: 0.9955 - precision: 0.9959 - recall: 0.99 - ETA: 5s - loss: 0.0115 - acc: 0.9956 - precision: 0.9961 - recall: 0.99 - ETA: 5s - loss: 0.0113 - acc: 0.9957 - precision: 0.9962 - recall: 0.99 - ETA: 5s - loss: 0.0110 - acc: 0.9959 - precision: 0.9963 - recall: 0.99 - ETA: 4s - loss: 0.0110 - acc: 0.9960 - precision: 0.9964 - recall: 0.99 - ETA: 4s - loss: 0.0108 - acc: 0.9961 - precision: 0.9965 - recall: 0.99 - ETA: 4s - loss: 0.0105 - acc: 0.9962 - precision: 0.9966 - recall: 0.99 - ETA: 4s - loss: 0.0102 - acc: 0.9963 - precision: 0.9967 - recall: 0.99 - ETA: 4s - loss: 0.0101 - acc: 0.9964 - precision: 0.9968 - recall: 0.99 - ETA: 4s - loss: 0.0100 - acc: 0.9965 - precision: 0.9969 - recall: 0.99 - ETA: 3s - loss: 0.0098 - acc: 0.9966 - precision: 0.9969 - recall: 0.99 - ETA: 3s - loss: 0.0096 - acc: 0.9967 - precision: 0.9970 - recall: 0.99 - ETA: 3s - loss: 0.0094 - acc: 0.9967 - precision: 0.9971 - recall: 0.99 - ETA: 3s - loss: 0.0092 - acc: 0.9968 - precision: 0.9971 - recall: 0.99 - ETA: 3s - loss: 0.0090 - acc: 0.9969 - precision: 0.9972 - recall: 0.99 - ETA: 3s - loss: 0.0089 - acc: 0.9969 - precision: 0.9973 - recall: 0.99 - ETA: 2s - loss: 0.0087 - acc: 0.9970 - precision: 0.9973 - recall: 0.99 - ETA: 2s - loss: 0.0087 - acc: 0.9971 - precision: 0.9974 - recall: 0.99 - ETA: 2s - loss: 0.0087 - acc: 0.9971 - precision: 0.9974 - recall: 0.99 - ETA: 2s - loss: 0.0094 - acc: 0.9969 - precision: 0.9971 - recall: 0.99 - ETA: 2s - loss: 0.0093 - acc: 0.9969 - precision: 0.9972 - recall: 0.99 - ETA: 2s - loss: 0.0092 - acc: 0.9970 - precision: 0.9972 - recall: 0.99 - ETA: 2s - loss: 0.0090 - acc: 0.9971 - precision: 0.9973 - recall: 0.99 - ETA: 1s - loss: 0.0091 - acc: 0.9968 - precision: 0.9970 - recall: 0.99 - ETA: 1s - loss: 0.0091 - acc: 0.9969 - precision: 0.9971 - recall: 0.99 - ETA: 1s - loss: 0.0090 - acc: 0.9969 - precision: 0.9971 - recall: 0.99 - ETA: 1s - loss: 0.0088 - acc: 0.9970 - precision: 0.9972 - recall: 0.99 - ETA: 1s - loss: 0.0088 - acc: 0.9970 - precision: 0.9972 - recall: 0.99 - ETA: 1s - loss: 0.0087 - acc: 0.9971 - precision: 0.9972 - recall: 0.99 - ETA: 0s - loss: 0.0086 - acc: 0.9971 - precision: 0.9973 - recall: 0.99 - ETA: 0s - loss: 0.0085 - acc: 0.9972 - precision: 0.9973 - recall: 0.99 - ETA: 0s - loss: 0.0084 - acc: 0.9972 - precision: 0.9974 - recall: 0.99 - ETA: 0s - loss: 0.0082 - acc: 0.9973 - precision: 0.9974 - recall: 0.99 - ETA: 0s - loss: 0.0082 - acc: 0.9973 - precision: 0.9975 - recall: 0.99 - ETA: 0s - loss: 0.0081 - acc: 0.9974 - precision: 0.9975 - recall: 0.99 - 12s 3ms/step - loss: 0.0081 - acc: 0.9974 - precision: 0.9975 - recall: 0.9994 - val_loss: 0.0946 - val_acc: 0.9813 - val_precision: 0.9819 - val_recall: 0.9967\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.06038\n",
      "Epoch 20/25\n",
      "4180/4180 [==============================] - ETA: 11s - loss: 1.9656e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 5.1298e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0033 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000    - ETA: 10s - loss: 0.0025 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0041 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0067 - acc: 0.9974 - precision: 1.0000 - recall: 0.997 - ETA: 9s - loss: 0.0060 - acc: 0.9978 - precision: 1.0000 - recall: 0.997 - ETA: 9s - loss: 0.0058 - acc: 0.9980 - precision: 1.0000 - recall: 0.99 - ETA: 9s - loss: 0.0054 - acc: 0.9983 - precision: 1.0000 - recall: 0.99 - ETA: 9s - loss: 0.0050 - acc: 0.9984 - precision: 1.0000 - recall: 0.99 - ETA: 9s - loss: 0.0046 - acc: 0.9986 - precision: 1.0000 - recall: 0.99 - ETA: 9s - loss: 0.0045 - acc: 0.9987 - precision: 1.0000 - recall: 0.99 - ETA: 8s - loss: 0.0042 - acc: 0.9988 - precision: 1.0000 - recall: 0.99 - ETA: 8s - loss: 0.0065 - acc: 0.9978 - precision: 0.9987 - recall: 0.99 - ETA: 8s - loss: 0.0068 - acc: 0.9979 - precision: 0.9988 - recall: 0.99 - ETA: 8s - loss: 0.0065 - acc: 0.9980 - precision: 0.9989 - recall: 0.99 - ETA: 8s - loss: 0.0062 - acc: 0.9982 - precision: 0.9990 - recall: 0.99 - ETA: 8s - loss: 0.0072 - acc: 0.9974 - precision: 0.9980 - recall: 0.99 - ETA: 8s - loss: 0.0071 - acc: 0.9975 - precision: 0.9981 - recall: 0.99 - ETA: 7s - loss: 0.0068 - acc: 0.9977 - precision: 0.9982 - recall: 0.99 - ETA: 7s - loss: 0.0064 - acc: 0.9978 - precision: 0.9983 - recall: 0.99 - ETA: 7s - loss: 0.0062 - acc: 0.9979 - precision: 0.9984 - recall: 0.99 - ETA: 7s - loss: 0.0060 - acc: 0.9980 - precision: 0.9985 - recall: 0.99 - ETA: 7s - loss: 0.0067 - acc: 0.9974 - precision: 0.9978 - recall: 0.99 - ETA: 7s - loss: 0.0085 - acc: 0.9969 - precision: 0.9979 - recall: 0.99 - ETA: 7s - loss: 0.0083 - acc: 0.9970 - precision: 0.9979 - recall: 0.99 - ETA: 7s - loss: 0.0104 - acc: 0.9959 - precision: 0.9967 - recall: 0.99 - ETA: 6s - loss: 0.0104 - acc: 0.9961 - precision: 0.9968 - recall: 0.99 - ETA: 6s - loss: 0.0101 - acc: 0.9962 - precision: 0.9969 - recall: 0.99 - ETA: 6s - loss: 0.0098 - acc: 0.9964 - precision: 0.9970 - recall: 0.99 - ETA: 6s - loss: 0.0101 - acc: 0.9960 - precision: 0.9966 - recall: 0.99 - ETA: 6s - loss: 0.0099 - acc: 0.9961 - precision: 0.9967 - recall: 0.99 - ETA: 6s - loss: 0.0097 - acc: 0.9962 - precision: 0.9968 - recall: 0.99 - ETA: 5s - loss: 0.0095 - acc: 0.9963 - precision: 0.9969 - recall: 0.99 - ETA: 5s - loss: 0.0093 - acc: 0.9964 - precision: 0.9970 - recall: 0.99 - ETA: 5s - loss: 0.0091 - acc: 0.9965 - precision: 0.9970 - recall: 0.99 - ETA: 5s - loss: 0.0089 - acc: 0.9966 - precision: 0.9971 - recall: 0.99 - ETA: 5s - loss: 0.0087 - acc: 0.9967 - precision: 0.9972 - recall: 0.99 - ETA: 4s - loss: 0.0085 - acc: 0.9968 - precision: 0.9973 - recall: 0.99 - ETA: 4s - loss: 0.0083 - acc: 0.9969 - precision: 0.9973 - recall: 0.99 - ETA: 4s - loss: 0.0081 - acc: 0.9970 - precision: 0.9974 - recall: 0.99 - ETA: 4s - loss: 0.0079 - acc: 0.9970 - precision: 0.9975 - recall: 0.99 - ETA: 4s - loss: 0.0078 - acc: 0.9971 - precision: 0.9975 - recall: 0.99 - ETA: 4s - loss: 0.0078 - acc: 0.9972 - precision: 0.9976 - recall: 0.99 - ETA: 3s - loss: 0.0076 - acc: 0.9972 - precision: 0.9976 - recall: 0.99 - ETA: 3s - loss: 0.0075 - acc: 0.9973 - precision: 0.9977 - recall: 0.99 - ETA: 3s - loss: 0.0076 - acc: 0.9970 - precision: 0.9974 - recall: 0.99 - ETA: 3s - loss: 0.0075 - acc: 0.9971 - precision: 0.9974 - recall: 0.99 - ETA: 3s - loss: 0.0074 - acc: 0.9971 - precision: 0.9975 - recall: 0.99 - ETA: 2s - loss: 0.0074 - acc: 0.9972 - precision: 0.9975 - recall: 0.99 - ETA: 2s - loss: 0.0073 - acc: 0.9972 - precision: 0.9975 - recall: 0.99 - ETA: 2s - loss: 0.0072 - acc: 0.9973 - precision: 0.9976 - recall: 0.99 - ETA: 2s - loss: 0.0071 - acc: 0.9973 - precision: 0.9976 - recall: 0.99 - ETA: 2s - loss: 0.0070 - acc: 0.9974 - precision: 0.9977 - recall: 0.99 - ETA: 1s - loss: 0.0072 - acc: 0.9972 - precision: 0.9974 - recall: 0.99 - ETA: 1s - loss: 0.0071 - acc: 0.9972 - precision: 0.9974 - recall: 0.99 - ETA: 1s - loss: 0.0072 - acc: 0.9973 - precision: 0.9975 - recall: 0.99 - ETA: 1s - loss: 0.0072 - acc: 0.9973 - precision: 0.9975 - recall: 0.99 - ETA: 1s - loss: 0.0074 - acc: 0.9971 - precision: 0.9976 - recall: 0.99 - ETA: 0s - loss: 0.0073 - acc: 0.9971 - precision: 0.9976 - recall: 0.99 - ETA: 0s - loss: 0.0072 - acc: 0.9972 - precision: 0.9976 - recall: 0.99 - ETA: 0s - loss: 0.0071 - acc: 0.9972 - precision: 0.9977 - recall: 0.99 - ETA: 0s - loss: 0.0076 - acc: 0.9970 - precision: 0.9974 - recall: 0.99 - ETA: 0s - loss: 0.0078 - acc: 0.9971 - precision: 0.9975 - recall: 0.99 - ETA: 0s - loss: 0.0077 - acc: 0.9971 - precision: 0.9975 - recall: 0.99 - 13s 3ms/step - loss: 0.0077 - acc: 0.9971 - precision: 0.9975 - recall: 0.9992 - val_loss: 0.0819 - val_acc: 0.9828 - val_precision: 0.9875 - val_recall: 0.9925\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.06038\n",
      "Epoch 21/25\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 0.0014 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0035 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0025 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0022 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0018 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0018 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0016 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0027 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0025 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0022 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0035 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0046 - acc: 0.9987 - precision: 0.9985 - recall: 1.000 - ETA: 9s - loss: 0.0044 - acc: 0.9988 - precision: 0.9986 - recall: 1.00 - ETA: 9s - loss: 0.0042 - acc: 0.9989 - precision: 0.9987 - recall: 1.00 - ETA: 9s - loss: 0.0039 - acc: 0.9990 - precision: 0.9988 - recall: 1.00 - ETA: 9s - loss: 0.0037 - acc: 0.9990 - precision: 0.9989 - recall: 1.00 - ETA: 8s - loss: 0.0035 - acc: 0.9991 - precision: 0.9989 - recall: 1.00 - ETA: 8s - loss: 0.0033 - acc: 0.9991 - precision: 0.9990 - recall: 1.00 - ETA: 8s - loss: 0.0032 - acc: 0.9992 - precision: 0.9991 - recall: 1.00 - ETA: 8s - loss: 0.0031 - acc: 0.9992 - precision: 0.9991 - recall: 1.00 - ETA: 8s - loss: 0.0030 - acc: 0.9993 - precision: 0.9991 - recall: 1.00 - ETA: 8s - loss: 0.0036 - acc: 0.9993 - precision: 0.9992 - recall: 1.00 - ETA: 8s - loss: 0.0035 - acc: 0.9993 - precision: 0.9992 - recall: 1.00 - ETA: 7s - loss: 0.0034 - acc: 0.9993 - precision: 0.9993 - recall: 1.00 - ETA: 7s - loss: 0.0033 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 7s - loss: 0.0049 - acc: 0.9988 - precision: 0.9986 - recall: 1.00 - ETA: 7s - loss: 0.0048 - acc: 0.9988 - precision: 0.9987 - recall: 1.00 - ETA: 7s - loss: 0.0049 - acc: 0.9989 - precision: 0.9987 - recall: 1.00 - ETA: 6s - loss: 0.0047 - acc: 0.9989 - precision: 0.9988 - recall: 1.00 - ETA: 6s - loss: 0.0046 - acc: 0.9990 - precision: 0.9988 - recall: 1.00 - ETA: 6s - loss: 0.0044 - acc: 0.9990 - precision: 0.9988 - recall: 1.00 - ETA: 6s - loss: 0.0043 - acc: 0.9990 - precision: 0.9989 - recall: 1.00 - ETA: 6s - loss: 0.0044 - acc: 0.9991 - precision: 0.9989 - recall: 1.00 - ETA: 5s - loss: 0.0044 - acc: 0.9991 - precision: 0.9989 - recall: 1.00 - ETA: 5s - loss: 0.0044 - acc: 0.9991 - precision: 0.9990 - recall: 1.00 - ETA: 5s - loss: 0.0043 - acc: 0.9991 - precision: 0.9990 - recall: 1.00 - ETA: 5s - loss: 0.0050 - acc: 0.9987 - precision: 0.9990 - recall: 0.99 - ETA: 5s - loss: 0.0049 - acc: 0.9988 - precision: 0.9991 - recall: 0.99 - ETA: 5s - loss: 0.0058 - acc: 0.9984 - precision: 0.9986 - recall: 0.99 - ETA: 4s - loss: 0.0067 - acc: 0.9980 - precision: 0.9982 - recall: 0.99 - ETA: 4s - loss: 0.0067 - acc: 0.9981 - precision: 0.9982 - recall: 0.99 - ETA: 4s - loss: 0.0066 - acc: 0.9981 - precision: 0.9983 - recall: 0.99 - ETA: 4s - loss: 0.0065 - acc: 0.9982 - precision: 0.9983 - recall: 0.99 - ETA: 4s - loss: 0.0065 - acc: 0.9982 - precision: 0.9984 - recall: 0.99 - ETA: 3s - loss: 0.0065 - acc: 0.9983 - precision: 0.9984 - recall: 0.99 - ETA: 3s - loss: 0.0064 - acc: 0.9983 - precision: 0.9984 - recall: 0.99 - ETA: 3s - loss: 0.0063 - acc: 0.9983 - precision: 0.9985 - recall: 0.99 - ETA: 3s - loss: 0.0070 - acc: 0.9980 - precision: 0.9981 - recall: 0.99 - ETA: 3s - loss: 0.0071 - acc: 0.9981 - precision: 0.9982 - recall: 0.99 - ETA: 2s - loss: 0.0074 - acc: 0.9978 - precision: 0.9982 - recall: 0.99 - ETA: 2s - loss: 0.0074 - acc: 0.9979 - precision: 0.9982 - recall: 0.99 - ETA: 2s - loss: 0.0073 - acc: 0.9979 - precision: 0.9983 - recall: 0.99 - ETA: 2s - loss: 0.0073 - acc: 0.9979 - precision: 0.9983 - recall: 0.99 - ETA: 2s - loss: 0.0072 - acc: 0.9980 - precision: 0.9983 - recall: 0.99 - ETA: 1s - loss: 0.0074 - acc: 0.9977 - precision: 0.9980 - recall: 0.99 - ETA: 1s - loss: 0.0073 - acc: 0.9978 - precision: 0.9981 - recall: 0.99 - ETA: 1s - loss: 0.0074 - acc: 0.9978 - precision: 0.9981 - recall: 0.99 - ETA: 1s - loss: 0.0073 - acc: 0.9978 - precision: 0.9981 - recall: 0.99 - ETA: 1s - loss: 0.0072 - acc: 0.9979 - precision: 0.9982 - recall: 0.99 - ETA: 1s - loss: 0.0071 - acc: 0.9979 - precision: 0.9982 - recall: 0.99 - ETA: 0s - loss: 0.0071 - acc: 0.9980 - precision: 0.9982 - recall: 0.99 - ETA: 0s - loss: 0.0069 - acc: 0.9980 - precision: 0.9983 - recall: 0.99 - ETA: 0s - loss: 0.0069 - acc: 0.9980 - precision: 0.9983 - recall: 0.99 - ETA: 0s - loss: 0.0068 - acc: 0.9980 - precision: 0.9983 - recall: 0.99 - ETA: 0s - loss: 0.0067 - acc: 0.9981 - precision: 0.9983 - recall: 0.99 - 13s 3ms/step - loss: 0.0067 - acc: 0.9981 - precision: 0.9983 - recall: 0.9994 - val_loss: 0.0863 - val_acc: 0.9849 - val_precision: 0.9884 - val_recall: 0.9942\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.06038\n",
      "Epoch 22/25\n",
      "4180/4180 [==============================] - ETA: 12s - loss: 0.0075 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0053 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0041 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0043 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0037 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0032 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0028 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0025 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0022 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0023 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0021 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0020 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0018 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0018 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0018 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0017 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0016 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0016 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0017 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0016 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0017 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0017 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0018 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0017 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0036 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 7s - loss: 0.0055 - acc: 0.9988 - precision: 0.9993 - recall: 0.99 - ETA: 7s - loss: 0.0071 - acc: 0.9983 - precision: 0.9987 - recall: 0.99 - ETA: 7s - loss: 0.0069 - acc: 0.9983 - precision: 0.9987 - recall: 0.99 - ETA: 6s - loss: 0.0067 - acc: 0.9984 - precision: 0.9988 - recall: 0.99 - ETA: 6s - loss: 0.0065 - acc: 0.9984 - precision: 0.9988 - recall: 0.99 - ETA: 6s - loss: 0.0063 - acc: 0.9985 - precision: 0.9988 - recall: 0.99 - ETA: 6s - loss: 0.0062 - acc: 0.9985 - precision: 0.9989 - recall: 0.99 - ETA: 6s - loss: 0.0060 - acc: 0.9986 - precision: 0.9989 - recall: 0.99 - ETA: 6s - loss: 0.0059 - acc: 0.9986 - precision: 0.9989 - recall: 0.99 - ETA: 5s - loss: 0.0057 - acc: 0.9987 - precision: 0.9990 - recall: 0.99 - ETA: 5s - loss: 0.0056 - acc: 0.9987 - precision: 0.9990 - recall: 0.99 - ETA: 5s - loss: 0.0054 - acc: 0.9987 - precision: 0.9990 - recall: 0.99 - ETA: 5s - loss: 0.0053 - acc: 0.9988 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0055 - acc: 0.9984 - precision: 0.9986 - recall: 0.99 - ETA: 4s - loss: 0.0054 - acc: 0.9984 - precision: 0.9986 - recall: 0.99 - ETA: 4s - loss: 0.0054 - acc: 0.9985 - precision: 0.9987 - recall: 0.99 - ETA: 4s - loss: 0.0052 - acc: 0.9985 - precision: 0.9987 - recall: 0.99 - ETA: 4s - loss: 0.0052 - acc: 0.9985 - precision: 0.9987 - recall: 0.99 - ETA: 3s - loss: 0.0051 - acc: 0.9986 - precision: 0.9988 - recall: 0.99 - ETA: 3s - loss: 0.0051 - acc: 0.9986 - precision: 0.9988 - recall: 0.99 - ETA: 3s - loss: 0.0050 - acc: 0.9986 - precision: 0.9988 - recall: 0.99 - ETA: 3s - loss: 0.0050 - acc: 0.9987 - precision: 0.9988 - recall: 0.99 - ETA: 3s - loss: 0.0050 - acc: 0.9987 - precision: 0.9989 - recall: 0.99 - ETA: 3s - loss: 0.0050 - acc: 0.9987 - precision: 0.9989 - recall: 0.99 - ETA: 2s - loss: 0.0050 - acc: 0.9988 - precision: 0.9989 - recall: 0.99 - ETA: 2s - loss: 0.0049 - acc: 0.9988 - precision: 0.9989 - recall: 0.99 - ETA: 2s - loss: 0.0053 - acc: 0.9985 - precision: 0.9986 - recall: 0.99 - ETA: 2s - loss: 0.0055 - acc: 0.9982 - precision: 0.9986 - recall: 0.99 - ETA: 2s - loss: 0.0054 - acc: 0.9983 - precision: 0.9987 - recall: 0.99 - ETA: 1s - loss: 0.0053 - acc: 0.9983 - precision: 0.9987 - recall: 0.99 - ETA: 1s - loss: 0.0053 - acc: 0.9983 - precision: 0.9987 - recall: 0.99 - ETA: 1s - loss: 0.0052 - acc: 0.9984 - precision: 0.9987 - recall: 0.99 - ETA: 1s - loss: 0.0051 - acc: 0.9984 - precision: 0.9988 - recall: 0.99 - ETA: 1s - loss: 0.0051 - acc: 0.9984 - precision: 0.9988 - recall: 0.99 - ETA: 0s - loss: 0.0050 - acc: 0.9984 - precision: 0.9988 - recall: 0.99 - ETA: 0s - loss: 0.0050 - acc: 0.9985 - precision: 0.9988 - recall: 0.99 - ETA: 0s - loss: 0.0054 - acc: 0.9982 - precision: 0.9986 - recall: 0.99 - ETA: 0s - loss: 0.0053 - acc: 0.9983 - precision: 0.9986 - recall: 0.99 - ETA: 0s - loss: 0.0053 - acc: 0.9983 - precision: 0.9986 - recall: 0.99 - ETA: 0s - loss: 0.0054 - acc: 0.9981 - precision: 0.9983 - recall: 0.99 - 13s 3ms/step - loss: 0.0054 - acc: 0.9981 - precision: 0.9983 - recall: 0.9994 - val_loss: 0.0965 - val_acc: 0.9821 - val_precision: 0.9949 - val_recall: 0.9841\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.06038\n",
      "Epoch 23/25\n",
      "4180/4180 [==============================] - ETA: 7s - loss: 6.5269e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0011 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000   - ETA: 9s - loss: 0.0026 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 10s - loss: 0.0022 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0018 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0019 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0019 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0017 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0016 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0016 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0015 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0015 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0014 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0013 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0012 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0012 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0011 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0011 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0011 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0010 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0010 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0010 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 9.7637e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 9.4158e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 9.4984e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 9.1879e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 8.8568e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 8.5468e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 8.2775e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 8.6484e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 8.6803e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 8.5808e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 5s - loss: 0.0012 - acc: 0.9995 - precision: 0.9995 - recall: 1.0000   - ETA: 5s - loss: 0.0013 - acc: 0.9995 - precision: 0.9995 - recall: 1.00 - ETA: 5s - loss: 0.0013 - acc: 0.9996 - precision: 0.9995 - recall: 1.00 - ETA: 5s - loss: 0.0012 - acc: 0.9996 - precision: 0.9995 - recall: 1.00 - ETA: 5s - loss: 0.0012 - acc: 0.9996 - precision: 0.9995 - recall: 1.00 - ETA: 4s - loss: 0.0012 - acc: 0.9996 - precision: 0.9995 - recall: 1.00 - ETA: 4s - loss: 0.0012 - acc: 0.9996 - precision: 0.9995 - recall: 1.00 - ETA: 4s - loss: 0.0015 - acc: 0.9992 - precision: 0.9991 - recall: 1.00 - ETA: 4s - loss: 0.0015 - acc: 0.9992 - precision: 0.9991 - recall: 1.00 - ETA: 4s - loss: 0.0015 - acc: 0.9993 - precision: 0.9991 - recall: 1.00 - ETA: 4s - loss: 0.0014 - acc: 0.9993 - precision: 0.9992 - recall: 1.00 - ETA: 3s - loss: 0.0014 - acc: 0.9993 - precision: 0.9992 - recall: 1.00 - ETA: 3s - loss: 0.0014 - acc: 0.9993 - precision: 0.9992 - recall: 1.00 - ETA: 3s - loss: 0.0014 - acc: 0.9993 - precision: 0.9992 - recall: 1.00 - ETA: 3s - loss: 0.0014 - acc: 0.9993 - precision: 0.9992 - recall: 1.00 - ETA: 3s - loss: 0.0014 - acc: 0.9993 - precision: 0.9993 - recall: 1.00 - ETA: 3s - loss: 0.0015 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 2s - loss: 0.0016 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 2s - loss: 0.0017 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 2s - loss: 0.0018 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 2s - loss: 0.0018 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 2s - loss: 0.0018 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 1s - loss: 0.0023 - acc: 0.9991 - precision: 0.9990 - recall: 1.00 - ETA: 1s - loss: 0.0024 - acc: 0.9989 - precision: 0.9990 - recall: 0.99 - ETA: 1s - loss: 0.0025 - acc: 0.9989 - precision: 0.9991 - recall: 0.99 - ETA: 1s - loss: 0.0026 - acc: 0.9989 - precision: 0.9991 - recall: 0.99 - ETA: 1s - loss: 0.0025 - acc: 0.9989 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0028 - acc: 0.9987 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0031 - acc: 0.9985 - precision: 0.9988 - recall: 0.99 - ETA: 0s - loss: 0.0031 - acc: 0.9985 - precision: 0.9988 - recall: 0.99 - ETA: 0s - loss: 0.0031 - acc: 0.9985 - precision: 0.9989 - recall: 0.99 - ETA: 0s - loss: 0.0031 - acc: 0.9985 - precision: 0.9989 - recall: 0.99 - ETA: 0s - loss: 0.0031 - acc: 0.9986 - precision: 0.9989 - recall: 0.99 - 13s 3ms/step - loss: 0.0031 - acc: 0.9986 - precision: 0.9989 - recall: 0.9994 - val_loss: 0.0829 - val_acc: 0.9842 - val_precision: 0.9876 - val_recall: 0.9942\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.06038\n",
      "Epoch 24/25\n",
      "4180/4180 [==============================] - ETA: 15s - loss: 5.4676e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 14s - loss: 0.0014 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000    - ETA: 13s - loss: 0.0020 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 13s - loss: 0.0015 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0013 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0015 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 12s - loss: 0.0036 - acc: 0.9978 - precision: 0.9974 - recall: 1.000 - ETA: 11s - loss: 0.0044 - acc: 0.9980 - precision: 0.9977 - recall: 1.000 - ETA: 11s - loss: 0.0039 - acc: 0.9983 - precision: 0.9980 - recall: 1.000 - ETA: 11s - loss: 0.0036 - acc: 0.9984 - precision: 0.9982 - recall: 1.000 - ETA: 10s - loss: 0.0039 - acc: 0.9986 - precision: 0.9984 - recall: 1.000 - ETA: 10s - loss: 0.0036 - acc: 0.9987 - precision: 0.9985 - recall: 1.000 - ETA: 10s - loss: 0.0033 - acc: 0.9988 - precision: 0.9986 - recall: 1.000 - ETA: 10s - loss: 0.0032 - acc: 0.9989 - precision: 0.9987 - recall: 1.000 - ETA: 9s - loss: 0.0033 - acc: 0.9990 - precision: 0.9988 - recall: 1.000 - ETA: 9s - loss: 0.0031 - acc: 0.9990 - precision: 0.9989 - recall: 1.00 - ETA: 9s - loss: 0.0030 - acc: 0.9991 - precision: 0.9989 - recall: 1.00 - ETA: 9s - loss: 0.0028 - acc: 0.9991 - precision: 0.9990 - recall: 1.00 - ETA: 9s - loss: 0.0027 - acc: 0.9992 - precision: 0.9990 - recall: 1.00 - ETA: 8s - loss: 0.0026 - acc: 0.9992 - precision: 0.9991 - recall: 1.00 - ETA: 8s - loss: 0.0025 - acc: 0.9993 - precision: 0.9991 - recall: 1.00 - ETA: 8s - loss: 0.0032 - acc: 0.9986 - precision: 0.9984 - recall: 1.00 - ETA: 8s - loss: 0.0031 - acc: 0.9986 - precision: 0.9984 - recall: 1.00 - ETA: 8s - loss: 0.0031 - acc: 0.9987 - precision: 0.9985 - recall: 1.00 - ETA: 7s - loss: 0.0030 - acc: 0.9988 - precision: 0.9985 - recall: 1.00 - ETA: 7s - loss: 0.0029 - acc: 0.9988 - precision: 0.9986 - recall: 1.00 - ETA: 7s - loss: 0.0028 - acc: 0.9988 - precision: 0.9987 - recall: 1.00 - ETA: 7s - loss: 0.0027 - acc: 0.9989 - precision: 0.9987 - recall: 1.00 - ETA: 7s - loss: 0.0027 - acc: 0.9989 - precision: 0.9988 - recall: 1.00 - ETA: 6s - loss: 0.0026 - acc: 0.9990 - precision: 0.9988 - recall: 1.00 - ETA: 6s - loss: 0.0025 - acc: 0.9990 - precision: 0.9988 - recall: 1.00 - ETA: 6s - loss: 0.0025 - acc: 0.9990 - precision: 0.9989 - recall: 1.00 - ETA: 6s - loss: 0.0024 - acc: 0.9991 - precision: 0.9989 - recall: 1.00 - ETA: 5s - loss: 0.0024 - acc: 0.9991 - precision: 0.9989 - recall: 1.00 - ETA: 5s - loss: 0.0023 - acc: 0.9991 - precision: 0.9990 - recall: 1.00 - ETA: 5s - loss: 0.0025 - acc: 0.9991 - precision: 0.9990 - recall: 1.00 - ETA: 5s - loss: 0.0061 - acc: 0.9987 - precision: 0.9990 - recall: 0.99 - ETA: 5s - loss: 0.0060 - acc: 0.9988 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0059 - acc: 0.9988 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0059 - acc: 0.9988 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0059 - acc: 0.9989 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0058 - acc: 0.9989 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0056 - acc: 0.9989 - precision: 0.9992 - recall: 0.99 - ETA: 3s - loss: 0.0056 - acc: 0.9989 - precision: 0.9992 - recall: 0.99 - ETA: 3s - loss: 0.0055 - acc: 0.9990 - precision: 0.9992 - recall: 0.99 - ETA: 3s - loss: 0.0054 - acc: 0.9990 - precision: 0.9992 - recall: 0.99 - ETA: 3s - loss: 0.0053 - acc: 0.9990 - precision: 0.9992 - recall: 0.99 - ETA: 3s - loss: 0.0054 - acc: 0.9990 - precision: 0.9992 - recall: 0.99 - ETA: 2s - loss: 0.0053 - acc: 0.9990 - precision: 0.9993 - recall: 0.99 - ETA: 2s - loss: 0.0052 - acc: 0.9991 - precision: 0.9993 - recall: 0.99 - ETA: 2s - loss: 0.0052 - acc: 0.9991 - precision: 0.9993 - recall: 0.99 - ETA: 2s - loss: 0.0051 - acc: 0.9991 - precision: 0.9993 - recall: 0.99 - ETA: 2s - loss: 0.0050 - acc: 0.9991 - precision: 0.9993 - recall: 0.99 - ETA: 2s - loss: 0.0049 - acc: 0.9991 - precision: 0.9993 - recall: 0.99 - ETA: 1s - loss: 0.0048 - acc: 0.9991 - precision: 0.9993 - recall: 0.99 - ETA: 1s - loss: 0.0048 - acc: 0.9992 - precision: 0.9994 - recall: 0.99 - ETA: 1s - loss: 0.0047 - acc: 0.9992 - precision: 0.9994 - recall: 0.99 - ETA: 1s - loss: 0.0047 - acc: 0.9992 - precision: 0.9994 - recall: 0.99 - ETA: 1s - loss: 0.0048 - acc: 0.9989 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0048 - acc: 0.9990 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0047 - acc: 0.9990 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0047 - acc: 0.9990 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0047 - acc: 0.9990 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0047 - acc: 0.9990 - precision: 0.9992 - recall: 0.99 - ETA: 0s - loss: 0.0047 - acc: 0.9990 - precision: 0.9992 - recall: 0.99 - 13s 3ms/step - loss: 0.0047 - acc: 0.9990 - precision: 0.9992 - recall: 0.9997 - val_loss: 0.0783 - val_acc: 0.9828 - val_precision: 0.9924 - val_recall: 0.9875\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.06038\n",
      "Epoch 25/25\n",
      "4180/4180 [==============================] - ETA: 9s - loss: 7.8842e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0011 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000   - ETA: 10s - loss: 7.8286e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 6.4672e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 8.6529e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 8.3367e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 9.4943e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 8.8134e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 9.0296e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0031 - acc: 0.9984 - precision: 0.9982 - recall: 1.0000    - ETA: 10s - loss: 0.0028 - acc: 0.9986 - precision: 0.9984 - recall: 1.000 - ETA: 10s - loss: 0.0026 - acc: 0.9987 - precision: 0.9985 - recall: 1.000 - ETA: 10s - loss: 0.0025 - acc: 0.9988 - precision: 0.9986 - recall: 1.000 - ETA: 9s - loss: 0.0023 - acc: 0.9989 - precision: 0.9987 - recall: 1.000 - ETA: 9s - loss: 0.0022 - acc: 0.9990 - precision: 0.9988 - recall: 1.00 - ETA: 9s - loss: 0.0021 - acc: 0.9990 - precision: 0.9989 - recall: 1.00 - ETA: 9s - loss: 0.0023 - acc: 0.9991 - precision: 0.9990 - recall: 1.00 - ETA: 8s - loss: 0.0021 - acc: 0.9991 - precision: 0.9990 - recall: 1.00 - ETA: 8s - loss: 0.0020 - acc: 0.9992 - precision: 0.9991 - recall: 1.00 - ETA: 8s - loss: 0.0019 - acc: 0.9992 - precision: 0.9991 - recall: 1.00 - ETA: 8s - loss: 0.0019 - acc: 0.9993 - precision: 0.9992 - recall: 1.00 - ETA: 8s - loss: 0.0022 - acc: 0.9993 - precision: 0.9992 - recall: 1.00 - ETA: 8s - loss: 0.0021 - acc: 0.9993 - precision: 0.9992 - recall: 1.00 - ETA: 7s - loss: 0.0020 - acc: 0.9993 - precision: 0.9993 - recall: 1.00 - ETA: 7s - loss: 0.0021 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 7s - loss: 0.0021 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 7s - loss: 0.0020 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 7s - loss: 0.0020 - acc: 0.9994 - precision: 0.9994 - recall: 1.00 - ETA: 6s - loss: 0.0028 - acc: 0.9989 - precision: 0.9988 - recall: 1.00 - ETA: 6s - loss: 0.0041 - acc: 0.9984 - precision: 0.9988 - recall: 0.99 - ETA: 6s - loss: 0.0040 - acc: 0.9985 - precision: 0.9988 - recall: 0.99 - ETA: 6s - loss: 0.0039 - acc: 0.9985 - precision: 0.9989 - recall: 0.99 - ETA: 6s - loss: 0.0038 - acc: 0.9986 - precision: 0.9989 - recall: 0.99 - ETA: 5s - loss: 0.0037 - acc: 0.9986 - precision: 0.9989 - recall: 0.99 - ETA: 5s - loss: 0.0037 - acc: 0.9987 - precision: 0.9990 - recall: 0.99 - ETA: 5s - loss: 0.0036 - acc: 0.9987 - precision: 0.9990 - recall: 0.99 - ETA: 5s - loss: 0.0035 - acc: 0.9987 - precision: 0.9990 - recall: 0.99 - ETA: 5s - loss: 0.0035 - acc: 0.9988 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0034 - acc: 0.9988 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0034 - acc: 0.9988 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0043 - acc: 0.9985 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0043 - acc: 0.9985 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0071 - acc: 0.9978 - precision: 0.9983 - recall: 0.99 - ETA: 3s - loss: 0.0072 - acc: 0.9975 - precision: 0.9980 - recall: 0.99 - ETA: 3s - loss: 0.0081 - acc: 0.9969 - precision: 0.9972 - recall: 0.99 - ETA: 3s - loss: 0.0083 - acc: 0.9966 - precision: 0.9973 - recall: 0.99 - ETA: 3s - loss: 0.0081 - acc: 0.9967 - precision: 0.9973 - recall: 0.99 - ETA: 3s - loss: 0.0080 - acc: 0.9967 - precision: 0.9974 - recall: 0.99 - ETA: 2s - loss: 0.0079 - acc: 0.9968 - precision: 0.9974 - recall: 0.99 - ETA: 2s - loss: 0.0077 - acc: 0.9969 - precision: 0.9975 - recall: 0.99 - ETA: 2s - loss: 0.0078 - acc: 0.9969 - precision: 0.9975 - recall: 0.99 - ETA: 2s - loss: 0.0077 - acc: 0.9970 - precision: 0.9976 - recall: 0.99 - ETA: 2s - loss: 0.0076 - acc: 0.9971 - precision: 0.9976 - recall: 0.99 - ETA: 2s - loss: 0.0074 - acc: 0.9971 - precision: 0.9977 - recall: 0.99 - ETA: 1s - loss: 0.0073 - acc: 0.9972 - precision: 0.9977 - recall: 0.99 - ETA: 1s - loss: 0.0072 - acc: 0.9972 - precision: 0.9978 - recall: 0.99 - ETA: 1s - loss: 0.0072 - acc: 0.9973 - precision: 0.9978 - recall: 0.99 - ETA: 1s - loss: 0.0071 - acc: 0.9973 - precision: 0.9978 - recall: 0.99 - ETA: 1s - loss: 0.0070 - acc: 0.9974 - precision: 0.9979 - recall: 0.99 - ETA: 0s - loss: 0.0069 - acc: 0.9974 - precision: 0.9979 - recall: 0.99 - ETA: 0s - loss: 0.0068 - acc: 0.9974 - precision: 0.9979 - recall: 0.99 - ETA: 0s - loss: 0.0067 - acc: 0.9975 - precision: 0.9980 - recall: 0.99 - ETA: 0s - loss: 0.0066 - acc: 0.9975 - precision: 0.9980 - recall: 0.99 - ETA: 0s - loss: 0.0065 - acc: 0.9976 - precision: 0.9980 - recall: 0.99 - ETA: 0s - loss: 0.0064 - acc: 0.9976 - precision: 0.9981 - recall: 0.99 - 12s 3ms/step - loss: 0.0064 - acc: 0.9976 - precision: 0.9981 - recall: 0.9992 - val_loss: 0.0845 - val_acc: 0.9835 - val_precision: 0.9876 - val_recall: 0.9933\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.06038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a6466d1ba8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize our ModelCheckpoint and TensorBoard callbacks\n",
    "# model checkpoint for saving best weights\n",
    "model_checkpoint = ModelCheckpoint(\"results/spam_classifier_{val_loss:.2f}\", save_best_only=True,\n",
    "                                    verbose=1)\n",
    "# for better visualization\n",
    "tensorboard = TensorBoard(f\"logs/spam_classifier_{time.time()}\")\n",
    "# print our data shapes\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)\n",
    "# train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "          batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "          callbacks=[tensorboard, model_checkpoint],\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"train_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown metric function:binary_precision",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0b4019c5d4d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mre_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_model.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    548\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'write'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[1;34m(h5dict, custom_objects, compile)\u001b[0m\n\u001b[0;32m    333\u001b[0m                       \u001b[0mweighted_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweighted_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                       \u001b[0mloss_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m                       sample_weight_mode=sample_weight_mode)\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;31m# Set optimizer weights.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m                 \u001b[0moutput_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnested_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m                 \u001b[0moutput_weighted_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnested_weighted_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m                 \u001b[0mhandle_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_metrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m                 \u001b[0mhandle_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_weighted_metrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mhandle_metrics\u001b[1;34m(metrics, weights)\u001b[0m\n\u001b[0;32m    409\u001b[0m                     \u001b[0mmetric_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetric_name_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m                     \u001b[0mmetric_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m                     \u001b[0mweighted_metric_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweighted_masked_objective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetric_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m                     \u001b[1;31m# Get metric name as string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\metrics.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0midentifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\metrics.py\u001b[0m in \u001b[0;36mdeserialize\u001b[1;34m(config, custom_objects)\u001b[0m\n\u001b[0;32m     71\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                                     printable_module_name='metric function')\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[1;32m--> 167\u001b[1;33m                                  ':' + function_name)\n\u001b[0m\u001b[0;32m    168\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown metric function:binary_precision"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "re_model=load_model(\"train_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
