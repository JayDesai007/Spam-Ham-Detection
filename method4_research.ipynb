{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-metrics\n",
      "  Using cached https://files.pythonhosted.org/packages/32/c9/a87420da8e73de944e63a8e9cdcfb1f03ca31a7c4cdcdbd45d2cdf13275a/keras_metrics-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: Keras>=2.1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-metrics) (2.2.5)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras-metrics) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras-metrics) (1.16.4)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras-metrics) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras-metrics) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras-metrics) (5.1.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras-metrics) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from Keras>=2.1.5->keras-metrics) (1.2.1)\n",
      "Installing collected packages: keras-metrics\n",
      "Successfully installed keras-metrics-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "import keras_metrics # for recall and precision metrics\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 100 # the length of all sequences (number of words per sample)\n",
    "EMBEDDING_SIZE = 100  # Using 100-Dimensional GloVe embedding vectors\n",
    "TEST_SIZE = 0.25 # ratio of testing set\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 25 # number of epochs\n",
    "\n",
    "# to convert labels to integers and vice-versa\n",
    "label2int = {\"ham\": 0, \"spam\": 1}\n",
    "int2label = {0: \"ham\", 1: \"spam\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads SMS Spam Collection dataset\n",
    "    \"\"\"\n",
    "    texts, labels = [], []\n",
    "    with open(\"SMSSpamCollection\") as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            labels.append(split[0].strip())\n",
    "            texts.append(' '.join(split[1:]).strip())\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text tokenization\n",
    "# vectorizing text, turning each text into sequence of integers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "# convert to sequence of integers\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "# pad sequences at the beginning of each sequence with 0's\n",
    "# for example if SEQUENCE_LENGTH=4:\n",
    "# [[5, 3, 2], [5, 1, 2, 3], [3, 4]]\n",
    "# will be transformed to:\n",
    "# [[0, 5, 3, 2], [5, 1, 2, 3], [0, 0, 3, 4]]\n",
    "X = pad_sequences(X, maxlen=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encoding labels\n",
    "# [spam, ham, spam, ham, ham] will be converted to:\n",
    "# [1, 0, 1, 0, 1] and then to:\n",
    "# [[0, 1], [1, 0], [0, 1], [1, 0], [0, 1]]\n",
    "\n",
    "y = [ label2int[label] for label in y ]\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and shuffle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vectors(tokenizer, dim=100):\n",
    "    embedding_index = {}\n",
    "    with open(f\"data/glove.6B.{dim}d.txt\", encoding='utf8') as f:\n",
    "        for line in tqdm.tqdm(f, \"Reading GloVe\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vectors = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_index[word] = vectors\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found will be 0s\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(tokenizer, lstm_units):\n",
    "    \"\"\"\n",
    "    Constructs the model,\n",
    "    Embedding vectors => LSTM => 2 output Fully-Connected neurons with softmax activation\n",
    "    \"\"\"\n",
    "    # get the GloVe embedding vectors\n",
    "    embedding_matrix = get_embedding_vectors(tokenizer)\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(tokenizer.word_index)+1,\n",
    "              EMBEDDING_SIZE,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              input_length=SEQUENCE_LENGTH))\n",
    "\n",
    "    model.add(LSTM(lstm_units, recurrent_dropout=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    # compile as rmsprop optimizer\n",
    "    # aswell as with recall metric\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\", keras_metrics.precision(), keras_metrics.recall()])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading GloVe: 400000it [00:31, 12830.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          901300    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 1,018,806\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 901,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# constructs the model with 128 LSTM units\n",
    "model = get_model(tokenizer=tokenizer, lstm_units=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (4180, 100)\n",
      "X_test.shape: (1394, 100)\n",
      "y_train.shape: (4180, 2)\n",
      "y_test.shape: (1394, 2)\n",
      "Train on 4180 samples, validate on 1394 samples\n",
      "Epoch 1/100\n",
      "4180/4180 [==============================] - ETA: 1:24 - loss: 0.7039 - acc: 0.5000 - precision: 0.8529 - recall: 0.51 - ETA: 47s - loss: 0.5364 - acc: 0.6875 - precision: 0.8673 - recall: 0.7589 - ETA: 34s - loss: 0.4903 - acc: 0.7240 - precision: 0.8395 - recall: 0.834 - ETA: 27s - loss: 0.4447 - acc: 0.7656 - precision: 0.8540 - recall: 0.877 - ETA: 23s - loss: 0.4232 - acc: 0.7812 - precision: 0.8517 - recall: 0.901 - ETA: 20s - loss: 0.4067 - acc: 0.7969 - precision: 0.8559 - recall: 0.918 - ETA: 18s - loss: 0.3806 - acc: 0.8170 - precision: 0.8684 - recall: 0.930 - ETA: 17s - loss: 0.3557 - acc: 0.8320 - precision: 0.8771 - recall: 0.939 - ETA: 16s - loss: 0.3371 - acc: 0.8438 - precision: 0.8842 - recall: 0.946 - ETA: 14s - loss: 0.3201 - acc: 0.8531 - precision: 0.8898 - recall: 0.952 - ETA: 14s - loss: 0.3198 - acc: 0.8537 - precision: 0.8859 - recall: 0.956 - ETA: 13s - loss: 0.3163 - acc: 0.8581 - precision: 0.8903 - recall: 0.955 - ETA: 12s - loss: 0.3072 - acc: 0.8642 - precision: 0.8937 - recall: 0.958 - ETA: 12s - loss: 0.2927 - acc: 0.8728 - precision: 0.9001 - recall: 0.961 - ETA: 11s - loss: 0.2902 - acc: 0.8750 - precision: 0.9009 - recall: 0.963 - ETA: 11s - loss: 0.2813 - acc: 0.8818 - precision: 0.9066 - recall: 0.964 - ETA: 10s - loss: 0.2727 - acc: 0.8851 - precision: 0.9082 - recall: 0.966 - ETA: 10s - loss: 0.2629 - acc: 0.8898 - precision: 0.9117 - recall: 0.967 - ETA: 10s - loss: 0.2539 - acc: 0.8947 - precision: 0.9161 - recall: 0.967 - ETA: 9s - loss: 0.2451 - acc: 0.8977 - precision: 0.9180 - recall: 0.969 - ETA: 9s - loss: 0.2479 - acc: 0.8966 - precision: 0.9213 - recall: 0.96 - ETA: 9s - loss: 0.2403 - acc: 0.9006 - precision: 0.9241 - recall: 0.96 - ETA: 8s - loss: 0.2324 - acc: 0.9049 - precision: 0.9273 - recall: 0.96 - ETA: 8s - loss: 0.2289 - acc: 0.9076 - precision: 0.9289 - recall: 0.96 - ETA: 8s - loss: 0.2245 - acc: 0.9100 - precision: 0.9303 - recall: 0.97 - ETA: 7s - loss: 0.2191 - acc: 0.9129 - precision: 0.9324 - recall: 0.97 - ETA: 7s - loss: 0.2164 - acc: 0.9149 - precision: 0.9338 - recall: 0.97 - ETA: 7s - loss: 0.2155 - acc: 0.9152 - precision: 0.9358 - recall: 0.97 - ETA: 7s - loss: 0.2139 - acc: 0.9154 - precision: 0.9361 - recall: 0.96 - ETA: 6s - loss: 0.2110 - acc: 0.9167 - precision: 0.9370 - recall: 0.97 - ETA: 6s - loss: 0.2060 - acc: 0.9194 - precision: 0.9389 - recall: 0.97 - ETA: 6s - loss: 0.2048 - acc: 0.9199 - precision: 0.9392 - recall: 0.97 - ETA: 6s - loss: 0.2027 - acc: 0.9209 - precision: 0.9400 - recall: 0.97 - ETA: 5s - loss: 0.2048 - acc: 0.9196 - precision: 0.9390 - recall: 0.97 - ETA: 5s - loss: 0.2027 - acc: 0.9205 - precision: 0.9402 - recall: 0.97 - ETA: 5s - loss: 0.1980 - acc: 0.9227 - precision: 0.9418 - recall: 0.97 - ETA: 5s - loss: 0.1951 - acc: 0.9240 - precision: 0.9428 - recall: 0.97 - ETA: 5s - loss: 0.1934 - acc: 0.9248 - precision: 0.9437 - recall: 0.97 - ETA: 4s - loss: 0.1896 - acc: 0.9263 - precision: 0.9447 - recall: 0.97 - ETA: 4s - loss: 0.1889 - acc: 0.9266 - precision: 0.9451 - recall: 0.97 - ETA: 4s - loss: 0.1866 - acc: 0.9268 - precision: 0.9454 - recall: 0.97 - ETA: 4s - loss: 0.1847 - acc: 0.9278 - precision: 0.9459 - recall: 0.97 - ETA: 4s - loss: 0.1869 - acc: 0.9273 - precision: 0.9471 - recall: 0.97 - ETA: 3s - loss: 0.1851 - acc: 0.9283 - precision: 0.9475 - recall: 0.97 - ETA: 3s - loss: 0.1845 - acc: 0.9285 - precision: 0.9472 - recall: 0.97 - ETA: 3s - loss: 0.1820 - acc: 0.9297 - precision: 0.9479 - recall: 0.97 - ETA: 3s - loss: 0.1807 - acc: 0.9302 - precision: 0.9483 - recall: 0.97 - ETA: 3s - loss: 0.1778 - acc: 0.9316 - precision: 0.9494 - recall: 0.97 - ETA: 2s - loss: 0.1764 - acc: 0.9321 - precision: 0.9494 - recall: 0.97 - ETA: 2s - loss: 0.1760 - acc: 0.9319 - precision: 0.9503 - recall: 0.97 - ETA: 2s - loss: 0.1747 - acc: 0.9323 - precision: 0.9503 - recall: 0.97 - ETA: 2s - loss: 0.1728 - acc: 0.9330 - precision: 0.9506 - recall: 0.97 - ETA: 2s - loss: 0.1713 - acc: 0.9334 - precision: 0.9505 - recall: 0.97 - ETA: 1s - loss: 0.1703 - acc: 0.9343 - precision: 0.9511 - recall: 0.97 - ETA: 1s - loss: 0.1687 - acc: 0.9352 - precision: 0.9518 - recall: 0.97 - ETA: 1s - loss: 0.1696 - acc: 0.9355 - precision: 0.9517 - recall: 0.97 - ETA: 1s - loss: 0.1718 - acc: 0.9348 - precision: 0.9521 - recall: 0.97 - ETA: 1s - loss: 0.1699 - acc: 0.9356 - precision: 0.9526 - recall: 0.97 - ETA: 1s - loss: 0.1700 - acc: 0.9354 - precision: 0.9522 - recall: 0.97 - ETA: 0s - loss: 0.1686 - acc: 0.9365 - precision: 0.9531 - recall: 0.97 - ETA: 0s - loss: 0.1682 - acc: 0.9367 - precision: 0.9535 - recall: 0.97 - ETA: 0s - loss: 0.1682 - acc: 0.9372 - precision: 0.9537 - recall: 0.97 - ETA: 0s - loss: 0.1667 - acc: 0.9377 - precision: 0.9541 - recall: 0.97 - ETA: 0s - loss: 0.1650 - acc: 0.9382 - precision: 0.9546 - recall: 0.97 - ETA: 0s - loss: 0.1640 - acc: 0.9389 - precision: 0.9550 - recall: 0.97 - 13s 3ms/step - loss: 0.1633 - acc: 0.9392 - precision: 0.9552 - recall: 0.9758 - val_loss: 0.0978 - val_acc: 0.9634 - val_precision: 0.9674 - val_recall: 0.9908\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.09777, saving model to results/spam_classifier_0.10\n",
      "Epoch 2/100\n",
      "4180/4180 [==============================] - ETA: 15s - loss: 0.1286 - acc: 0.9375 - precision: 0.9464 - recall: 0.981 - ETA: 12s - loss: 0.1293 - acc: 0.9531 - precision: 0.9649 - recall: 0.982 - ETA: 11s - loss: 0.0999 - acc: 0.9635 - precision: 0.9711 - recall: 0.988 - ETA: 11s - loss: 0.0834 - acc: 0.9727 - precision: 0.9781 - recall: 0.991 - ETA: 10s - loss: 0.0820 - acc: 0.9719 - precision: 0.9785 - recall: 0.989 - ETA: 10s - loss: 0.0932 - acc: 0.9714 - precision: 0.9789 - recall: 0.987 - ETA: 9s - loss: 0.0839 - acc: 0.9754 - precision: 0.9819 - recall: 0.989 - ETA: 9s - loss: 0.0758 - acc: 0.9785 - precision: 0.9844 - recall: 0.99 - ETA: 9s - loss: 0.0741 - acc: 0.9774 - precision: 0.9822 - recall: 0.99 - ETA: 8s - loss: 0.0809 - acc: 0.9734 - precision: 0.9769 - recall: 0.99 - ETA: 8s - loss: 0.0893 - acc: 0.9702 - precision: 0.9788 - recall: 0.98 - ETA: 8s - loss: 0.0839 - acc: 0.9727 - precision: 0.9806 - recall: 0.98 - ETA: 8s - loss: 0.0802 - acc: 0.9736 - precision: 0.9808 - recall: 0.98 - ETA: 8s - loss: 0.0801 - acc: 0.9743 - precision: 0.9822 - recall: 0.98 - ETA: 8s - loss: 0.0841 - acc: 0.9740 - precision: 0.9810 - recall: 0.98 - ETA: 8s - loss: 0.0849 - acc: 0.9727 - precision: 0.9821 - recall: 0.98 - ETA: 7s - loss: 0.0856 - acc: 0.9724 - precision: 0.9810 - recall: 0.98 - ETA: 7s - loss: 0.0849 - acc: 0.9731 - precision: 0.9811 - recall: 0.98 - ETA: 7s - loss: 0.0851 - acc: 0.9729 - precision: 0.9812 - recall: 0.98 - ETA: 7s - loss: 0.0870 - acc: 0.9711 - precision: 0.9786 - recall: 0.98 - ETA: 7s - loss: 0.0879 - acc: 0.9702 - precision: 0.9795 - recall: 0.98 - ETA: 7s - loss: 0.0889 - acc: 0.9702 - precision: 0.9788 - recall: 0.98 - ETA: 6s - loss: 0.0882 - acc: 0.9701 - precision: 0.9790 - recall: 0.98 - ETA: 6s - loss: 0.0884 - acc: 0.9701 - precision: 0.9784 - recall: 0.98 - ETA: 6s - loss: 0.0876 - acc: 0.9706 - precision: 0.9792 - recall: 0.98 - ETA: 6s - loss: 0.0847 - acc: 0.9718 - precision: 0.9800 - recall: 0.98 - ETA: 6s - loss: 0.0854 - acc: 0.9711 - precision: 0.9788 - recall: 0.98 - ETA: 6s - loss: 0.0862 - acc: 0.9710 - precision: 0.9790 - recall: 0.98 - ETA: 5s - loss: 0.0865 - acc: 0.9709 - precision: 0.9784 - recall: 0.98 - ETA: 5s - loss: 0.0860 - acc: 0.9714 - precision: 0.9792 - recall: 0.98 - ETA: 5s - loss: 0.0874 - acc: 0.9708 - precision: 0.9781 - recall: 0.98 - ETA: 5s - loss: 0.0879 - acc: 0.9712 - precision: 0.9788 - recall: 0.98 - ETA: 5s - loss: 0.0864 - acc: 0.9721 - precision: 0.9794 - recall: 0.98 - ETA: 5s - loss: 0.0845 - acc: 0.9729 - precision: 0.9801 - recall: 0.98 - ETA: 5s - loss: 0.0853 - acc: 0.9732 - precision: 0.9801 - recall: 0.98 - ETA: 4s - loss: 0.0857 - acc: 0.9731 - precision: 0.9802 - recall: 0.98 - ETA: 4s - loss: 0.0882 - acc: 0.9721 - precision: 0.9798 - recall: 0.98 - ETA: 4s - loss: 0.0913 - acc: 0.9708 - precision: 0.9780 - recall: 0.98 - ETA: 4s - loss: 0.0904 - acc: 0.9712 - precision: 0.9785 - recall: 0.98 - ETA: 4s - loss: 0.0894 - acc: 0.9715 - precision: 0.9790 - recall: 0.98 - ETA: 4s - loss: 0.0896 - acc: 0.9718 - precision: 0.9792 - recall: 0.98 - ETA: 3s - loss: 0.0877 - acc: 0.9725 - precision: 0.9797 - recall: 0.98 - ETA: 3s - loss: 0.0870 - acc: 0.9727 - precision: 0.9797 - recall: 0.98 - ETA: 3s - loss: 0.0864 - acc: 0.9730 - precision: 0.9797 - recall: 0.98 - ETA: 3s - loss: 0.0859 - acc: 0.9733 - precision: 0.9798 - recall: 0.98 - ETA: 3s - loss: 0.0845 - acc: 0.9738 - precision: 0.9802 - recall: 0.98 - ETA: 3s - loss: 0.0831 - acc: 0.9741 - precision: 0.9806 - recall: 0.98 - ETA: 2s - loss: 0.0835 - acc: 0.9740 - precision: 0.9803 - recall: 0.98 - ETA: 2s - loss: 0.0828 - acc: 0.9742 - precision: 0.9804 - recall: 0.99 - ETA: 2s - loss: 0.0826 - acc: 0.9738 - precision: 0.9808 - recall: 0.98 - ETA: 2s - loss: 0.0828 - acc: 0.9730 - precision: 0.9801 - recall: 0.98 - ETA: 2s - loss: 0.0830 - acc: 0.9727 - precision: 0.9798 - recall: 0.98 - ETA: 2s - loss: 0.0823 - acc: 0.9726 - precision: 0.9795 - recall: 0.98 - ETA: 1s - loss: 0.0834 - acc: 0.9716 - precision: 0.9786 - recall: 0.98 - ETA: 1s - loss: 0.0836 - acc: 0.9719 - precision: 0.9787 - recall: 0.98 - ETA: 1s - loss: 0.0839 - acc: 0.9718 - precision: 0.9791 - recall: 0.98 - ETA: 1s - loss: 0.0858 - acc: 0.9715 - precision: 0.9788 - recall: 0.98 - ETA: 1s - loss: 0.0851 - acc: 0.9717 - precision: 0.9788 - recall: 0.98 - ETA: 1s - loss: 0.0858 - acc: 0.9714 - precision: 0.9789 - recall: 0.98 - ETA: 0s - loss: 0.0880 - acc: 0.9706 - precision: 0.9777 - recall: 0.98 - ETA: 0s - loss: 0.0885 - acc: 0.9705 - precision: 0.9778 - recall: 0.98 - ETA: 0s - loss: 0.0897 - acc: 0.9703 - precision: 0.9779 - recall: 0.98 - ETA: 0s - loss: 0.0889 - acc: 0.9707 - precision: 0.9782 - recall: 0.98 - ETA: 0s - loss: 0.0880 - acc: 0.9712 - precision: 0.9786 - recall: 0.98 - ETA: 0s - loss: 0.0872 - acc: 0.9714 - precision: 0.9786 - recall: 0.98 - 12s 3ms/step - loss: 0.0872 - acc: 0.9713 - precision: 0.9785 - recall: 0.9887 - val_loss: 0.0856 - val_acc: 0.9706 - val_precision: 0.9857 - val_recall: 0.9800\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.09777 to 0.08558, saving model to results/spam_classifier_0.09\n",
      "Epoch 3/100\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 0.0211 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0225 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0252 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0382 - acc: 0.9961 - precision: 0.9954 - recall: 1.00 - ETA: 8s - loss: 0.0456 - acc: 0.9906 - precision: 0.9926 - recall: 0.99 - ETA: 8s - loss: 0.0530 - acc: 0.9870 - precision: 0.9908 - recall: 0.99 - ETA: 8s - loss: 0.0561 - acc: 0.9821 - precision: 0.9896 - recall: 0.98 - ETA: 8s - loss: 0.0579 - acc: 0.9805 - precision: 0.9864 - recall: 0.99 - ETA: 8s - loss: 0.0625 - acc: 0.9792 - precision: 0.9859 - recall: 0.98 - ETA: 8s - loss: 0.0637 - acc: 0.9781 - precision: 0.9856 - recall: 0.98 - ETA: 7s - loss: 0.0606 - acc: 0.9787 - precision: 0.9852 - recall: 0.99 - ETA: 7s - loss: 0.0621 - acc: 0.9792 - precision: 0.9849 - recall: 0.99 - ETA: 7s - loss: 0.0586 - acc: 0.9808 - precision: 0.9861 - recall: 0.99 - ETA: 7s - loss: 0.0607 - acc: 0.9810 - precision: 0.9871 - recall: 0.99 - ETA: 7s - loss: 0.0656 - acc: 0.9792 - precision: 0.9844 - recall: 0.99 - ETA: 7s - loss: 0.0643 - acc: 0.9795 - precision: 0.9854 - recall: 0.99 - ETA: 7s - loss: 0.0652 - acc: 0.9789 - precision: 0.9841 - recall: 0.99 - ETA: 6s - loss: 0.0656 - acc: 0.9783 - precision: 0.9840 - recall: 0.99 - ETA: 6s - loss: 0.0634 - acc: 0.9794 - precision: 0.9849 - recall: 0.99 - ETA: 6s - loss: 0.0669 - acc: 0.9797 - precision: 0.9848 - recall: 0.99 - ETA: 6s - loss: 0.0647 - acc: 0.9799 - precision: 0.9846 - recall: 0.99 - ETA: 6s - loss: 0.0658 - acc: 0.9787 - precision: 0.9853 - recall: 0.99 - ETA: 6s - loss: 0.0721 - acc: 0.9762 - precision: 0.9821 - recall: 0.99 - ETA: 6s - loss: 0.0719 - acc: 0.9772 - precision: 0.9829 - recall: 0.99 - ETA: 5s - loss: 0.0701 - acc: 0.9781 - precision: 0.9836 - recall: 0.99 - ETA: 5s - loss: 0.0696 - acc: 0.9778 - precision: 0.9828 - recall: 0.99 - ETA: 5s - loss: 0.0681 - acc: 0.9780 - precision: 0.9828 - recall: 0.99 - ETA: 5s - loss: 0.0664 - acc: 0.9788 - precision: 0.9834 - recall: 0.99 - ETA: 5s - loss: 0.0679 - acc: 0.9790 - precision: 0.9840 - recall: 0.99 - ETA: 5s - loss: 0.0663 - acc: 0.9797 - precision: 0.9845 - recall: 0.99 - ETA: 5s - loss: 0.0653 - acc: 0.9798 - precision: 0.9844 - recall: 0.99 - ETA: 4s - loss: 0.0651 - acc: 0.9800 - precision: 0.9849 - recall: 0.99 - ETA: 4s - loss: 0.0642 - acc: 0.9796 - precision: 0.9842 - recall: 0.99 - ETA: 4s - loss: 0.0631 - acc: 0.9798 - precision: 0.9842 - recall: 0.99 - ETA: 4s - loss: 0.0632 - acc: 0.9799 - precision: 0.9841 - recall: 0.99 - ETA: 4s - loss: 0.0665 - acc: 0.9787 - precision: 0.9836 - recall: 0.99 - ETA: 4s - loss: 0.0700 - acc: 0.9772 - precision: 0.9831 - recall: 0.99 - ETA: 4s - loss: 0.0716 - acc: 0.9766 - precision: 0.9821 - recall: 0.99 - ETA: 3s - loss: 0.0713 - acc: 0.9768 - precision: 0.9826 - recall: 0.99 - ETA: 3s - loss: 0.0703 - acc: 0.9773 - precision: 0.9831 - recall: 0.99 - ETA: 3s - loss: 0.0711 - acc: 0.9768 - precision: 0.9822 - recall: 0.99 - ETA: 3s - loss: 0.0718 - acc: 0.9762 - precision: 0.9813 - recall: 0.99 - ETA: 3s - loss: 0.0723 - acc: 0.9764 - precision: 0.9813 - recall: 0.99 - ETA: 3s - loss: 0.0748 - acc: 0.9762 - precision: 0.9809 - recall: 0.99 - ETA: 3s - loss: 0.0759 - acc: 0.9760 - precision: 0.9813 - recall: 0.99 - ETA: 2s - loss: 0.0766 - acc: 0.9762 - precision: 0.9813 - recall: 0.99 - ETA: 2s - loss: 0.0778 - acc: 0.9757 - precision: 0.9806 - recall: 0.99 - ETA: 2s - loss: 0.0769 - acc: 0.9762 - precision: 0.9810 - recall: 0.99 - ETA: 2s - loss: 0.0761 - acc: 0.9764 - precision: 0.9810 - recall: 0.99 - ETA: 2s - loss: 0.0764 - acc: 0.9759 - precision: 0.9807 - recall: 0.99 - ETA: 2s - loss: 0.0751 - acc: 0.9764 - precision: 0.9811 - recall: 0.99 - ETA: 1s - loss: 0.0740 - acc: 0.9769 - precision: 0.9815 - recall: 0.99 - ETA: 1s - loss: 0.0752 - acc: 0.9767 - precision: 0.9812 - recall: 0.99 - ETA: 1s - loss: 0.0745 - acc: 0.9769 - precision: 0.9812 - recall: 0.99 - ETA: 1s - loss: 0.0739 - acc: 0.9767 - precision: 0.9815 - recall: 0.99 - ETA: 1s - loss: 0.0735 - acc: 0.9768 - precision: 0.9816 - recall: 0.99 - ETA: 1s - loss: 0.0736 - acc: 0.9767 - precision: 0.9813 - recall: 0.99 - ETA: 1s - loss: 0.0729 - acc: 0.9771 - precision: 0.9816 - recall: 0.99 - ETA: 0s - loss: 0.0723 - acc: 0.9772 - precision: 0.9816 - recall: 0.99 - ETA: 0s - loss: 0.0722 - acc: 0.9771 - precision: 0.9819 - recall: 0.99 - ETA: 0s - loss: 0.0724 - acc: 0.9772 - precision: 0.9819 - recall: 0.99 - ETA: 0s - loss: 0.0717 - acc: 0.9776 - precision: 0.9822 - recall: 0.99 - ETA: 0s - loss: 0.0714 - acc: 0.9777 - precision: 0.9822 - recall: 0.99 - ETA: 0s - loss: 0.0709 - acc: 0.9778 - precision: 0.9825 - recall: 0.99 - ETA: 0s - loss: 0.0701 - acc: 0.9779 - precision: 0.9825 - recall: 0.99 - 11s 3ms/step - loss: 0.0699 - acc: 0.9780 - precision: 0.9825 - recall: 0.9923 - val_loss: 0.0834 - val_acc: 0.9720 - val_precision: 0.9770 - val_recall: 0.9908\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.08558 to 0.08340, saving model to results/spam_classifier_0.08\n",
      "Epoch 4/100\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 0.0855 - acc: 0.9688 - precision: 0.9828 - recall: 0.982 - ETA: 10s - loss: 0.0459 - acc: 0.9844 - precision: 0.9912 - recall: 0.991 - ETA: 10s - loss: 0.0434 - acc: 0.9792 - precision: 0.9826 - recall: 0.994 - ETA: 9s - loss: 0.0398 - acc: 0.9805 - precision: 0.9870 - recall: 0.991 - ETA: 9s - loss: 0.0391 - acc: 0.9812 - precision: 0.9859 - recall: 0.99 - ETA: 9s - loss: 0.0529 - acc: 0.9766 - precision: 0.9882 - recall: 0.98 - ETA: 8s - loss: 0.0752 - acc: 0.9710 - precision: 0.9797 - recall: 0.98 - ETA: 8s - loss: 0.0684 - acc: 0.9746 - precision: 0.9823 - recall: 0.98 - ETA: 8s - loss: 0.0661 - acc: 0.9757 - precision: 0.9821 - recall: 0.99 - ETA: 8s - loss: 0.0623 - acc: 0.9781 - precision: 0.9840 - recall: 0.99 - ETA: 8s - loss: 0.0586 - acc: 0.9801 - precision: 0.9854 - recall: 0.99 - ETA: 7s - loss: 0.0563 - acc: 0.9805 - precision: 0.9866 - recall: 0.99 - ETA: 7s - loss: 0.0581 - acc: 0.9808 - precision: 0.9862 - recall: 0.99 - ETA: 7s - loss: 0.0575 - acc: 0.9810 - precision: 0.9860 - recall: 0.99 - ETA: 7s - loss: 0.0545 - acc: 0.9823 - precision: 0.9870 - recall: 0.99 - ETA: 7s - loss: 0.0544 - acc: 0.9824 - precision: 0.9867 - recall: 0.99 - ETA: 7s - loss: 0.0517 - acc: 0.9835 - precision: 0.9875 - recall: 0.99 - ETA: 6s - loss: 0.0541 - acc: 0.9826 - precision: 0.9863 - recall: 0.99 - ETA: 6s - loss: 0.0530 - acc: 0.9827 - precision: 0.9861 - recall: 0.99 - ETA: 6s - loss: 0.0523 - acc: 0.9836 - precision: 0.9868 - recall: 0.99 - ETA: 6s - loss: 0.0537 - acc: 0.9836 - precision: 0.9865 - recall: 0.99 - ETA: 6s - loss: 0.0526 - acc: 0.9837 - precision: 0.9872 - recall: 0.99 - ETA: 6s - loss: 0.0547 - acc: 0.9837 - precision: 0.9870 - recall: 0.99 - ETA: 5s - loss: 0.0530 - acc: 0.9844 - precision: 0.9875 - recall: 0.99 - ETA: 5s - loss: 0.0531 - acc: 0.9844 - precision: 0.9873 - recall: 0.99 - ETA: 5s - loss: 0.0581 - acc: 0.9838 - precision: 0.9870 - recall: 0.99 - ETA: 5s - loss: 0.0575 - acc: 0.9838 - precision: 0.9869 - recall: 0.99 - ETA: 5s - loss: 0.0562 - acc: 0.9844 - precision: 0.9873 - recall: 0.99 - ETA: 5s - loss: 0.0557 - acc: 0.9844 - precision: 0.9871 - recall: 0.99 - ETA: 5s - loss: 0.0568 - acc: 0.9839 - precision: 0.9875 - recall: 0.99 - ETA: 4s - loss: 0.0579 - acc: 0.9834 - precision: 0.9868 - recall: 0.99 - ETA: 4s - loss: 0.0568 - acc: 0.9839 - precision: 0.9873 - recall: 0.99 - ETA: 4s - loss: 0.0581 - acc: 0.9825 - precision: 0.9861 - recall: 0.99 - ETA: 4s - loss: 0.0585 - acc: 0.9825 - precision: 0.9864 - recall: 0.99 - ETA: 4s - loss: 0.0580 - acc: 0.9826 - precision: 0.9863 - recall: 0.99 - ETA: 4s - loss: 0.0590 - acc: 0.9826 - precision: 0.9862 - recall: 0.99 - ETA: 4s - loss: 0.0588 - acc: 0.9827 - precision: 0.9865 - recall: 0.99 - ETA: 3s - loss: 0.0573 - acc: 0.9831 - precision: 0.9869 - recall: 0.99 - ETA: 3s - loss: 0.0573 - acc: 0.9832 - precision: 0.9872 - recall: 0.99 - ETA: 3s - loss: 0.0600 - acc: 0.9824 - precision: 0.9862 - recall: 0.99 - ETA: 3s - loss: 0.0595 - acc: 0.9821 - precision: 0.9857 - recall: 0.99 - ETA: 3s - loss: 0.0601 - acc: 0.9821 - precision: 0.9861 - recall: 0.99 - ETA: 3s - loss: 0.0596 - acc: 0.9822 - precision: 0.9860 - recall: 0.99 - ETA: 3s - loss: 0.0600 - acc: 0.9819 - precision: 0.9859 - recall: 0.99 - ETA: 2s - loss: 0.0596 - acc: 0.9819 - precision: 0.9862 - recall: 0.99 - ETA: 2s - loss: 0.0609 - acc: 0.9817 - precision: 0.9861 - recall: 0.99 - ETA: 2s - loss: 0.0598 - acc: 0.9820 - precision: 0.9864 - recall: 0.99 - ETA: 2s - loss: 0.0597 - acc: 0.9818 - precision: 0.9859 - recall: 0.99 - ETA: 2s - loss: 0.0600 - acc: 0.9809 - precision: 0.9862 - recall: 0.99 - ETA: 2s - loss: 0.0591 - acc: 0.9812 - precision: 0.9864 - recall: 0.99 - ETA: 2s - loss: 0.0591 - acc: 0.9813 - precision: 0.9864 - recall: 0.99 - ETA: 1s - loss: 0.0584 - acc: 0.9817 - precision: 0.9866 - recall: 0.99 - ETA: 1s - loss: 0.0574 - acc: 0.9820 - precision: 0.9869 - recall: 0.99 - ETA: 1s - loss: 0.0565 - acc: 0.9823 - precision: 0.9871 - recall: 0.99 - ETA: 1s - loss: 0.0577 - acc: 0.9815 - precision: 0.9867 - recall: 0.99 - ETA: 1s - loss: 0.0581 - acc: 0.9810 - precision: 0.9863 - recall: 0.99 - ETA: 1s - loss: 0.0583 - acc: 0.9808 - precision: 0.9862 - recall: 0.99 - ETA: 1s - loss: 0.0581 - acc: 0.9806 - precision: 0.9865 - recall: 0.99 - ETA: 0s - loss: 0.0595 - acc: 0.9804 - precision: 0.9861 - recall: 0.99 - ETA: 0s - loss: 0.0604 - acc: 0.9802 - precision: 0.9860 - recall: 0.99 - ETA: 0s - loss: 0.0620 - acc: 0.9798 - precision: 0.9856 - recall: 0.99 - ETA: 0s - loss: 0.0615 - acc: 0.9801 - precision: 0.9859 - recall: 0.99 - ETA: 0s - loss: 0.0611 - acc: 0.9799 - precision: 0.9855 - recall: 0.99 - ETA: 0s - loss: 0.0608 - acc: 0.9802 - precision: 0.9857 - recall: 0.99 - ETA: 0s - loss: 0.0607 - acc: 0.9803 - precision: 0.9857 - recall: 0.99 - 10s 3ms/step - loss: 0.0606 - acc: 0.9804 - precision: 0.9858 - recall: 0.9917 - val_loss: 0.1016 - val_acc: 0.9684 - val_precision: 0.9661 - val_recall: 0.9983\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.08340\n",
      "Epoch 5/100\n",
      "4180/4180 [==============================] - ETA: 11s - loss: 0.0548 - acc: 0.9688 - precision: 0.9661 - recall: 1.000 - ETA: 10s - loss: 0.0907 - acc: 0.9688 - precision: 0.9664 - recall: 1.000 - ETA: 9s - loss: 0.0858 - acc: 0.9740 - precision: 0.9771 - recall: 0.994 - ETA: 9s - loss: 0.0727 - acc: 0.9766 - precision: 0.9784 - recall: 0.99 - ETA: 8s - loss: 0.0622 - acc: 0.9812 - precision: 0.9826 - recall: 0.99 - ETA: 8s - loss: 0.0549 - acc: 0.9844 - precision: 0.9853 - recall: 0.99 - ETA: 8s - loss: 0.0515 - acc: 0.9844 - precision: 0.9849 - recall: 0.99 - ETA: 8s - loss: 0.0484 - acc: 0.9863 - precision: 0.9868 - recall: 0.99 - ETA: 8s - loss: 0.0570 - acc: 0.9844 - precision: 0.9843 - recall: 0.99 - ETA: 7s - loss: 0.0536 - acc: 0.9844 - precision: 0.9858 - recall: 0.99 - ETA: 7s - loss: 0.0509 - acc: 0.9858 - precision: 0.9870 - recall: 0.99 - ETA: 7s - loss: 0.0539 - acc: 0.9844 - precision: 0.9865 - recall: 0.99 - ETA: 7s - loss: 0.0505 - acc: 0.9856 - precision: 0.9875 - recall: 0.99 - ETA: 7s - loss: 0.0473 - acc: 0.9866 - precision: 0.9884 - recall: 0.99 - ETA: 7s - loss: 0.0481 - acc: 0.9865 - precision: 0.9880 - recall: 0.99 - ETA: 7s - loss: 0.0478 - acc: 0.9863 - precision: 0.9887 - recall: 0.99 - ETA: 7s - loss: 0.0471 - acc: 0.9853 - precision: 0.9894 - recall: 0.99 - ETA: 6s - loss: 0.0622 - acc: 0.9809 - precision: 0.9840 - recall: 0.99 - ETA: 6s - loss: 0.0612 - acc: 0.9819 - precision: 0.9847 - recall: 0.99 - ETA: 6s - loss: 0.0594 - acc: 0.9828 - precision: 0.9856 - recall: 0.99 - ETA: 6s - loss: 0.0586 - acc: 0.9829 - precision: 0.9854 - recall: 0.99 - ETA: 6s - loss: 0.0573 - acc: 0.9830 - precision: 0.9852 - recall: 0.99 - ETA: 6s - loss: 0.0569 - acc: 0.9830 - precision: 0.9859 - recall: 0.99 - ETA: 5s - loss: 0.0548 - acc: 0.9837 - precision: 0.9865 - recall: 0.99 - ETA: 5s - loss: 0.0544 - acc: 0.9838 - precision: 0.9862 - recall: 0.99 - ETA: 5s - loss: 0.0561 - acc: 0.9838 - precision: 0.9860 - recall: 0.99 - ETA: 5s - loss: 0.0550 - acc: 0.9844 - precision: 0.9866 - recall: 0.99 - ETA: 5s - loss: 0.0561 - acc: 0.9844 - precision: 0.9864 - recall: 0.99 - ETA: 5s - loss: 0.0556 - acc: 0.9844 - precision: 0.9863 - recall: 0.99 - ETA: 5s - loss: 0.0548 - acc: 0.9844 - precision: 0.9867 - recall: 0.99 - ETA: 4s - loss: 0.0545 - acc: 0.9844 - precision: 0.9866 - recall: 0.99 - ETA: 4s - loss: 0.0538 - acc: 0.9844 - precision: 0.9864 - recall: 0.99 - ETA: 4s - loss: 0.0539 - acc: 0.9844 - precision: 0.9868 - recall: 0.99 - ETA: 4s - loss: 0.0527 - acc: 0.9848 - precision: 0.9872 - recall: 0.99 - ETA: 4s - loss: 0.0520 - acc: 0.9848 - precision: 0.9870 - recall: 0.99 - ETA: 4s - loss: 0.0518 - acc: 0.9844 - precision: 0.9869 - recall: 0.99 - ETA: 4s - loss: 0.0565 - acc: 0.9835 - precision: 0.9868 - recall: 0.99 - ETA: 3s - loss: 0.0567 - acc: 0.9831 - precision: 0.9862 - recall: 0.99 - ETA: 3s - loss: 0.0562 - acc: 0.9832 - precision: 0.9865 - recall: 0.99 - ETA: 3s - loss: 0.0552 - acc: 0.9836 - precision: 0.9869 - recall: 0.99 - ETA: 3s - loss: 0.0539 - acc: 0.9840 - precision: 0.9873 - recall: 0.99 - ETA: 3s - loss: 0.0559 - acc: 0.9833 - precision: 0.9863 - recall: 0.99 - ETA: 3s - loss: 0.0549 - acc: 0.9836 - precision: 0.9866 - recall: 0.99 - ETA: 3s - loss: 0.0559 - acc: 0.9833 - precision: 0.9861 - recall: 0.99 - ETA: 2s - loss: 0.0551 - acc: 0.9837 - precision: 0.9864 - recall: 0.99 - ETA: 2s - loss: 0.0541 - acc: 0.9840 - precision: 0.9867 - recall: 0.99 - ETA: 2s - loss: 0.0546 - acc: 0.9840 - precision: 0.9866 - recall: 0.99 - ETA: 2s - loss: 0.0549 - acc: 0.9837 - precision: 0.9862 - recall: 0.99 - ETA: 2s - loss: 0.0553 - acc: 0.9834 - precision: 0.9861 - recall: 0.99 - ETA: 2s - loss: 0.0544 - acc: 0.9838 - precision: 0.9864 - recall: 0.99 - ETA: 2s - loss: 0.0550 - acc: 0.9835 - precision: 0.9863 - recall: 0.99 - ETA: 1s - loss: 0.0543 - acc: 0.9838 - precision: 0.9866 - recall: 0.99 - ETA: 1s - loss: 0.0537 - acc: 0.9841 - precision: 0.9868 - recall: 0.99 - ETA: 1s - loss: 0.0536 - acc: 0.9841 - precision: 0.9871 - recall: 0.99 - ETA: 1s - loss: 0.0528 - acc: 0.9844 - precision: 0.9873 - recall: 0.99 - ETA: 1s - loss: 0.0542 - acc: 0.9841 - precision: 0.9869 - recall: 0.99 - ETA: 1s - loss: 0.0542 - acc: 0.9844 - precision: 0.9871 - recall: 0.99 - ETA: 1s - loss: 0.0539 - acc: 0.9844 - precision: 0.9873 - recall: 0.99 - ETA: 0s - loss: 0.0535 - acc: 0.9844 - precision: 0.9872 - recall: 0.99 - ETA: 0s - loss: 0.0535 - acc: 0.9841 - precision: 0.9869 - recall: 0.99 - ETA: 0s - loss: 0.0544 - acc: 0.9841 - precision: 0.9868 - recall: 0.99 - ETA: 0s - loss: 0.0540 - acc: 0.9841 - precision: 0.9870 - recall: 0.99 - ETA: 0s - loss: 0.0533 - acc: 0.9844 - precision: 0.9872 - recall: 0.99 - ETA: 0s - loss: 0.0533 - acc: 0.9844 - precision: 0.9872 - recall: 0.99 - ETA: 0s - loss: 0.0536 - acc: 0.9844 - precision: 0.9871 - recall: 0.99 - 10s 2ms/step - loss: 0.0534 - acc: 0.9844 - precision: 0.9872 - recall: 0.9950 - val_loss: 0.0649 - val_acc: 0.9785 - val_precision: 0.9875 - val_recall: 0.9875\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.08340 to 0.06494, saving model to results/spam_classifier_0.06\n",
      "Epoch 6/100\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 0.0143 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0135 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0127 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0259 - acc: 0.9922 - precision: 0.9956 - recall: 0.99 - ETA: 8s - loss: 0.0259 - acc: 0.9875 - precision: 0.9895 - recall: 0.99 - ETA: 8s - loss: 0.0339 - acc: 0.9844 - precision: 0.9882 - recall: 0.99 - ETA: 8s - loss: 0.0314 - acc: 0.9866 - precision: 0.9897 - recall: 0.99 - ETA: 8s - loss: 0.0395 - acc: 0.9863 - precision: 0.9888 - recall: 0.99 - ETA: 8s - loss: 0.0409 - acc: 0.9861 - precision: 0.9881 - recall: 0.99 - ETA: 8s - loss: 0.0396 - acc: 0.9875 - precision: 0.9892 - recall: 0.99 - ETA: 7s - loss: 0.0383 - acc: 0.9872 - precision: 0.9902 - recall: 0.99 - ETA: 7s - loss: 0.0423 - acc: 0.9870 - precision: 0.9895 - recall: 0.99 - ETA: 7s - loss: 0.0406 - acc: 0.9868 - precision: 0.9903 - recall: 0.99 - ETA: 7s - loss: 0.0390 - acc: 0.9877 - precision: 0.9910 - recall: 0.99 - ETA: 7s - loss: 0.0376 - acc: 0.9875 - precision: 0.9916 - recall: 0.99 - ETA: 7s - loss: 0.0364 - acc: 0.9883 - precision: 0.9921 - recall: 0.99 - ETA: 7s - loss: 0.0367 - acc: 0.9881 - precision: 0.9915 - recall: 0.99 - ETA: 6s - loss: 0.0394 - acc: 0.9861 - precision: 0.9909 - recall: 0.99 - ETA: 6s - loss: 0.0418 - acc: 0.9852 - precision: 0.9895 - recall: 0.99 - ETA: 6s - loss: 0.0408 - acc: 0.9859 - precision: 0.9900 - recall: 0.99 - ETA: 6s - loss: 0.0396 - acc: 0.9866 - precision: 0.9905 - recall: 0.99 - ETA: 6s - loss: 0.0387 - acc: 0.9865 - precision: 0.9901 - recall: 0.99 - ETA: 6s - loss: 0.0375 - acc: 0.9871 - precision: 0.9905 - recall: 0.99 - ETA: 6s - loss: 0.0384 - acc: 0.9863 - precision: 0.9894 - recall: 0.99 - ETA: 5s - loss: 0.0385 - acc: 0.9856 - precision: 0.9899 - recall: 0.99 - ETA: 5s - loss: 0.0381 - acc: 0.9856 - precision: 0.9896 - recall: 0.99 - ETA: 5s - loss: 0.0406 - acc: 0.9850 - precision: 0.9886 - recall: 0.99 - ETA: 5s - loss: 0.0414 - acc: 0.9838 - precision: 0.9890 - recall: 0.99 - ETA: 5s - loss: 0.0417 - acc: 0.9833 - precision: 0.9882 - recall: 0.99 - ETA: 5s - loss: 0.0419 - acc: 0.9833 - precision: 0.9886 - recall: 0.99 - ETA: 4s - loss: 0.0418 - acc: 0.9839 - precision: 0.9889 - recall: 0.99 - ETA: 4s - loss: 0.0410 - acc: 0.9844 - precision: 0.9893 - recall: 0.99 - ETA: 4s - loss: 0.0424 - acc: 0.9844 - precision: 0.9891 - recall: 0.99 - ETA: 4s - loss: 0.0421 - acc: 0.9844 - precision: 0.9889 - recall: 0.99 - ETA: 4s - loss: 0.0417 - acc: 0.9848 - precision: 0.9892 - recall: 0.99 - ETA: 4s - loss: 0.0412 - acc: 0.9848 - precision: 0.9895 - recall: 0.99 - ETA: 4s - loss: 0.0438 - acc: 0.9844 - precision: 0.9888 - recall: 0.99 - ETA: 3s - loss: 0.0439 - acc: 0.9844 - precision: 0.9886 - recall: 0.99 - ETA: 3s - loss: 0.0454 - acc: 0.9840 - precision: 0.9884 - recall: 0.99 - ETA: 3s - loss: 0.0445 - acc: 0.9844 - precision: 0.9887 - recall: 0.99 - ETA: 3s - loss: 0.0471 - acc: 0.9840 - precision: 0.9881 - recall: 0.99 - ETA: 3s - loss: 0.0465 - acc: 0.9844 - precision: 0.9884 - recall: 0.99 - ETA: 3s - loss: 0.0458 - acc: 0.9847 - precision: 0.9887 - recall: 0.99 - ETA: 3s - loss: 0.0449 - acc: 0.9851 - precision: 0.9890 - recall: 0.99 - ETA: 2s - loss: 0.0453 - acc: 0.9851 - precision: 0.9888 - recall: 0.99 - ETA: 2s - loss: 0.0456 - acc: 0.9851 - precision: 0.9890 - recall: 0.99 - ETA: 2s - loss: 0.0447 - acc: 0.9854 - precision: 0.9893 - recall: 0.99 - ETA: 2s - loss: 0.0442 - acc: 0.9857 - precision: 0.9895 - recall: 0.99 - ETA: 2s - loss: 0.0435 - acc: 0.9860 - precision: 0.9897 - recall: 0.99 - ETA: 2s - loss: 0.0434 - acc: 0.9856 - precision: 0.9892 - recall: 0.99 - ETA: 2s - loss: 0.0446 - acc: 0.9850 - precision: 0.9887 - recall: 0.99 - ETA: 1s - loss: 0.0440 - acc: 0.9853 - precision: 0.9889 - recall: 0.99 - ETA: 1s - loss: 0.0441 - acc: 0.9853 - precision: 0.9888 - recall: 0.99 - ETA: 1s - loss: 0.0450 - acc: 0.9847 - precision: 0.9890 - recall: 0.99 - ETA: 1s - loss: 0.0448 - acc: 0.9847 - precision: 0.9892 - recall: 0.99 - ETA: 1s - loss: 0.0453 - acc: 0.9844 - precision: 0.9888 - recall: 0.99 - ETA: 1s - loss: 0.0454 - acc: 0.9844 - precision: 0.9886 - recall: 0.99 - ETA: 1s - loss: 0.0448 - acc: 0.9846 - precision: 0.9888 - recall: 0.99 - ETA: 0s - loss: 0.0452 - acc: 0.9846 - precision: 0.9888 - recall: 0.99 - ETA: 0s - loss: 0.0450 - acc: 0.9846 - precision: 0.9887 - recall: 0.99 - ETA: 0s - loss: 0.0443 - acc: 0.9849 - precision: 0.9888 - recall: 0.99 - ETA: 0s - loss: 0.0438 - acc: 0.9851 - precision: 0.9890 - recall: 0.99 - ETA: 0s - loss: 0.0447 - acc: 0.9851 - precision: 0.9889 - recall: 0.99 - ETA: 0s - loss: 0.0450 - acc: 0.9849 - precision: 0.9888 - recall: 0.99 - ETA: 0s - loss: 0.0444 - acc: 0.9851 - precision: 0.9890 - recall: 0.99 - 10s 3ms/step - loss: 0.0453 - acc: 0.9849 - precision: 0.9890 - recall: 0.9937 - val_loss: 0.2190 - val_acc: 0.9290 - val_precision: 0.9237 - val_recall: 1.0000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.06494\n",
      "Epoch 7/100\n",
      "4180/4180 [==============================] - ETA: 11s - loss: 0.1569 - acc: 0.9219 - precision: 0.9123 - recall: 1.000 - ETA: 10s - loss: 0.0939 - acc: 0.9531 - precision: 0.9474 - recall: 1.000 - ETA: 10s - loss: 0.0732 - acc: 0.9635 - precision: 0.9588 - recall: 1.000 - ETA: 10s - loss: 0.0684 - acc: 0.9609 - precision: 0.9554 - recall: 1.000 - ETA: 10s - loss: 0.0692 - acc: 0.9594 - precision: 0.9544 - recall: 1.000 - ETA: 9s - loss: 0.0642 - acc: 0.9635 - precision: 0.9588 - recall: 1.000 - ETA: 9s - loss: 0.0584 - acc: 0.9688 - precision: 0.9646 - recall: 1.00 - ETA: 9s - loss: 0.0571 - acc: 0.9707 - precision: 0.9669 - recall: 1.00 - ETA: 9s - loss: 0.0523 - acc: 0.9740 - precision: 0.9705 - recall: 1.00 - ETA: 8s - loss: 0.0479 - acc: 0.9766 - precision: 0.9735 - recall: 1.00 - ETA: 8s - loss: 0.0441 - acc: 0.9787 - precision: 0.9758 - recall: 1.00 - ETA: 8s - loss: 0.0443 - acc: 0.9792 - precision: 0.9763 - recall: 1.00 - ETA: 8s - loss: 0.0438 - acc: 0.9796 - precision: 0.9782 - recall: 0.99 - ETA: 8s - loss: 0.0409 - acc: 0.9810 - precision: 0.9798 - recall: 0.99 - ETA: 8s - loss: 0.0390 - acc: 0.9823 - precision: 0.9812 - recall: 0.99 - ETA: 7s - loss: 0.0374 - acc: 0.9834 - precision: 0.9823 - recall: 0.99 - ETA: 7s - loss: 0.0357 - acc: 0.9844 - precision: 0.9834 - recall: 0.99 - ETA: 7s - loss: 0.0394 - acc: 0.9835 - precision: 0.9824 - recall: 0.99 - ETA: 7s - loss: 0.0402 - acc: 0.9836 - precision: 0.9823 - recall: 0.99 - ETA: 7s - loss: 0.0390 - acc: 0.9844 - precision: 0.9832 - recall: 0.99 - ETA: 7s - loss: 0.0377 - acc: 0.9851 - precision: 0.9840 - recall: 0.99 - ETA: 6s - loss: 0.0365 - acc: 0.9858 - precision: 0.9847 - recall: 0.99 - ETA: 6s - loss: 0.0351 - acc: 0.9864 - precision: 0.9854 - recall: 0.99 - ETA: 6s - loss: 0.0339 - acc: 0.9870 - precision: 0.9859 - recall: 0.99 - ETA: 6s - loss: 0.0329 - acc: 0.9875 - precision: 0.9865 - recall: 0.99 - ETA: 6s - loss: 0.0324 - acc: 0.9880 - precision: 0.9870 - recall: 0.99 - ETA: 5s - loss: 0.0332 - acc: 0.9878 - precision: 0.9869 - recall: 0.99 - ETA: 5s - loss: 0.0338 - acc: 0.9877 - precision: 0.9867 - recall: 0.99 - ETA: 5s - loss: 0.0345 - acc: 0.9876 - precision: 0.9865 - recall: 0.99 - ETA: 5s - loss: 0.0384 - acc: 0.9870 - precision: 0.9864 - recall: 0.99 - ETA: 5s - loss: 0.0377 - acc: 0.9874 - precision: 0.9868 - recall: 0.99 - ETA: 5s - loss: 0.0400 - acc: 0.9868 - precision: 0.9861 - recall: 0.99 - ETA: 5s - loss: 0.0406 - acc: 0.9867 - precision: 0.9865 - recall: 0.99 - ETA: 4s - loss: 0.0475 - acc: 0.9858 - precision: 0.9853 - recall: 0.99 - ETA: 4s - loss: 0.0468 - acc: 0.9862 - precision: 0.9857 - recall: 0.99 - ETA: 4s - loss: 0.0460 - acc: 0.9865 - precision: 0.9861 - recall: 0.99 - ETA: 4s - loss: 0.0452 - acc: 0.9869 - precision: 0.9865 - recall: 0.99 - ETA: 4s - loss: 0.0444 - acc: 0.9873 - precision: 0.9869 - recall: 0.99 - ETA: 4s - loss: 0.0435 - acc: 0.9876 - precision: 0.9872 - recall: 0.99 - ETA: 3s - loss: 0.0441 - acc: 0.9871 - precision: 0.9871 - recall: 0.99 - ETA: 3s - loss: 0.0454 - acc: 0.9859 - precision: 0.9857 - recall: 0.99 - ETA: 3s - loss: 0.0459 - acc: 0.9859 - precision: 0.9860 - recall: 0.99 - ETA: 3s - loss: 0.0464 - acc: 0.9858 - precision: 0.9859 - recall: 0.99 - ETA: 3s - loss: 0.0456 - acc: 0.9862 - precision: 0.9863 - recall: 0.99 - ETA: 3s - loss: 0.0454 - acc: 0.9861 - precision: 0.9862 - recall: 0.99 - ETA: 2s - loss: 0.0461 - acc: 0.9861 - precision: 0.9861 - recall: 0.99 - ETA: 2s - loss: 0.0454 - acc: 0.9864 - precision: 0.9864 - recall: 0.99 - ETA: 2s - loss: 0.0447 - acc: 0.9867 - precision: 0.9867 - recall: 0.99 - ETA: 2s - loss: 0.0441 - acc: 0.9869 - precision: 0.9870 - recall: 0.99 - ETA: 2s - loss: 0.0435 - acc: 0.9872 - precision: 0.9872 - recall: 0.99 - ETA: 2s - loss: 0.0427 - acc: 0.9874 - precision: 0.9875 - recall: 0.99 - ETA: 2s - loss: 0.0427 - acc: 0.9871 - precision: 0.9874 - recall: 0.99 - ETA: 1s - loss: 0.0423 - acc: 0.9870 - precision: 0.9873 - recall: 0.99 - ETA: 1s - loss: 0.0418 - acc: 0.9873 - precision: 0.9875 - recall: 0.99 - ETA: 1s - loss: 0.0411 - acc: 0.9875 - precision: 0.9877 - recall: 0.99 - ETA: 1s - loss: 0.0412 - acc: 0.9874 - precision: 0.9879 - recall: 0.99 - ETA: 1s - loss: 0.0408 - acc: 0.9874 - precision: 0.9879 - recall: 0.99 - ETA: 1s - loss: 0.0405 - acc: 0.9873 - precision: 0.9878 - recall: 0.99 - ETA: 0s - loss: 0.0412 - acc: 0.9870 - precision: 0.9874 - recall: 0.99 - ETA: 0s - loss: 0.0417 - acc: 0.9867 - precision: 0.9873 - recall: 0.99 - ETA: 0s - loss: 0.0420 - acc: 0.9864 - precision: 0.9872 - recall: 0.99 - ETA: 0s - loss: 0.0422 - acc: 0.9861 - precision: 0.9868 - recall: 0.99 - ETA: 0s - loss: 0.0420 - acc: 0.9861 - precision: 0.9867 - recall: 0.99 - ETA: 0s - loss: 0.0416 - acc: 0.9861 - precision: 0.9869 - recall: 0.99 - ETA: 0s - loss: 0.0413 - acc: 0.9863 - precision: 0.9871 - recall: 0.99 - 11s 3ms/step - loss: 0.0411 - acc: 0.9864 - precision: 0.9872 - recall: 0.9972 - val_loss: 0.0645 - val_acc: 0.9763 - val_precision: 0.9834 - val_recall: 0.9891\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.06494 to 0.06455, saving model to results/spam_classifier_0.06\n",
      "Epoch 8/100\n",
      "4180/4180 [==============================] - ETA: 11s - loss: 0.0182 - acc: 0.9844 - precision: 0.9818 - recall: 1.000 - ETA: 10s - loss: 0.0193 - acc: 0.9922 - precision: 0.9910 - recall: 1.000 - ETA: 9s - loss: 0.0172 - acc: 0.9948 - precision: 0.9941 - recall: 1.000 - ETA: 9s - loss: 0.0151 - acc: 0.9961 - precision: 0.9955 - recall: 1.00 - ETA: 9s - loss: 0.0239 - acc: 0.9906 - precision: 0.9928 - recall: 0.99 - ETA: 9s - loss: 0.0225 - acc: 0.9896 - precision: 0.9910 - recall: 0.99 - ETA: 8s - loss: 0.0201 - acc: 0.9911 - precision: 0.9923 - recall: 0.99 - ETA: 8s - loss: 0.0180 - acc: 0.9922 - precision: 0.9933 - recall: 0.99 - ETA: 8s - loss: 0.0176 - acc: 0.9931 - precision: 0.9940 - recall: 0.99 - ETA: 8s - loss: 0.0162 - acc: 0.9938 - precision: 0.9946 - recall: 0.99 - ETA: 8s - loss: 0.0171 - acc: 0.9929 - precision: 0.9934 - recall: 0.99 - ETA: 7s - loss: 0.0236 - acc: 0.9896 - precision: 0.9910 - recall: 0.99 - ETA: 7s - loss: 0.0254 - acc: 0.9892 - precision: 0.9916 - recall: 0.99 - ETA: 7s - loss: 0.0239 - acc: 0.9900 - precision: 0.9922 - recall: 0.99 - ETA: 7s - loss: 0.0233 - acc: 0.9906 - precision: 0.9927 - recall: 0.99 - ETA: 7s - loss: 0.0234 - acc: 0.9902 - precision: 0.9932 - recall: 0.99 - ETA: 7s - loss: 0.0229 - acc: 0.9899 - precision: 0.9926 - recall: 0.99 - ETA: 7s - loss: 0.0219 - acc: 0.9905 - precision: 0.9930 - recall: 0.99 - ETA: 7s - loss: 0.0277 - acc: 0.9893 - precision: 0.9915 - recall: 0.99 - ETA: 6s - loss: 0.0270 - acc: 0.9898 - precision: 0.9919 - recall: 0.99 - ETA: 6s - loss: 0.0273 - acc: 0.9896 - precision: 0.9914 - recall: 0.99 - ETA: 6s - loss: 0.0270 - acc: 0.9901 - precision: 0.9918 - recall: 0.99 - ETA: 6s - loss: 0.0263 - acc: 0.9905 - precision: 0.9921 - recall: 0.99 - ETA: 6s - loss: 0.0258 - acc: 0.9909 - precision: 0.9925 - recall: 0.99 - ETA: 6s - loss: 0.0318 - acc: 0.9900 - precision: 0.9913 - recall: 0.99 - ETA: 6s - loss: 0.0310 - acc: 0.9904 - precision: 0.9917 - recall: 0.99 - ETA: 5s - loss: 0.0313 - acc: 0.9907 - precision: 0.9920 - recall: 0.99 - ETA: 5s - loss: 0.0334 - acc: 0.9900 - precision: 0.9910 - recall: 0.99 - ETA: 5s - loss: 0.0326 - acc: 0.9903 - precision: 0.9913 - recall: 0.99 - ETA: 5s - loss: 0.0336 - acc: 0.9901 - precision: 0.9910 - recall: 0.99 - ETA: 5s - loss: 0.0329 - acc: 0.9904 - precision: 0.9913 - recall: 0.99 - ETA: 5s - loss: 0.0322 - acc: 0.9907 - precision: 0.9916 - recall: 0.99 - ETA: 5s - loss: 0.0314 - acc: 0.9910 - precision: 0.9918 - recall: 0.99 - ETA: 4s - loss: 0.0329 - acc: 0.9908 - precision: 0.9916 - recall: 0.99 - ETA: 4s - loss: 0.0343 - acc: 0.9906 - precision: 0.9913 - recall: 0.99 - ETA: 4s - loss: 0.0335 - acc: 0.9909 - precision: 0.9915 - recall: 0.99 - ETA: 4s - loss: 0.0330 - acc: 0.9911 - precision: 0.9918 - recall: 0.99 - ETA: 4s - loss: 0.0325 - acc: 0.9914 - precision: 0.9920 - recall: 0.99 - ETA: 4s - loss: 0.0321 - acc: 0.9916 - precision: 0.9922 - recall: 0.99 - ETA: 3s - loss: 0.0324 - acc: 0.9914 - precision: 0.9924 - recall: 0.99 - ETA: 3s - loss: 0.0324 - acc: 0.9912 - precision: 0.9921 - recall: 0.99 - ETA: 3s - loss: 0.0330 - acc: 0.9911 - precision: 0.9923 - recall: 0.99 - ETA: 3s - loss: 0.0347 - acc: 0.9909 - precision: 0.9921 - recall: 0.99 - ETA: 3s - loss: 0.0345 - acc: 0.9908 - precision: 0.9919 - recall: 0.99 - ETA: 3s - loss: 0.0353 - acc: 0.9906 - precision: 0.9916 - recall: 0.99 - ETA: 3s - loss: 0.0353 - acc: 0.9905 - precision: 0.9914 - recall: 0.99 - ETA: 2s - loss: 0.0349 - acc: 0.9907 - precision: 0.9916 - recall: 0.99 - ETA: 2s - loss: 0.0345 - acc: 0.9909 - precision: 0.9918 - recall: 0.99 - ETA: 2s - loss: 0.0340 - acc: 0.9911 - precision: 0.9919 - recall: 0.99 - ETA: 2s - loss: 0.0335 - acc: 0.9912 - precision: 0.9921 - recall: 0.99 - ETA: 2s - loss: 0.0330 - acc: 0.9914 - precision: 0.9922 - recall: 0.99 - ETA: 2s - loss: 0.0333 - acc: 0.9913 - precision: 0.9921 - recall: 0.99 - ETA: 1s - loss: 0.0333 - acc: 0.9912 - precision: 0.9922 - recall: 0.99 - ETA: 1s - loss: 0.0327 - acc: 0.9913 - precision: 0.9924 - recall: 0.99 - ETA: 1s - loss: 0.0345 - acc: 0.9909 - precision: 0.9918 - recall: 0.99 - ETA: 1s - loss: 0.0342 - acc: 0.9911 - precision: 0.9920 - recall: 0.99 - ETA: 1s - loss: 0.0346 - acc: 0.9910 - precision: 0.9918 - recall: 0.99 - ETA: 1s - loss: 0.0343 - acc: 0.9911 - precision: 0.9920 - recall: 0.99 - ETA: 0s - loss: 0.0354 - acc: 0.9907 - precision: 0.9915 - recall: 0.99 - ETA: 0s - loss: 0.0357 - acc: 0.9904 - precision: 0.9916 - recall: 0.99 - ETA: 0s - loss: 0.0353 - acc: 0.9905 - precision: 0.9918 - recall: 0.99 - ETA: 0s - loss: 0.0352 - acc: 0.9904 - precision: 0.9916 - recall: 0.99 - ETA: 0s - loss: 0.0349 - acc: 0.9906 - precision: 0.9918 - recall: 0.99 - ETA: 0s - loss: 0.0345 - acc: 0.9907 - precision: 0.9919 - recall: 0.99 - ETA: 0s - loss: 0.0341 - acc: 0.9909 - precision: 0.9920 - recall: 0.99 - 11s 3ms/step - loss: 0.0340 - acc: 0.9909 - precision: 0.9921 - recall: 0.9975 - val_loss: 0.0705 - val_acc: 0.9806 - val_precision: 0.9859 - val_recall: 0.9917\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.06455\n",
      "Epoch 9/100\n",
      "4180/4180 [==============================] - ETA: 12s - loss: 0.0825 - acc: 0.9844 - precision: 0.9800 - recall: 1.000 - ETA: 11s - loss: 0.0531 - acc: 0.9844 - precision: 0.9908 - recall: 0.990 - ETA: 11s - loss: 0.0389 - acc: 0.9896 - precision: 0.9941 - recall: 0.994 - ETA: 10s - loss: 0.0309 - acc: 0.9922 - precision: 0.9956 - recall: 0.995 - ETA: 10s - loss: 0.0249 - acc: 0.9938 - precision: 0.9965 - recall: 0.996 - ETA: 9s - loss: 0.0216 - acc: 0.9948 - precision: 0.9971 - recall: 0.997 - ETA: 9s - loss: 0.0199 - acc: 0.9955 - precision: 0.9975 - recall: 0.99 - ETA: 9s - loss: 0.0275 - acc: 0.9941 - precision: 0.9956 - recall: 0.99 - ETA: 9s - loss: 0.0364 - acc: 0.9913 - precision: 0.9922 - recall: 0.99 - ETA: 9s - loss: 0.0348 - acc: 0.9922 - precision: 0.9930 - recall: 0.99 - ETA: 9s - loss: 0.0364 - acc: 0.9915 - precision: 0.9920 - recall: 0.99 - ETA: 8s - loss: 0.0346 - acc: 0.9909 - precision: 0.9926 - recall: 0.99 - ETA: 8s - loss: 0.0352 - acc: 0.9892 - precision: 0.9932 - recall: 0.99 - ETA: 8s - loss: 0.0359 - acc: 0.9888 - precision: 0.9924 - recall: 0.99 - ETA: 8s - loss: 0.0360 - acc: 0.9885 - precision: 0.9917 - recall: 0.99 - ETA: 8s - loss: 0.0344 - acc: 0.9893 - precision: 0.9922 - recall: 0.99 - ETA: 7s - loss: 0.0334 - acc: 0.9890 - precision: 0.9917 - recall: 0.99 - ETA: 7s - loss: 0.0321 - acc: 0.9896 - precision: 0.9921 - recall: 0.99 - ETA: 7s - loss: 0.0308 - acc: 0.9901 - precision: 0.9925 - recall: 0.99 - ETA: 7s - loss: 0.0337 - acc: 0.9898 - precision: 0.9920 - recall: 0.99 - ETA: 7s - loss: 0.0324 - acc: 0.9903 - precision: 0.9924 - recall: 0.99 - ETA: 7s - loss: 0.0362 - acc: 0.9886 - precision: 0.9903 - recall: 0.99 - ETA: 6s - loss: 0.0368 - acc: 0.9885 - precision: 0.9907 - recall: 0.99 - ETA: 6s - loss: 0.0387 - acc: 0.9876 - precision: 0.9896 - recall: 0.99 - ETA: 6s - loss: 0.0387 - acc: 0.9875 - precision: 0.9900 - recall: 0.99 - ETA: 6s - loss: 0.0381 - acc: 0.9880 - precision: 0.9903 - recall: 0.99 - ETA: 6s - loss: 0.0372 - acc: 0.9884 - precision: 0.9907 - recall: 0.99 - ETA: 6s - loss: 0.0362 - acc: 0.9888 - precision: 0.9910 - recall: 0.99 - ETA: 5s - loss: 0.0353 - acc: 0.9892 - precision: 0.9913 - recall: 0.99 - ETA: 5s - loss: 0.0347 - acc: 0.9896 - precision: 0.9916 - recall: 0.99 - ETA: 5s - loss: 0.0350 - acc: 0.9894 - precision: 0.9919 - recall: 0.99 - ETA: 5s - loss: 0.0340 - acc: 0.9897 - precision: 0.9921 - recall: 0.99 - ETA: 5s - loss: 0.0366 - acc: 0.9891 - precision: 0.9913 - recall: 0.99 - ETA: 4s - loss: 0.0372 - acc: 0.9885 - precision: 0.9915 - recall: 0.99 - ETA: 4s - loss: 0.0386 - acc: 0.9884 - precision: 0.9912 - recall: 0.99 - ETA: 4s - loss: 0.0379 - acc: 0.9887 - precision: 0.9915 - recall: 0.99 - ETA: 4s - loss: 0.0388 - acc: 0.9882 - precision: 0.9912 - recall: 0.99 - ETA: 4s - loss: 0.0381 - acc: 0.9885 - precision: 0.9915 - recall: 0.99 - ETA: 4s - loss: 0.0373 - acc: 0.9888 - precision: 0.9917 - recall: 0.99 - ETA: 4s - loss: 0.0373 - acc: 0.9887 - precision: 0.9914 - recall: 0.99 - ETA: 3s - loss: 0.0382 - acc: 0.9886 - precision: 0.9912 - recall: 0.99 - ETA: 3s - loss: 0.0377 - acc: 0.9888 - precision: 0.9914 - recall: 0.99 - ETA: 3s - loss: 0.0373 - acc: 0.9887 - precision: 0.9912 - recall: 0.99 - ETA: 3s - loss: 0.0371 - acc: 0.9886 - precision: 0.9910 - recall: 0.99 - ETA: 3s - loss: 0.0379 - acc: 0.9885 - precision: 0.9908 - recall: 0.99 - ETA: 3s - loss: 0.0372 - acc: 0.9888 - precision: 0.9910 - recall: 0.99 - ETA: 2s - loss: 0.0365 - acc: 0.9890 - precision: 0.9912 - recall: 0.99 - ETA: 2s - loss: 0.0359 - acc: 0.9893 - precision: 0.9914 - recall: 0.99 - ETA: 2s - loss: 0.0356 - acc: 0.9892 - precision: 0.9916 - recall: 0.99 - ETA: 2s - loss: 0.0350 - acc: 0.9894 - precision: 0.9917 - recall: 0.99 - ETA: 2s - loss: 0.0350 - acc: 0.9893 - precision: 0.9915 - recall: 0.99 - ETA: 2s - loss: 0.0354 - acc: 0.9892 - precision: 0.9914 - recall: 0.99 - ETA: 1s - loss: 0.0348 - acc: 0.9894 - precision: 0.9915 - recall: 0.99 - ETA: 1s - loss: 0.0351 - acc: 0.9893 - precision: 0.9914 - recall: 0.99 - ETA: 1s - loss: 0.0351 - acc: 0.9892 - precision: 0.9912 - recall: 0.99 - ETA: 1s - loss: 0.0357 - acc: 0.9891 - precision: 0.9914 - recall: 0.99 - ETA: 1s - loss: 0.0359 - acc: 0.9890 - precision: 0.9912 - recall: 0.99 - ETA: 1s - loss: 0.0359 - acc: 0.9887 - precision: 0.9913 - recall: 0.99 - ETA: 0s - loss: 0.0354 - acc: 0.9889 - precision: 0.9915 - recall: 0.99 - ETA: 0s - loss: 0.0351 - acc: 0.9891 - precision: 0.9916 - recall: 0.99 - ETA: 0s - loss: 0.0353 - acc: 0.9890 - precision: 0.9918 - recall: 0.99 - ETA: 0s - loss: 0.0349 - acc: 0.9892 - precision: 0.9919 - recall: 0.99 - ETA: 0s - loss: 0.0347 - acc: 0.9893 - precision: 0.9920 - recall: 0.99 - ETA: 0s - loss: 0.0342 - acc: 0.9895 - precision: 0.9922 - recall: 0.99 - ETA: 0s - loss: 0.0350 - acc: 0.9894 - precision: 0.9920 - recall: 0.99 - 11s 3ms/step - loss: 0.0348 - acc: 0.9895 - precision: 0.9920 - recall: 0.9959 - val_loss: 0.0623 - val_acc: 0.9806 - val_precision: 0.9908 - val_recall: 0.9866\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.06455 to 0.06228, saving model to results/spam_classifier_0.06\n",
      "Epoch 10/100\n",
      "4180/4180 [==============================] - ETA: 11s - loss: 0.0260 - acc: 0.9844 - precision: 0.9808 - recall: 1.000 - ETA: 10s - loss: 0.0245 - acc: 0.9844 - precision: 0.9811 - recall: 1.000 - ETA: 10s - loss: 0.0184 - acc: 0.9896 - precision: 0.9876 - recall: 1.000 - ETA: 9s - loss: 0.0142 - acc: 0.9922 - precision: 0.9909 - recall: 1.000 - ETA: 9s - loss: 0.0120 - acc: 0.9938 - precision: 0.9927 - recall: 1.00 - ETA: 9s - loss: 0.0168 - acc: 0.9922 - precision: 0.9910 - recall: 1.00 - ETA: 8s - loss: 0.0268 - acc: 0.9911 - precision: 0.9897 - recall: 1.00 - ETA: 8s - loss: 0.0258 - acc: 0.9902 - precision: 0.9888 - recall: 1.00 - ETA: 8s - loss: 0.0233 - acc: 0.9913 - precision: 0.9901 - recall: 1.00 - ETA: 8s - loss: 0.0214 - acc: 0.9922 - precision: 0.9910 - recall: 1.00 - ETA: 8s - loss: 0.0221 - acc: 0.9915 - precision: 0.9901 - recall: 1.00 - ETA: 8s - loss: 0.0213 - acc: 0.9922 - precision: 0.9910 - recall: 1.00 - ETA: 8s - loss: 0.0259 - acc: 0.9904 - precision: 0.9903 - recall: 0.99 - ETA: 8s - loss: 0.0242 - acc: 0.9911 - precision: 0.9910 - recall: 0.99 - ETA: 8s - loss: 0.0231 - acc: 0.9917 - precision: 0.9916 - recall: 0.99 - ETA: 7s - loss: 0.0219 - acc: 0.9922 - precision: 0.9921 - recall: 0.99 - ETA: 7s - loss: 0.0213 - acc: 0.9926 - precision: 0.9926 - recall: 0.99 - ETA: 7s - loss: 0.0209 - acc: 0.9931 - precision: 0.9930 - recall: 0.99 - ETA: 7s - loss: 0.0199 - acc: 0.9934 - precision: 0.9934 - recall: 0.99 - ETA: 7s - loss: 0.0196 - acc: 0.9938 - precision: 0.9937 - recall: 0.99 - ETA: 6s - loss: 0.0213 - acc: 0.9926 - precision: 0.9940 - recall: 0.99 - ETA: 6s - loss: 0.0221 - acc: 0.9922 - precision: 0.9934 - recall: 0.99 - ETA: 6s - loss: 0.0220 - acc: 0.9918 - precision: 0.9930 - recall: 0.99 - ETA: 6s - loss: 0.0215 - acc: 0.9922 - precision: 0.9933 - recall: 0.99 - ETA: 6s - loss: 0.0224 - acc: 0.9912 - precision: 0.9921 - recall: 0.99 - ETA: 6s - loss: 0.0219 - acc: 0.9916 - precision: 0.9924 - recall: 0.99 - ETA: 5s - loss: 0.0233 - acc: 0.9907 - precision: 0.9927 - recall: 0.99 - ETA: 5s - loss: 0.0229 - acc: 0.9911 - precision: 0.9930 - recall: 0.99 - ETA: 5s - loss: 0.0222 - acc: 0.9914 - precision: 0.9932 - recall: 0.99 - ETA: 5s - loss: 0.0237 - acc: 0.9911 - precision: 0.9928 - recall: 0.99 - ETA: 5s - loss: 0.0232 - acc: 0.9914 - precision: 0.9931 - recall: 0.99 - ETA: 5s - loss: 0.0233 - acc: 0.9912 - precision: 0.9927 - recall: 0.99 - ETA: 5s - loss: 0.0254 - acc: 0.9910 - precision: 0.9924 - recall: 0.99 - ETA: 4s - loss: 0.0266 - acc: 0.9908 - precision: 0.9921 - recall: 0.99 - ETA: 4s - loss: 0.0263 - acc: 0.9911 - precision: 0.9923 - recall: 0.99 - ETA: 4s - loss: 0.0257 - acc: 0.9913 - precision: 0.9925 - recall: 0.99 - ETA: 4s - loss: 0.0257 - acc: 0.9911 - precision: 0.9927 - recall: 0.99 - ETA: 4s - loss: 0.0252 - acc: 0.9914 - precision: 0.9929 - recall: 0.99 - ETA: 4s - loss: 0.0250 - acc: 0.9912 - precision: 0.9927 - recall: 0.99 - ETA: 4s - loss: 0.0249 - acc: 0.9910 - precision: 0.9924 - recall: 0.99 - ETA: 3s - loss: 0.0244 - acc: 0.9912 - precision: 0.9926 - recall: 0.99 - ETA: 3s - loss: 0.0244 - acc: 0.9911 - precision: 0.9928 - recall: 0.99 - ETA: 3s - loss: 0.0240 - acc: 0.9913 - precision: 0.9929 - recall: 0.99 - ETA: 3s - loss: 0.0235 - acc: 0.9915 - precision: 0.9931 - recall: 0.99 - ETA: 3s - loss: 0.0243 - acc: 0.9910 - precision: 0.9924 - recall: 0.99 - ETA: 3s - loss: 0.0243 - acc: 0.9912 - precision: 0.9926 - recall: 0.99 - ETA: 2s - loss: 0.0242 - acc: 0.9914 - precision: 0.9928 - recall: 0.99 - ETA: 2s - loss: 0.0244 - acc: 0.9912 - precision: 0.9925 - recall: 0.99 - ETA: 2s - loss: 0.0240 - acc: 0.9914 - precision: 0.9927 - recall: 0.99 - ETA: 2s - loss: 0.0237 - acc: 0.9916 - precision: 0.9928 - recall: 0.99 - ETA: 2s - loss: 0.0234 - acc: 0.9917 - precision: 0.9930 - recall: 0.99 - ETA: 2s - loss: 0.0248 - acc: 0.9916 - precision: 0.9927 - recall: 0.99 - ETA: 1s - loss: 0.0247 - acc: 0.9915 - precision: 0.9929 - recall: 0.99 - ETA: 1s - loss: 0.0245 - acc: 0.9916 - precision: 0.9930 - recall: 0.99 - ETA: 1s - loss: 0.0248 - acc: 0.9915 - precision: 0.9928 - recall: 0.99 - ETA: 1s - loss: 0.0258 - acc: 0.9911 - precision: 0.9926 - recall: 0.99 - ETA: 1s - loss: 0.0265 - acc: 0.9910 - precision: 0.9925 - recall: 0.99 - ETA: 1s - loss: 0.0262 - acc: 0.9911 - precision: 0.9926 - recall: 0.99 - ETA: 0s - loss: 0.0267 - acc: 0.9910 - precision: 0.9924 - recall: 0.99 - ETA: 0s - loss: 0.0268 - acc: 0.9909 - precision: 0.9922 - recall: 0.99 - ETA: 0s - loss: 0.0265 - acc: 0.9910 - precision: 0.9924 - recall: 0.99 - ETA: 0s - loss: 0.0263 - acc: 0.9912 - precision: 0.9925 - recall: 0.99 - ETA: 0s - loss: 0.0259 - acc: 0.9913 - precision: 0.9926 - recall: 0.99 - ETA: 0s - loss: 0.0271 - acc: 0.9912 - precision: 0.9924 - recall: 0.99 - ETA: 0s - loss: 0.0268 - acc: 0.9913 - precision: 0.9926 - recall: 0.99 - 11s 3ms/step - loss: 0.0267 - acc: 0.9914 - precision: 0.9926 - recall: 0.9975 - val_loss: 0.0556 - val_acc: 0.9835 - val_precision: 0.9908 - val_recall: 0.9900\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.06228 to 0.05557, saving model to results/spam_classifier_0.06\n",
      "Epoch 11/100\n",
      "4180/4180 [==============================] - ETA: 12s - loss: 0.0099 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0130 - acc: 0.9922 - precision: 1.0000 - recall: 0.991 - ETA: 9s - loss: 0.0110 - acc: 0.9948 - precision: 1.0000 - recall: 0.994 - ETA: 9s - loss: 0.0119 - acc: 0.9961 - precision: 1.0000 - recall: 0.99 - ETA: 9s - loss: 0.0133 - acc: 0.9938 - precision: 0.9965 - recall: 0.99 - ETA: 8s - loss: 0.0148 - acc: 0.9948 - precision: 0.9971 - recall: 0.99 - ETA: 8s - loss: 0.0130 - acc: 0.9955 - precision: 0.9974 - recall: 0.99 - ETA: 8s - loss: 0.0121 - acc: 0.9961 - precision: 0.9978 - recall: 0.99 - ETA: 8s - loss: 0.0109 - acc: 0.9965 - precision: 0.9980 - recall: 0.99 - ETA: 8s - loss: 0.0140 - acc: 0.9953 - precision: 0.9964 - recall: 0.99 - ETA: 8s - loss: 0.0142 - acc: 0.9957 - precision: 0.9967 - recall: 0.99 - ETA: 7s - loss: 0.0132 - acc: 0.9961 - precision: 0.9970 - recall: 0.99 - ETA: 7s - loss: 0.0165 - acc: 0.9952 - precision: 0.9958 - recall: 0.99 - ETA: 7s - loss: 0.0157 - acc: 0.9955 - precision: 0.9961 - recall: 0.99 - ETA: 7s - loss: 0.0148 - acc: 0.9958 - precision: 0.9964 - recall: 0.99 - ETA: 7s - loss: 0.0154 - acc: 0.9951 - precision: 0.9955 - recall: 0.99 - ETA: 6s - loss: 0.0152 - acc: 0.9954 - precision: 0.9957 - recall: 0.99 - ETA: 6s - loss: 0.0147 - acc: 0.9957 - precision: 0.9960 - recall: 0.99 - ETA: 6s - loss: 0.0141 - acc: 0.9959 - precision: 0.9962 - recall: 0.99 - ETA: 6s - loss: 0.0154 - acc: 0.9953 - precision: 0.9964 - recall: 0.99 - ETA: 6s - loss: 0.0188 - acc: 0.9940 - precision: 0.9948 - recall: 0.99 - ETA: 6s - loss: 0.0211 - acc: 0.9936 - precision: 0.9942 - recall: 0.99 - ETA: 6s - loss: 0.0208 - acc: 0.9939 - precision: 0.9945 - recall: 0.99 - ETA: 5s - loss: 0.0202 - acc: 0.9941 - precision: 0.9947 - recall: 0.99 - ETA: 5s - loss: 0.0197 - acc: 0.9944 - precision: 0.9950 - recall: 0.99 - ETA: 5s - loss: 0.0193 - acc: 0.9946 - precision: 0.9952 - recall: 0.99 - ETA: 5s - loss: 0.0189 - acc: 0.9948 - precision: 0.9953 - recall: 0.99 - ETA: 5s - loss: 0.0210 - acc: 0.9939 - precision: 0.9942 - recall: 0.99 - ETA: 5s - loss: 0.0205 - acc: 0.9941 - precision: 0.9944 - recall: 0.99 - ETA: 5s - loss: 0.0203 - acc: 0.9943 - precision: 0.9946 - recall: 0.99 - ETA: 4s - loss: 0.0209 - acc: 0.9940 - precision: 0.9942 - recall: 0.99 - ETA: 4s - loss: 0.0207 - acc: 0.9941 - precision: 0.9944 - recall: 0.99 - ETA: 4s - loss: 0.0202 - acc: 0.9943 - precision: 0.9945 - recall: 0.99 - ETA: 4s - loss: 0.0207 - acc: 0.9936 - precision: 0.9947 - recall: 0.99 - ETA: 4s - loss: 0.0217 - acc: 0.9929 - precision: 0.9938 - recall: 0.99 - ETA: 4s - loss: 0.0216 - acc: 0.9931 - precision: 0.9940 - recall: 0.99 - ETA: 4s - loss: 0.0213 - acc: 0.9932 - precision: 0.9942 - recall: 0.99 - ETA: 3s - loss: 0.0215 - acc: 0.9934 - precision: 0.9943 - recall: 0.99 - ETA: 3s - loss: 0.0211 - acc: 0.9936 - precision: 0.9945 - recall: 0.99 - ETA: 3s - loss: 0.0208 - acc: 0.9938 - precision: 0.9946 - recall: 0.99 - ETA: 3s - loss: 0.0213 - acc: 0.9935 - precision: 0.9943 - recall: 0.99 - ETA: 3s - loss: 0.0209 - acc: 0.9937 - precision: 0.9944 - recall: 0.99 - ETA: 3s - loss: 0.0205 - acc: 0.9938 - precision: 0.9946 - recall: 0.99 - ETA: 3s - loss: 0.0202 - acc: 0.9940 - precision: 0.9947 - recall: 0.99 - ETA: 2s - loss: 0.0199 - acc: 0.9941 - precision: 0.9948 - recall: 0.99 - ETA: 2s - loss: 0.0195 - acc: 0.9942 - precision: 0.9949 - recall: 0.99 - ETA: 2s - loss: 0.0192 - acc: 0.9943 - precision: 0.9950 - recall: 0.99 - ETA: 2s - loss: 0.0189 - acc: 0.9945 - precision: 0.9951 - recall: 0.99 - ETA: 2s - loss: 0.0185 - acc: 0.9946 - precision: 0.9952 - recall: 0.99 - ETA: 2s - loss: 0.0205 - acc: 0.9941 - precision: 0.9946 - recall: 0.99 - ETA: 2s - loss: 0.0231 - acc: 0.9933 - precision: 0.9947 - recall: 0.99 - ETA: 1s - loss: 0.0228 - acc: 0.9934 - precision: 0.9948 - recall: 0.99 - ETA: 1s - loss: 0.0228 - acc: 0.9932 - precision: 0.9946 - recall: 0.99 - ETA: 1s - loss: 0.0230 - acc: 0.9931 - precision: 0.9944 - recall: 0.99 - ETA: 1s - loss: 0.0228 - acc: 0.9932 - precision: 0.9945 - recall: 0.99 - ETA: 1s - loss: 0.0225 - acc: 0.9933 - precision: 0.9946 - recall: 0.99 - ETA: 1s - loss: 0.0223 - acc: 0.9934 - precision: 0.9947 - recall: 0.99 - ETA: 1s - loss: 0.0220 - acc: 0.9935 - precision: 0.9947 - recall: 0.99 - ETA: 0s - loss: 0.0229 - acc: 0.9934 - precision: 0.9945 - recall: 0.99 - ETA: 0s - loss: 0.0225 - acc: 0.9935 - precision: 0.9946 - recall: 0.99 - ETA: 0s - loss: 0.0223 - acc: 0.9936 - precision: 0.9947 - recall: 0.99 - ETA: 0s - loss: 0.0229 - acc: 0.9934 - precision: 0.9945 - recall: 0.99 - ETA: 0s - loss: 0.0226 - acc: 0.9936 - precision: 0.9946 - recall: 0.99 - ETA: 0s - loss: 0.0228 - acc: 0.9934 - precision: 0.9944 - recall: 0.99 - ETA: 0s - loss: 0.0228 - acc: 0.9933 - precision: 0.9942 - recall: 0.99 - 11s 3ms/step - loss: 0.0230 - acc: 0.9931 - precision: 0.9942 - recall: 0.9978 - val_loss: 0.0617 - val_acc: 0.9821 - val_precision: 0.9827 - val_recall: 0.9967\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.05557\n",
      "Epoch 12/100\n",
      "4180/4180 [==============================] - ETA: 9s - loss: 0.0043 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0271 - acc: 0.9922 - precision: 0.9911 - recall: 1.00 - ETA: 9s - loss: 0.0200 - acc: 0.9948 - precision: 0.9938 - recall: 1.00 - ETA: 9s - loss: 0.0181 - acc: 0.9961 - precision: 0.9953 - recall: 1.00 - ETA: 8s - loss: 0.0159 - acc: 0.9969 - precision: 0.9963 - recall: 1.00 - ETA: 8s - loss: 0.0136 - acc: 0.9974 - precision: 0.9970 - recall: 1.00 - ETA: 8s - loss: 0.0170 - acc: 0.9955 - precision: 0.9949 - recall: 1.00 - ETA: 8s - loss: 0.0184 - acc: 0.9941 - precision: 0.9932 - recall: 1.00 - ETA: 8s - loss: 0.0176 - acc: 0.9948 - precision: 0.9940 - recall: 1.00 - ETA: 8s - loss: 0.0164 - acc: 0.9953 - precision: 0.9946 - recall: 1.00 - ETA: 7s - loss: 0.0151 - acc: 0.9957 - precision: 0.9951 - recall: 1.00 - ETA: 7s - loss: 0.0154 - acc: 0.9948 - precision: 0.9941 - recall: 1.00 - ETA: 7s - loss: 0.0149 - acc: 0.9952 - precision: 0.9945 - recall: 1.00 - ETA: 7s - loss: 0.0172 - acc: 0.9944 - precision: 0.9937 - recall: 1.00 - ETA: 7s - loss: 0.0166 - acc: 0.9948 - precision: 0.9941 - recall: 1.00 - ETA: 7s - loss: 0.0158 - acc: 0.9951 - precision: 0.9944 - recall: 1.00 - ETA: 7s - loss: 0.0195 - acc: 0.9936 - precision: 0.9937 - recall: 0.99 - ETA: 7s - loss: 0.0205 - acc: 0.9931 - precision: 0.9931 - recall: 0.99 - ETA: 6s - loss: 0.0206 - acc: 0.9926 - precision: 0.9925 - recall: 0.99 - ETA: 6s - loss: 0.0205 - acc: 0.9930 - precision: 0.9928 - recall: 0.99 - ETA: 6s - loss: 0.0273 - acc: 0.9918 - precision: 0.9932 - recall: 0.99 - ETA: 6s - loss: 0.0261 - acc: 0.9922 - precision: 0.9935 - recall: 0.99 - ETA: 6s - loss: 0.0290 - acc: 0.9912 - precision: 0.9922 - recall: 0.99 - ETA: 6s - loss: 0.0281 - acc: 0.9915 - precision: 0.9925 - recall: 0.99 - ETA: 5s - loss: 0.0293 - acc: 0.9912 - precision: 0.9921 - recall: 0.99 - ETA: 5s - loss: 0.0286 - acc: 0.9916 - precision: 0.9924 - recall: 0.99 - ETA: 5s - loss: 0.0276 - acc: 0.9919 - precision: 0.9927 - recall: 0.99 - ETA: 5s - loss: 0.0276 - acc: 0.9916 - precision: 0.9923 - recall: 0.99 - ETA: 5s - loss: 0.0274 - acc: 0.9914 - precision: 0.9919 - recall: 0.99 - ETA: 5s - loss: 0.0281 - acc: 0.9911 - precision: 0.9922 - recall: 0.99 - ETA: 5s - loss: 0.0275 - acc: 0.9914 - precision: 0.9925 - recall: 0.99 - ETA: 4s - loss: 0.0267 - acc: 0.9917 - precision: 0.9927 - recall: 0.99 - ETA: 4s - loss: 0.0275 - acc: 0.9915 - precision: 0.9924 - recall: 0.99 - ETA: 4s - loss: 0.0271 - acc: 0.9917 - precision: 0.9927 - recall: 0.99 - ETA: 4s - loss: 0.0267 - acc: 0.9920 - precision: 0.9928 - recall: 0.99 - ETA: 4s - loss: 0.0261 - acc: 0.9922 - precision: 0.9931 - recall: 0.99 - ETA: 4s - loss: 0.0256 - acc: 0.9924 - precision: 0.9932 - recall: 0.99 - ETA: 3s - loss: 0.0250 - acc: 0.9926 - precision: 0.9934 - recall: 0.99 - ETA: 3s - loss: 0.0251 - acc: 0.9924 - precision: 0.9931 - recall: 0.99 - ETA: 3s - loss: 0.0246 - acc: 0.9926 - precision: 0.9933 - recall: 0.99 - ETA: 3s - loss: 0.0241 - acc: 0.9928 - precision: 0.9935 - recall: 0.99 - ETA: 3s - loss: 0.0237 - acc: 0.9929 - precision: 0.9936 - recall: 0.99 - ETA: 3s - loss: 0.0241 - acc: 0.9927 - precision: 0.9934 - recall: 0.99 - ETA: 3s - loss: 0.0250 - acc: 0.9922 - precision: 0.9935 - recall: 0.99 - ETA: 2s - loss: 0.0245 - acc: 0.9924 - precision: 0.9936 - recall: 0.99 - ETA: 2s - loss: 0.0243 - acc: 0.9922 - precision: 0.9934 - recall: 0.99 - ETA: 2s - loss: 0.0239 - acc: 0.9924 - precision: 0.9935 - recall: 0.99 - ETA: 2s - loss: 0.0245 - acc: 0.9922 - precision: 0.9933 - recall: 0.99 - ETA: 2s - loss: 0.0243 - acc: 0.9923 - precision: 0.9934 - recall: 0.99 - ETA: 2s - loss: 0.0240 - acc: 0.9925 - precision: 0.9936 - recall: 0.99 - ETA: 2s - loss: 0.0238 - acc: 0.9926 - precision: 0.9937 - recall: 0.99 - ETA: 1s - loss: 0.0244 - acc: 0.9922 - precision: 0.9931 - recall: 0.99 - ETA: 1s - loss: 0.0244 - acc: 0.9920 - precision: 0.9933 - recall: 0.99 - ETA: 1s - loss: 0.0241 - acc: 0.9922 - precision: 0.9934 - recall: 0.99 - ETA: 1s - loss: 0.0238 - acc: 0.9923 - precision: 0.9935 - recall: 0.99 - ETA: 1s - loss: 0.0245 - acc: 0.9919 - precision: 0.9933 - recall: 0.99 - ETA: 1s - loss: 0.0241 - acc: 0.9921 - precision: 0.9934 - recall: 0.99 - ETA: 1s - loss: 0.0237 - acc: 0.9922 - precision: 0.9935 - recall: 0.99 - ETA: 0s - loss: 0.0234 - acc: 0.9923 - precision: 0.9936 - recall: 0.99 - ETA: 0s - loss: 0.0231 - acc: 0.9924 - precision: 0.9937 - recall: 0.99 - ETA: 0s - loss: 0.0231 - acc: 0.9923 - precision: 0.9935 - recall: 0.99 - ETA: 0s - loss: 0.0228 - acc: 0.9924 - precision: 0.9936 - recall: 0.99 - ETA: 0s - loss: 0.0225 - acc: 0.9926 - precision: 0.9937 - recall: 0.99 - ETA: 0s - loss: 0.0222 - acc: 0.9927 - precision: 0.9938 - recall: 0.99 - ETA: 0s - loss: 0.0233 - acc: 0.9925 - precision: 0.9937 - recall: 0.99 - 10s 3ms/step - loss: 0.0232 - acc: 0.9926 - precision: 0.9937 - recall: 0.9978 - val_loss: 0.0655 - val_acc: 0.9828 - val_precision: 0.9916 - val_recall: 0.9883\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.05557\n",
      "Epoch 13/100\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 0.0039 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0035 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0034 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0032 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0104 - acc: 0.9969 - precision: 0.9964 - recall: 1.00 - ETA: 9s - loss: 0.0113 - acc: 0.9974 - precision: 0.9970 - recall: 1.00 - ETA: 8s - loss: 0.0100 - acc: 0.9978 - precision: 0.9974 - recall: 1.00 - ETA: 8s - loss: 0.0089 - acc: 0.9980 - precision: 0.9978 - recall: 1.00 - ETA: 8s - loss: 0.0090 - acc: 0.9983 - precision: 0.9980 - recall: 1.00 - ETA: 8s - loss: 0.0178 - acc: 0.9953 - precision: 0.9946 - recall: 1.00 - ETA: 8s - loss: 0.0174 - acc: 0.9957 - precision: 0.9951 - recall: 1.00 - ETA: 8s - loss: 0.0166 - acc: 0.9961 - precision: 0.9955 - recall: 1.00 - ETA: 7s - loss: 0.0157 - acc: 0.9964 - precision: 0.9959 - recall: 1.00 - ETA: 7s - loss: 0.0149 - acc: 0.9967 - precision: 0.9962 - recall: 1.00 - ETA: 7s - loss: 0.0161 - acc: 0.9958 - precision: 0.9952 - recall: 1.00 - ETA: 7s - loss: 0.0162 - acc: 0.9951 - precision: 0.9944 - recall: 1.00 - ETA: 7s - loss: 0.0159 - acc: 0.9954 - precision: 0.9947 - recall: 1.00 - ETA: 7s - loss: 0.0152 - acc: 0.9957 - precision: 0.9950 - recall: 1.00 - ETA: 6s - loss: 0.0147 - acc: 0.9959 - precision: 0.9953 - recall: 1.00 - ETA: 6s - loss: 0.0142 - acc: 0.9961 - precision: 0.9955 - recall: 1.00 - ETA: 6s - loss: 0.0136 - acc: 0.9963 - precision: 0.9957 - recall: 1.00 - ETA: 6s - loss: 0.0130 - acc: 0.9964 - precision: 0.9959 - recall: 1.00 - ETA: 6s - loss: 0.0143 - acc: 0.9959 - precision: 0.9953 - recall: 1.00 - ETA: 6s - loss: 0.0147 - acc: 0.9961 - precision: 0.9955 - recall: 1.00 - ETA: 5s - loss: 0.0152 - acc: 0.9956 - precision: 0.9950 - recall: 1.00 - ETA: 5s - loss: 0.0156 - acc: 0.9952 - precision: 0.9952 - recall: 0.99 - ETA: 5s - loss: 0.0165 - acc: 0.9948 - precision: 0.9947 - recall: 0.99 - ETA: 5s - loss: 0.0169 - acc: 0.9944 - precision: 0.9949 - recall: 0.99 - ETA: 5s - loss: 0.0165 - acc: 0.9946 - precision: 0.9950 - recall: 0.99 - ETA: 5s - loss: 0.0163 - acc: 0.9948 - precision: 0.9952 - recall: 0.99 - ETA: 5s - loss: 0.0163 - acc: 0.9945 - precision: 0.9948 - recall: 0.99 - ETA: 4s - loss: 0.0161 - acc: 0.9946 - precision: 0.9950 - recall: 0.99 - ETA: 4s - loss: 0.0157 - acc: 0.9948 - precision: 0.9951 - recall: 0.99 - ETA: 4s - loss: 0.0157 - acc: 0.9949 - precision: 0.9952 - recall: 0.99 - ETA: 4s - loss: 0.0153 - acc: 0.9951 - precision: 0.9954 - recall: 0.99 - ETA: 4s - loss: 0.0149 - acc: 0.9952 - precision: 0.9955 - recall: 0.99 - ETA: 4s - loss: 0.0146 - acc: 0.9954 - precision: 0.9956 - recall: 0.99 - ETA: 4s - loss: 0.0143 - acc: 0.9955 - precision: 0.9958 - recall: 0.99 - ETA: 3s - loss: 0.0140 - acc: 0.9956 - precision: 0.9959 - recall: 0.99 - ETA: 3s - loss: 0.0137 - acc: 0.9957 - precision: 0.9960 - recall: 0.99 - ETA: 3s - loss: 0.0146 - acc: 0.9954 - precision: 0.9956 - recall: 0.99 - ETA: 3s - loss: 0.0143 - acc: 0.9955 - precision: 0.9957 - recall: 0.99 - ETA: 3s - loss: 0.0140 - acc: 0.9956 - precision: 0.9958 - recall: 0.99 - ETA: 3s - loss: 0.0157 - acc: 0.9954 - precision: 0.9959 - recall: 0.99 - ETA: 3s - loss: 0.0153 - acc: 0.9955 - precision: 0.9960 - recall: 0.99 - ETA: 2s - loss: 0.0156 - acc: 0.9952 - precision: 0.9957 - recall: 0.99 - ETA: 2s - loss: 0.0154 - acc: 0.9953 - precision: 0.9958 - recall: 0.99 - ETA: 2s - loss: 0.0152 - acc: 0.9954 - precision: 0.9959 - recall: 0.99 - ETA: 2s - loss: 0.0152 - acc: 0.9952 - precision: 0.9956 - recall: 0.99 - ETA: 2s - loss: 0.0151 - acc: 0.9953 - precision: 0.9957 - recall: 0.99 - ETA: 2s - loss: 0.0151 - acc: 0.9954 - precision: 0.9958 - recall: 0.99 - ETA: 1s - loss: 0.0150 - acc: 0.9955 - precision: 0.9959 - recall: 0.99 - ETA: 1s - loss: 0.0149 - acc: 0.9956 - precision: 0.9959 - recall: 0.99 - ETA: 1s - loss: 0.0146 - acc: 0.9957 - precision: 0.9960 - recall: 0.99 - ETA: 1s - loss: 0.0144 - acc: 0.9957 - precision: 0.9961 - recall: 0.99 - ETA: 1s - loss: 0.0144 - acc: 0.9958 - precision: 0.9962 - recall: 0.99 - ETA: 1s - loss: 0.0143 - acc: 0.9959 - precision: 0.9962 - recall: 0.99 - ETA: 1s - loss: 0.0143 - acc: 0.9957 - precision: 0.9960 - recall: 0.99 - ETA: 0s - loss: 0.0143 - acc: 0.9958 - precision: 0.9960 - recall: 0.99 - ETA: 0s - loss: 0.0152 - acc: 0.9956 - precision: 0.9958 - recall: 0.99 - ETA: 0s - loss: 0.0157 - acc: 0.9951 - precision: 0.9956 - recall: 0.99 - ETA: 0s - loss: 0.0164 - acc: 0.9950 - precision: 0.9954 - recall: 0.99 - ETA: 0s - loss: 0.0166 - acc: 0.9948 - precision: 0.9954 - recall: 0.99 - ETA: 0s - loss: 0.0165 - acc: 0.9949 - precision: 0.9955 - recall: 0.99 - ETA: 0s - loss: 0.0163 - acc: 0.9950 - precision: 0.9956 - recall: 0.99 - 11s 3ms/step - loss: 0.0163 - acc: 0.9950 - precision: 0.9956 - recall: 0.9986 - val_loss: 0.0641 - val_acc: 0.9806 - val_precision: 0.9803 - val_recall: 0.9975\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.05557\n",
      "Epoch 14/100\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 0.0053 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0406 - acc: 0.9922 - precision: 0.9911 - recall: 1.000 - ETA: 10s - loss: 0.0295 - acc: 0.9948 - precision: 0.9940 - recall: 1.000 - ETA: 9s - loss: 0.0263 - acc: 0.9961 - precision: 0.9954 - recall: 1.000 - ETA: 9s - loss: 0.0217 - acc: 0.9969 - precision: 0.9963 - recall: 1.00 - ETA: 9s - loss: 0.0210 - acc: 0.9948 - precision: 0.9940 - recall: 1.00 - ETA: 9s - loss: 0.0191 - acc: 0.9955 - precision: 0.9949 - recall: 1.00 - ETA: 8s - loss: 0.0411 - acc: 0.9941 - precision: 0.9955 - recall: 0.99 - ETA: 8s - loss: 0.0383 - acc: 0.9948 - precision: 0.9960 - recall: 0.99 - ETA: 8s - loss: 0.0350 - acc: 0.9953 - precision: 0.9964 - recall: 0.99 - ETA: 8s - loss: 0.0322 - acc: 0.9957 - precision: 0.9967 - recall: 0.99 - ETA: 8s - loss: 0.0297 - acc: 0.9961 - precision: 0.9970 - recall: 0.99 - ETA: 7s - loss: 0.0277 - acc: 0.9964 - precision: 0.9972 - recall: 0.99 - ETA: 7s - loss: 0.0288 - acc: 0.9955 - precision: 0.9961 - recall: 0.99 - ETA: 7s - loss: 0.0272 - acc: 0.9958 - precision: 0.9964 - recall: 0.99 - ETA: 7s - loss: 0.0257 - acc: 0.9961 - precision: 0.9966 - recall: 0.99 - ETA: 7s - loss: 0.0267 - acc: 0.9954 - precision: 0.9957 - recall: 0.99 - ETA: 7s - loss: 0.0266 - acc: 0.9957 - precision: 0.9960 - recall: 0.99 - ETA: 7s - loss: 0.0261 - acc: 0.9951 - precision: 0.9952 - recall: 0.99 - ETA: 7s - loss: 0.0251 - acc: 0.9953 - precision: 0.9955 - recall: 0.99 - ETA: 6s - loss: 0.0241 - acc: 0.9955 - precision: 0.9957 - recall: 0.99 - ETA: 6s - loss: 0.0234 - acc: 0.9957 - precision: 0.9959 - recall: 0.99 - ETA: 6s - loss: 0.0239 - acc: 0.9952 - precision: 0.9953 - recall: 0.99 - ETA: 6s - loss: 0.0237 - acc: 0.9948 - precision: 0.9955 - recall: 0.99 - ETA: 6s - loss: 0.0237 - acc: 0.9944 - precision: 0.9949 - recall: 0.99 - ETA: 6s - loss: 0.0231 - acc: 0.9946 - precision: 0.9951 - recall: 0.99 - ETA: 5s - loss: 0.0224 - acc: 0.9948 - precision: 0.9953 - recall: 0.99 - ETA: 5s - loss: 0.0221 - acc: 0.9944 - precision: 0.9948 - recall: 0.99 - ETA: 5s - loss: 0.0222 - acc: 0.9946 - precision: 0.9950 - recall: 0.99 - ETA: 5s - loss: 0.0227 - acc: 0.9943 - precision: 0.9946 - recall: 0.99 - ETA: 5s - loss: 0.0224 - acc: 0.9945 - precision: 0.9948 - recall: 0.99 - ETA: 5s - loss: 0.0217 - acc: 0.9946 - precision: 0.9949 - recall: 0.99 - ETA: 5s - loss: 0.0212 - acc: 0.9948 - precision: 0.9951 - recall: 0.99 - ETA: 4s - loss: 0.0206 - acc: 0.9949 - precision: 0.9952 - recall: 0.99 - ETA: 4s - loss: 0.0209 - acc: 0.9942 - precision: 0.9949 - recall: 0.99 - ETA: 4s - loss: 0.0205 - acc: 0.9944 - precision: 0.9950 - recall: 0.99 - ETA: 4s - loss: 0.0200 - acc: 0.9945 - precision: 0.9951 - recall: 0.99 - ETA: 4s - loss: 0.0195 - acc: 0.9947 - precision: 0.9953 - recall: 0.99 - ETA: 4s - loss: 0.0192 - acc: 0.9948 - precision: 0.9954 - recall: 0.99 - ETA: 4s - loss: 0.0188 - acc: 0.9949 - precision: 0.9955 - recall: 0.99 - ETA: 4s - loss: 0.0200 - acc: 0.9947 - precision: 0.9952 - recall: 0.99 - ETA: 3s - loss: 0.0196 - acc: 0.9948 - precision: 0.9953 - recall: 0.99 - ETA: 3s - loss: 0.0194 - acc: 0.9949 - precision: 0.9954 - recall: 0.99 - ETA: 3s - loss: 0.0193 - acc: 0.9950 - precision: 0.9955 - recall: 0.99 - ETA: 3s - loss: 0.0192 - acc: 0.9948 - precision: 0.9952 - recall: 0.99 - ETA: 3s - loss: 0.0190 - acc: 0.9949 - precision: 0.9953 - recall: 0.99 - ETA: 3s - loss: 0.0187 - acc: 0.9950 - precision: 0.9954 - recall: 0.99 - ETA: 2s - loss: 0.0183 - acc: 0.9951 - precision: 0.9955 - recall: 0.99 - ETA: 2s - loss: 0.0180 - acc: 0.9952 - precision: 0.9956 - recall: 0.99 - ETA: 2s - loss: 0.0176 - acc: 0.9953 - precision: 0.9957 - recall: 0.99 - ETA: 2s - loss: 0.0174 - acc: 0.9954 - precision: 0.9958 - recall: 0.99 - ETA: 2s - loss: 0.0171 - acc: 0.9955 - precision: 0.9959 - recall: 0.99 - ETA: 2s - loss: 0.0169 - acc: 0.9956 - precision: 0.9959 - recall: 0.99 - ETA: 1s - loss: 0.0166 - acc: 0.9957 - precision: 0.9960 - recall: 0.99 - ETA: 1s - loss: 0.0164 - acc: 0.9957 - precision: 0.9961 - recall: 0.99 - ETA: 1s - loss: 0.0181 - acc: 0.9953 - precision: 0.9955 - recall: 0.99 - ETA: 1s - loss: 0.0186 - acc: 0.9951 - precision: 0.9956 - recall: 0.99 - ETA: 1s - loss: 0.0185 - acc: 0.9952 - precision: 0.9957 - recall: 0.99 - ETA: 1s - loss: 0.0182 - acc: 0.9952 - precision: 0.9957 - recall: 0.99 - ETA: 0s - loss: 0.0192 - acc: 0.9948 - precision: 0.9952 - recall: 0.99 - ETA: 0s - loss: 0.0197 - acc: 0.9944 - precision: 0.9950 - recall: 0.99 - ETA: 0s - loss: 0.0194 - acc: 0.9945 - precision: 0.9951 - recall: 0.99 - ETA: 0s - loss: 0.0192 - acc: 0.9945 - precision: 0.9952 - recall: 0.99 - ETA: 0s - loss: 0.0190 - acc: 0.9946 - precision: 0.9952 - recall: 0.99 - ETA: 0s - loss: 0.0187 - acc: 0.9947 - precision: 0.9953 - recall: 0.99 - 13s 3ms/step - loss: 0.0191 - acc: 0.9945 - precision: 0.9953 - recall: 0.9983 - val_loss: 0.0926 - val_acc: 0.9785 - val_precision: 0.9763 - val_recall: 0.9992\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.05557\n",
      "Epoch 15/100\n",
      "4180/4180 [==============================] - ETA: 14s - loss: 0.0115 - acc: 0.9844 - precision: 0.9839 - recall: 1.000 - ETA: 14s - loss: 0.0067 - acc: 0.9922 - precision: 0.9919 - recall: 1.000 - ETA: 14s - loss: 0.0218 - acc: 0.9896 - precision: 0.9884 - recall: 1.000 - ETA: 13s - loss: 0.0168 - acc: 0.9922 - precision: 0.9912 - recall: 1.000 - ETA: 13s - loss: 0.0141 - acc: 0.9938 - precision: 0.9929 - recall: 1.000 - ETA: 13s - loss: 0.0154 - acc: 0.9922 - precision: 0.9910 - recall: 1.000 - ETA: 12s - loss: 0.0153 - acc: 0.9933 - precision: 0.9923 - recall: 1.000 - ETA: 13s - loss: 0.0145 - acc: 0.9941 - precision: 0.9932 - recall: 1.000 - ETA: 12s - loss: 0.0134 - acc: 0.9948 - precision: 0.9940 - recall: 1.000 - ETA: 12s - loss: 0.0133 - acc: 0.9953 - precision: 0.9945 - recall: 1.000 - ETA: 12s - loss: 0.0122 - acc: 0.9957 - precision: 0.9950 - recall: 1.000 - ETA: 12s - loss: 0.0114 - acc: 0.9961 - precision: 0.9955 - recall: 1.000 - ETA: 11s - loss: 0.0107 - acc: 0.9964 - precision: 0.9958 - recall: 1.000 - ETA: 11s - loss: 0.0155 - acc: 0.9955 - precision: 0.9948 - recall: 1.000 - ETA: 11s - loss: 0.0162 - acc: 0.9948 - precision: 0.9951 - recall: 0.998 - ETA: 11s - loss: 0.0156 - acc: 0.9951 - precision: 0.9954 - recall: 0.998 - ETA: 10s - loss: 0.0162 - acc: 0.9945 - precision: 0.9946 - recall: 0.998 - ETA: 10s - loss: 0.0156 - acc: 0.9948 - precision: 0.9949 - recall: 0.999 - ETA: 10s - loss: 0.0152 - acc: 0.9951 - precision: 0.9952 - recall: 0.999 - ETA: 10s - loss: 0.0146 - acc: 0.9953 - precision: 0.9955 - recall: 0.999 - ETA: 9s - loss: 0.0142 - acc: 0.9955 - precision: 0.9957 - recall: 0.999 - ETA: 9s - loss: 0.0136 - acc: 0.9957 - precision: 0.9959 - recall: 0.99 - ETA: 9s - loss: 0.0130 - acc: 0.9959 - precision: 0.9961 - recall: 0.99 - ETA: 9s - loss: 0.0125 - acc: 0.9961 - precision: 0.9963 - recall: 0.99 - ETA: 9s - loss: 0.0122 - acc: 0.9962 - precision: 0.9964 - recall: 0.99 - ETA: 8s - loss: 0.0118 - acc: 0.9964 - precision: 0.9965 - recall: 0.99 - ETA: 8s - loss: 0.0114 - acc: 0.9965 - precision: 0.9967 - recall: 0.99 - ETA: 8s - loss: 0.0110 - acc: 0.9967 - precision: 0.9968 - recall: 0.99 - ETA: 8s - loss: 0.0107 - acc: 0.9968 - precision: 0.9969 - recall: 0.99 - ETA: 7s - loss: 0.0103 - acc: 0.9969 - precision: 0.9970 - recall: 0.99 - ETA: 7s - loss: 0.0100 - acc: 0.9970 - precision: 0.9971 - recall: 0.99 - ETA: 7s - loss: 0.0098 - acc: 0.9971 - precision: 0.9972 - recall: 0.99 - ETA: 7s - loss: 0.0096 - acc: 0.9972 - precision: 0.9973 - recall: 0.99 - ETA: 7s - loss: 0.0122 - acc: 0.9968 - precision: 0.9968 - recall: 0.99 - ETA: 6s - loss: 0.0122 - acc: 0.9969 - precision: 0.9969 - recall: 0.99 - ETA: 6s - loss: 0.0119 - acc: 0.9970 - precision: 0.9970 - recall: 0.99 - ETA: 6s - loss: 0.0118 - acc: 0.9970 - precision: 0.9971 - recall: 0.99 - ETA: 6s - loss: 0.0119 - acc: 0.9967 - precision: 0.9972 - recall: 0.99 - ETA: 5s - loss: 0.0116 - acc: 0.9968 - precision: 0.9972 - recall: 0.99 - ETA: 5s - loss: 0.0114 - acc: 0.9969 - precision: 0.9973 - recall: 0.99 - ETA: 5s - loss: 0.0114 - acc: 0.9970 - precision: 0.9974 - recall: 0.99 - ETA: 5s - loss: 0.0115 - acc: 0.9967 - precision: 0.9970 - recall: 0.99 - ETA: 5s - loss: 0.0113 - acc: 0.9967 - precision: 0.9971 - recall: 0.99 - ETA: 4s - loss: 0.0113 - acc: 0.9968 - precision: 0.9971 - recall: 0.99 - ETA: 4s - loss: 0.0122 - acc: 0.9965 - precision: 0.9968 - recall: 0.99 - ETA: 4s - loss: 0.0121 - acc: 0.9966 - precision: 0.9969 - recall: 0.99 - ETA: 4s - loss: 0.0123 - acc: 0.9963 - precision: 0.9966 - recall: 0.99 - ETA: 3s - loss: 0.0122 - acc: 0.9964 - precision: 0.9966 - recall: 0.99 - ETA: 3s - loss: 0.0121 - acc: 0.9965 - precision: 0.9967 - recall: 0.99 - ETA: 3s - loss: 0.0119 - acc: 0.9966 - precision: 0.9968 - recall: 0.99 - ETA: 3s - loss: 0.0117 - acc: 0.9966 - precision: 0.9968 - recall: 0.99 - ETA: 2s - loss: 0.0125 - acc: 0.9964 - precision: 0.9966 - recall: 0.99 - ETA: 2s - loss: 0.0123 - acc: 0.9965 - precision: 0.9966 - recall: 0.99 - ETA: 2s - loss: 0.0125 - acc: 0.9962 - precision: 0.9963 - recall: 0.99 - ETA: 2s - loss: 0.0124 - acc: 0.9963 - precision: 0.9964 - recall: 0.99 - ETA: 2s - loss: 0.0123 - acc: 0.9964 - precision: 0.9965 - recall: 0.99 - ETA: 1s - loss: 0.0129 - acc: 0.9962 - precision: 0.9962 - recall: 0.99 - ETA: 1s - loss: 0.0128 - acc: 0.9962 - precision: 0.9963 - recall: 0.99 - ETA: 1s - loss: 0.0128 - acc: 0.9963 - precision: 0.9963 - recall: 0.99 - ETA: 1s - loss: 0.0128 - acc: 0.9961 - precision: 0.9964 - recall: 0.99 - ETA: 0s - loss: 0.0127 - acc: 0.9962 - precision: 0.9965 - recall: 0.99 - ETA: 0s - loss: 0.0125 - acc: 0.9962 - precision: 0.9965 - recall: 0.99 - ETA: 0s - loss: 0.0125 - acc: 0.9960 - precision: 0.9963 - recall: 0.99 - ETA: 0s - loss: 0.0124 - acc: 0.9961 - precision: 0.9964 - recall: 0.99 - ETA: 0s - loss: 0.0123 - acc: 0.9962 - precision: 0.9964 - recall: 0.99 - 16s 4ms/step - loss: 0.0122 - acc: 0.9962 - precision: 0.9964 - recall: 0.9992 - val_loss: 0.0816 - val_acc: 0.9821 - val_precision: 0.9908 - val_recall: 0.9883\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.05557\n",
      "Epoch 16/100\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 0.0140 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0111 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0161 - acc: 0.9948 - precision: 0.9939 - recall: 1.00 - ETA: 8s - loss: 0.0193 - acc: 0.9922 - precision: 0.9955 - recall: 0.99 - ETA: 9s - loss: 0.0160 - acc: 0.9938 - precision: 0.9964 - recall: 0.99 - ETA: 9s - loss: 0.0134 - acc: 0.9948 - precision: 0.9970 - recall: 0.99 - ETA: 9s - loss: 0.0117 - acc: 0.9955 - precision: 0.9974 - recall: 0.99 - ETA: 9s - loss: 0.0119 - acc: 0.9941 - precision: 0.9978 - recall: 0.99 - ETA: 8s - loss: 0.0107 - acc: 0.9948 - precision: 0.9980 - recall: 0.99 - ETA: 8s - loss: 0.0116 - acc: 0.9938 - precision: 0.9964 - recall: 0.99 - ETA: 8s - loss: 0.0111 - acc: 0.9943 - precision: 0.9967 - recall: 0.99 - ETA: 8s - loss: 0.0103 - acc: 0.9948 - precision: 0.9970 - recall: 0.99 - ETA: 8s - loss: 0.0105 - acc: 0.9940 - precision: 0.9958 - recall: 0.99 - ETA: 8s - loss: 0.0117 - acc: 0.9933 - precision: 0.9962 - recall: 0.99 - ETA: 7s - loss: 0.0135 - acc: 0.9927 - precision: 0.9952 - recall: 0.99 - ETA: 7s - loss: 0.0137 - acc: 0.9932 - precision: 0.9955 - recall: 0.99 - ETA: 7s - loss: 0.0138 - acc: 0.9936 - precision: 0.9958 - recall: 0.99 - ETA: 7s - loss: 0.0133 - acc: 0.9939 - precision: 0.9960 - recall: 0.99 - ETA: 7s - loss: 0.0128 - acc: 0.9942 - precision: 0.9962 - recall: 0.99 - ETA: 7s - loss: 0.0125 - acc: 0.9945 - precision: 0.9964 - recall: 0.99 - ETA: 6s - loss: 0.0128 - acc: 0.9940 - precision: 0.9957 - recall: 0.99 - ETA: 6s - loss: 0.0144 - acc: 0.9929 - precision: 0.9959 - recall: 0.99 - ETA: 6s - loss: 0.0139 - acc: 0.9932 - precision: 0.9961 - recall: 0.99 - ETA: 6s - loss: 0.0139 - acc: 0.9928 - precision: 0.9955 - recall: 0.99 - ETA: 6s - loss: 0.0133 - acc: 0.9931 - precision: 0.9957 - recall: 0.99 - ETA: 6s - loss: 0.0128 - acc: 0.9934 - precision: 0.9958 - recall: 0.99 - ETA: 5s - loss: 0.0125 - acc: 0.9936 - precision: 0.9960 - recall: 0.99 - ETA: 5s - loss: 0.0137 - acc: 0.9933 - precision: 0.9955 - recall: 0.99 - ETA: 5s - loss: 0.0133 - acc: 0.9935 - precision: 0.9956 - recall: 0.99 - ETA: 5s - loss: 0.0130 - acc: 0.9938 - precision: 0.9958 - recall: 0.99 - ETA: 5s - loss: 0.0126 - acc: 0.9940 - precision: 0.9959 - recall: 0.99 - ETA: 5s - loss: 0.0123 - acc: 0.9941 - precision: 0.9961 - recall: 0.99 - ETA: 5s - loss: 0.0135 - acc: 0.9929 - precision: 0.9946 - recall: 0.99 - ETA: 4s - loss: 0.0132 - acc: 0.9931 - precision: 0.9947 - recall: 0.99 - ETA: 4s - loss: 0.0129 - acc: 0.9933 - precision: 0.9949 - recall: 0.99 - ETA: 4s - loss: 0.0126 - acc: 0.9935 - precision: 0.9950 - recall: 0.99 - ETA: 4s - loss: 0.0124 - acc: 0.9937 - precision: 0.9951 - recall: 0.99 - ETA: 4s - loss: 0.0128 - acc: 0.9934 - precision: 0.9953 - recall: 0.99 - ETA: 4s - loss: 0.0124 - acc: 0.9936 - precision: 0.9954 - recall: 0.99 - ETA: 4s - loss: 0.0122 - acc: 0.9938 - precision: 0.9955 - recall: 0.99 - ETA: 3s - loss: 0.0121 - acc: 0.9939 - precision: 0.9956 - recall: 0.99 - ETA: 3s - loss: 0.0124 - acc: 0.9937 - precision: 0.9953 - recall: 0.99 - ETA: 3s - loss: 0.0122 - acc: 0.9938 - precision: 0.9954 - recall: 0.99 - ETA: 3s - loss: 0.0123 - acc: 0.9936 - precision: 0.9955 - recall: 0.99 - ETA: 3s - loss: 0.0120 - acc: 0.9938 - precision: 0.9956 - recall: 0.99 - ETA: 3s - loss: 0.0118 - acc: 0.9939 - precision: 0.9957 - recall: 0.99 - ETA: 2s - loss: 0.0117 - acc: 0.9940 - precision: 0.9958 - recall: 0.99 - ETA: 2s - loss: 0.0116 - acc: 0.9941 - precision: 0.9959 - recall: 0.99 - ETA: 2s - loss: 0.0114 - acc: 0.9943 - precision: 0.9960 - recall: 0.99 - ETA: 2s - loss: 0.0113 - acc: 0.9944 - precision: 0.9961 - recall: 0.99 - ETA: 2s - loss: 0.0112 - acc: 0.9945 - precision: 0.9961 - recall: 0.99 - ETA: 2s - loss: 0.0110 - acc: 0.9946 - precision: 0.9962 - recall: 0.99 - ETA: 1s - loss: 0.0110 - acc: 0.9947 - precision: 0.9963 - recall: 0.99 - ETA: 1s - loss: 0.0108 - acc: 0.9948 - precision: 0.9963 - recall: 0.99 - ETA: 1s - loss: 0.0106 - acc: 0.9949 - precision: 0.9964 - recall: 0.99 - ETA: 1s - loss: 0.0104 - acc: 0.9950 - precision: 0.9965 - recall: 0.99 - ETA: 1s - loss: 0.0103 - acc: 0.9951 - precision: 0.9965 - recall: 0.99 - ETA: 1s - loss: 0.0102 - acc: 0.9952 - precision: 0.9966 - recall: 0.99 - ETA: 0s - loss: 0.0102 - acc: 0.9952 - precision: 0.9966 - recall: 0.99 - ETA: 0s - loss: 0.0101 - acc: 0.9953 - precision: 0.9967 - recall: 0.99 - ETA: 0s - loss: 0.0100 - acc: 0.9954 - precision: 0.9968 - recall: 0.99 - ETA: 0s - loss: 0.0099 - acc: 0.9955 - precision: 0.9968 - recall: 0.99 - ETA: 0s - loss: 0.0098 - acc: 0.9955 - precision: 0.9969 - recall: 0.99 - ETA: 0s - loss: 0.0097 - acc: 0.9956 - precision: 0.9969 - recall: 0.99 - ETA: 0s - loss: 0.0098 - acc: 0.9954 - precision: 0.9967 - recall: 0.99 - 11s 3ms/step - loss: 0.0106 - acc: 0.9952 - precision: 0.9964 - recall: 0.9981 - val_loss: 0.1422 - val_acc: 0.9648 - val_precision: 0.9948 - val_recall: 0.9641\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.05557\n",
      "Epoch 17/100\n",
      "4180/4180 [==============================] - ETA: 15s - loss: 0.0968 - acc: 0.9688 - precision: 1.0000 - recall: 0.964 - ETA: 13s - loss: 0.0501 - acc: 0.9844 - precision: 1.0000 - recall: 0.982 - ETA: 11s - loss: 0.0338 - acc: 0.9896 - precision: 1.0000 - recall: 0.987 - ETA: 10s - loss: 0.0293 - acc: 0.9883 - precision: 0.9955 - recall: 0.991 - ETA: 10s - loss: 0.0304 - acc: 0.9875 - precision: 0.9928 - recall: 0.992 - ETA: 9s - loss: 0.0304 - acc: 0.9870 - precision: 0.9939 - recall: 0.990 - ETA: 9s - loss: 0.0303 - acc: 0.9866 - precision: 0.9923 - recall: 0.99 - ETA: 9s - loss: 0.0271 - acc: 0.9883 - precision: 0.9933 - recall: 0.99 - ETA: 9s - loss: 0.0243 - acc: 0.9896 - precision: 0.9940 - recall: 0.99 - ETA: 8s - loss: 0.0223 - acc: 0.9906 - precision: 0.9946 - recall: 0.99 - ETA: 8s - loss: 0.0207 - acc: 0.9915 - precision: 0.9951 - recall: 0.99 - ETA: 8s - loss: 0.0195 - acc: 0.9922 - precision: 0.9955 - recall: 0.99 - ETA: 8s - loss: 0.0181 - acc: 0.9928 - precision: 0.9959 - recall: 0.99 - ETA: 7s - loss: 0.0170 - acc: 0.9933 - precision: 0.9961 - recall: 0.99 - ETA: 7s - loss: 0.0159 - acc: 0.9938 - precision: 0.9964 - recall: 0.99 - ETA: 7s - loss: 0.0152 - acc: 0.9941 - precision: 0.9966 - recall: 0.99 - ETA: 7s - loss: 0.0176 - acc: 0.9926 - precision: 0.9947 - recall: 0.99 - ETA: 7s - loss: 0.0177 - acc: 0.9922 - precision: 0.9950 - recall: 0.99 - ETA: 7s - loss: 0.0169 - acc: 0.9926 - precision: 0.9952 - recall: 0.99 - ETA: 6s - loss: 0.0174 - acc: 0.9922 - precision: 0.9955 - recall: 0.99 - ETA: 6s - loss: 0.0170 - acc: 0.9926 - precision: 0.9957 - recall: 0.99 - ETA: 6s - loss: 0.0164 - acc: 0.9929 - precision: 0.9959 - recall: 0.99 - ETA: 6s - loss: 0.0180 - acc: 0.9925 - precision: 0.9953 - recall: 0.99 - ETA: 6s - loss: 0.0198 - acc: 0.9915 - precision: 0.9955 - recall: 0.99 - ETA: 6s - loss: 0.0197 - acc: 0.9912 - precision: 0.9949 - recall: 0.99 - ETA: 5s - loss: 0.0191 - acc: 0.9916 - precision: 0.9951 - recall: 0.99 - ETA: 5s - loss: 0.0185 - acc: 0.9919 - precision: 0.9953 - recall: 0.99 - ETA: 5s - loss: 0.0179 - acc: 0.9922 - precision: 0.9955 - recall: 0.99 - ETA: 5s - loss: 0.0173 - acc: 0.9925 - precision: 0.9956 - recall: 0.99 - ETA: 5s - loss: 0.0173 - acc: 0.9922 - precision: 0.9952 - recall: 0.99 - ETA: 5s - loss: 0.0171 - acc: 0.9924 - precision: 0.9954 - recall: 0.99 - ETA: 5s - loss: 0.0166 - acc: 0.9927 - precision: 0.9955 - recall: 0.99 - ETA: 4s - loss: 0.0161 - acc: 0.9929 - precision: 0.9957 - recall: 0.99 - ETA: 4s - loss: 0.0157 - acc: 0.9931 - precision: 0.9958 - recall: 0.99 - ETA: 4s - loss: 0.0153 - acc: 0.9933 - precision: 0.9959 - recall: 0.99 - ETA: 4s - loss: 0.0149 - acc: 0.9935 - precision: 0.9960 - recall: 0.99 - ETA: 4s - loss: 0.0146 - acc: 0.9937 - precision: 0.9961 - recall: 0.99 - ETA: 4s - loss: 0.0143 - acc: 0.9938 - precision: 0.9962 - recall: 0.99 - ETA: 3s - loss: 0.0140 - acc: 0.9940 - precision: 0.9963 - recall: 0.99 - ETA: 3s - loss: 0.0136 - acc: 0.9941 - precision: 0.9964 - recall: 0.99 - ETA: 3s - loss: 0.0133 - acc: 0.9943 - precision: 0.9965 - recall: 0.99 - ETA: 3s - loss: 0.0130 - acc: 0.9944 - precision: 0.9966 - recall: 0.99 - ETA: 3s - loss: 0.0130 - acc: 0.9942 - precision: 0.9963 - recall: 0.99 - ETA: 3s - loss: 0.0127 - acc: 0.9943 - precision: 0.9963 - recall: 0.99 - ETA: 3s - loss: 0.0125 - acc: 0.9944 - precision: 0.9964 - recall: 0.99 - ETA: 2s - loss: 0.0122 - acc: 0.9946 - precision: 0.9965 - recall: 0.99 - ETA: 2s - loss: 0.0120 - acc: 0.9947 - precision: 0.9966 - recall: 0.99 - ETA: 2s - loss: 0.0119 - acc: 0.9948 - precision: 0.9967 - recall: 0.99 - ETA: 2s - loss: 0.0117 - acc: 0.9949 - precision: 0.9967 - recall: 0.99 - ETA: 2s - loss: 0.0114 - acc: 0.9950 - precision: 0.9968 - recall: 0.99 - ETA: 2s - loss: 0.0113 - acc: 0.9951 - precision: 0.9969 - recall: 0.99 - ETA: 1s - loss: 0.0113 - acc: 0.9952 - precision: 0.9969 - recall: 0.99 - ETA: 1s - loss: 0.0111 - acc: 0.9953 - precision: 0.9970 - recall: 0.99 - ETA: 1s - loss: 0.0111 - acc: 0.9951 - precision: 0.9967 - recall: 0.99 - ETA: 1s - loss: 0.0112 - acc: 0.9949 - precision: 0.9968 - recall: 0.99 - ETA: 1s - loss: 0.0116 - acc: 0.9947 - precision: 0.9965 - recall: 0.99 - ETA: 1s - loss: 0.0115 - acc: 0.9948 - precision: 0.9965 - recall: 0.99 - ETA: 1s - loss: 0.0113 - acc: 0.9949 - precision: 0.9966 - recall: 0.99 - ETA: 0s - loss: 0.0112 - acc: 0.9950 - precision: 0.9967 - recall: 0.99 - ETA: 0s - loss: 0.0111 - acc: 0.9951 - precision: 0.9967 - recall: 0.99 - ETA: 0s - loss: 0.0110 - acc: 0.9951 - precision: 0.9968 - recall: 0.99 - ETA: 0s - loss: 0.0108 - acc: 0.9952 - precision: 0.9968 - recall: 0.99 - ETA: 0s - loss: 0.0106 - acc: 0.9953 - precision: 0.9969 - recall: 0.99 - ETA: 0s - loss: 0.0107 - acc: 0.9951 - precision: 0.9966 - recall: 0.99 - ETA: 0s - loss: 0.0106 - acc: 0.9952 - precision: 0.9967 - recall: 0.99 - 11s 3ms/step - loss: 0.0105 - acc: 0.9952 - precision: 0.9967 - recall: 0.9978 - val_loss: 0.0873 - val_acc: 0.9799 - val_precision: 0.9908 - val_recall: 0.9858\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.05557\n",
      "Epoch 18/100\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 8.5666e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0018 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000    - ETA: 9s - loss: 0.0016 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0021 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0017 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0015 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0013 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0012 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0017 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0018 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0033 - acc: 0.9986 - precision: 0.9984 - recall: 1.00 - ETA: 7s - loss: 0.0038 - acc: 0.9987 - precision: 0.9985 - recall: 1.00 - ETA: 7s - loss: 0.0036 - acc: 0.9988 - precision: 0.9986 - recall: 1.00 - ETA: 7s - loss: 0.0034 - acc: 0.9989 - precision: 0.9987 - recall: 1.00 - ETA: 7s - loss: 0.0035 - acc: 0.9990 - precision: 0.9988 - recall: 1.00 - ETA: 7s - loss: 0.0065 - acc: 0.9980 - precision: 0.9978 - recall: 1.00 - ETA: 7s - loss: 0.0067 - acc: 0.9982 - precision: 0.9979 - recall: 1.00 - ETA: 6s - loss: 0.0064 - acc: 0.9983 - precision: 0.9980 - recall: 1.00 - ETA: 6s - loss: 0.0061 - acc: 0.9984 - precision: 0.9981 - recall: 1.00 - ETA: 6s - loss: 0.0088 - acc: 0.9977 - precision: 0.9973 - recall: 1.00 - ETA: 6s - loss: 0.0086 - acc: 0.9978 - precision: 0.9975 - recall: 1.00 - ETA: 6s - loss: 0.0083 - acc: 0.9979 - precision: 0.9976 - recall: 1.00 - ETA: 6s - loss: 0.0087 - acc: 0.9973 - precision: 0.9969 - recall: 1.00 - ETA: 6s - loss: 0.0086 - acc: 0.9974 - precision: 0.9970 - recall: 1.00 - ETA: 5s - loss: 0.0083 - acc: 0.9975 - precision: 0.9971 - recall: 1.00 - ETA: 5s - loss: 0.0094 - acc: 0.9970 - precision: 0.9966 - recall: 1.00 - ETA: 5s - loss: 0.0093 - acc: 0.9971 - precision: 0.9967 - recall: 1.00 - ETA: 5s - loss: 0.0090 - acc: 0.9972 - precision: 0.9968 - recall: 1.00 - ETA: 5s - loss: 0.0088 - acc: 0.9973 - precision: 0.9969 - recall: 1.00 - ETA: 5s - loss: 0.0089 - acc: 0.9974 - precision: 0.9970 - recall: 1.00 - ETA: 4s - loss: 0.0110 - acc: 0.9965 - precision: 0.9960 - recall: 1.00 - ETA: 4s - loss: 0.0128 - acc: 0.9951 - precision: 0.9961 - recall: 0.99 - ETA: 4s - loss: 0.0126 - acc: 0.9953 - precision: 0.9962 - recall: 0.99 - ETA: 4s - loss: 0.0124 - acc: 0.9954 - precision: 0.9963 - recall: 0.99 - ETA: 4s - loss: 0.0122 - acc: 0.9955 - precision: 0.9964 - recall: 0.99 - ETA: 4s - loss: 0.0122 - acc: 0.9957 - precision: 0.9965 - recall: 0.99 - ETA: 4s - loss: 0.0119 - acc: 0.9958 - precision: 0.9966 - recall: 0.99 - ETA: 3s - loss: 0.0116 - acc: 0.9959 - precision: 0.9967 - recall: 0.99 - ETA: 3s - loss: 0.0114 - acc: 0.9960 - precision: 0.9968 - recall: 0.99 - ETA: 3s - loss: 0.0111 - acc: 0.9961 - precision: 0.9969 - recall: 0.99 - ETA: 3s - loss: 0.0109 - acc: 0.9962 - precision: 0.9969 - recall: 0.99 - ETA: 3s - loss: 0.0106 - acc: 0.9963 - precision: 0.9970 - recall: 0.99 - ETA: 3s - loss: 0.0104 - acc: 0.9964 - precision: 0.9971 - recall: 0.99 - ETA: 3s - loss: 0.0105 - acc: 0.9964 - precision: 0.9971 - recall: 0.99 - ETA: 2s - loss: 0.0103 - acc: 0.9965 - precision: 0.9972 - recall: 0.99 - ETA: 2s - loss: 0.0102 - acc: 0.9966 - precision: 0.9973 - recall: 0.99 - ETA: 2s - loss: 0.0100 - acc: 0.9967 - precision: 0.9973 - recall: 0.99 - ETA: 2s - loss: 0.0098 - acc: 0.9967 - precision: 0.9974 - recall: 0.99 - ETA: 2s - loss: 0.0097 - acc: 0.9968 - precision: 0.9974 - recall: 0.99 - ETA: 2s - loss: 0.0095 - acc: 0.9969 - precision: 0.9975 - recall: 0.99 - ETA: 2s - loss: 0.0094 - acc: 0.9969 - precision: 0.9975 - recall: 0.99 - ETA: 1s - loss: 0.0092 - acc: 0.9970 - precision: 0.9976 - recall: 0.99 - ETA: 1s - loss: 0.0095 - acc: 0.9968 - precision: 0.9973 - recall: 0.99 - ETA: 1s - loss: 0.0095 - acc: 0.9968 - precision: 0.9973 - recall: 0.99 - ETA: 1s - loss: 0.0093 - acc: 0.9969 - precision: 0.9974 - recall: 0.99 - ETA: 1s - loss: 0.0092 - acc: 0.9969 - precision: 0.9974 - recall: 0.99 - ETA: 1s - loss: 0.0091 - acc: 0.9970 - precision: 0.9975 - recall: 0.99 - ETA: 1s - loss: 0.0089 - acc: 0.9970 - precision: 0.9975 - recall: 0.99 - ETA: 0s - loss: 0.0092 - acc: 0.9968 - precision: 0.9973 - recall: 0.99 - ETA: 0s - loss: 0.0092 - acc: 0.9969 - precision: 0.9973 - recall: 0.99 - ETA: 0s - loss: 0.0091 - acc: 0.9969 - precision: 0.9973 - recall: 0.99 - ETA: 0s - loss: 0.0100 - acc: 0.9967 - precision: 0.9971 - recall: 0.99 - ETA: 0s - loss: 0.0099 - acc: 0.9968 - precision: 0.9971 - recall: 0.99 - ETA: 0s - loss: 0.0099 - acc: 0.9968 - precision: 0.9972 - recall: 0.99 - ETA: 0s - loss: 0.0103 - acc: 0.9966 - precision: 0.9970 - recall: 0.99 - 11s 3ms/step - loss: 0.0103 - acc: 0.9967 - precision: 0.9970 - recall: 0.9992 - val_loss: 0.0644 - val_acc: 0.9864 - val_precision: 0.9917 - val_recall: 0.9925\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.05557\n",
      "Epoch 19/100\n",
      "4180/4180 [==============================] - ETA: 12s - loss: 0.0040 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 11s - loss: 0.0027 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0024 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0019 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0021 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0031 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0029 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0026 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0034 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0034 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0031 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0029 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0028 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0048 - acc: 0.9978 - precision: 0.9975 - recall: 1.00 - ETA: 7s - loss: 0.0048 - acc: 0.9979 - precision: 0.9976 - recall: 1.00 - ETA: 7s - loss: 0.0047 - acc: 0.9980 - precision: 0.9978 - recall: 1.00 - ETA: 7s - loss: 0.0051 - acc: 0.9982 - precision: 0.9979 - recall: 1.00 - ETA: 7s - loss: 0.0049 - acc: 0.9983 - precision: 0.9980 - recall: 1.00 - ETA: 7s - loss: 0.0050 - acc: 0.9984 - precision: 0.9981 - recall: 1.00 - ETA: 7s - loss: 0.0056 - acc: 0.9977 - precision: 0.9973 - recall: 1.00 - ETA: 6s - loss: 0.0055 - acc: 0.9978 - precision: 0.9974 - recall: 1.00 - ETA: 6s - loss: 0.0080 - acc: 0.9972 - precision: 0.9976 - recall: 0.99 - ETA: 6s - loss: 0.0092 - acc: 0.9966 - precision: 0.9969 - recall: 0.99 - ETA: 6s - loss: 0.0089 - acc: 0.9967 - precision: 0.9970 - recall: 0.99 - ETA: 6s - loss: 0.0086 - acc: 0.9969 - precision: 0.9971 - recall: 0.99 - ETA: 6s - loss: 0.0096 - acc: 0.9964 - precision: 0.9966 - recall: 0.99 - ETA: 5s - loss: 0.0093 - acc: 0.9965 - precision: 0.9967 - recall: 0.99 - ETA: 5s - loss: 0.0091 - acc: 0.9967 - precision: 0.9968 - recall: 0.99 - ETA: 5s - loss: 0.0092 - acc: 0.9968 - precision: 0.9969 - recall: 0.99 - ETA: 5s - loss: 0.0089 - acc: 0.9969 - precision: 0.9970 - recall: 0.99 - ETA: 5s - loss: 0.0089 - acc: 0.9970 - precision: 0.9971 - recall: 0.99 - ETA: 5s - loss: 0.0087 - acc: 0.9971 - precision: 0.9972 - recall: 0.99 - ETA: 5s - loss: 0.0084 - acc: 0.9972 - precision: 0.9973 - recall: 0.99 - ETA: 4s - loss: 0.0082 - acc: 0.9972 - precision: 0.9974 - recall: 0.99 - ETA: 4s - loss: 0.0080 - acc: 0.9973 - precision: 0.9974 - recall: 0.99 - ETA: 4s - loss: 0.0078 - acc: 0.9974 - precision: 0.9975 - recall: 0.99 - ETA: 4s - loss: 0.0076 - acc: 0.9975 - precision: 0.9976 - recall: 0.99 - ETA: 4s - loss: 0.0075 - acc: 0.9975 - precision: 0.9976 - recall: 0.99 - ETA: 4s - loss: 0.0073 - acc: 0.9976 - precision: 0.9977 - recall: 0.99 - ETA: 3s - loss: 0.0071 - acc: 0.9977 - precision: 0.9978 - recall: 0.99 - ETA: 3s - loss: 0.0070 - acc: 0.9977 - precision: 0.9978 - recall: 0.99 - ETA: 3s - loss: 0.0068 - acc: 0.9978 - precision: 0.9979 - recall: 0.99 - ETA: 3s - loss: 0.0067 - acc: 0.9978 - precision: 0.9979 - recall: 0.99 - ETA: 3s - loss: 0.0066 - acc: 0.9979 - precision: 0.9980 - recall: 0.99 - ETA: 3s - loss: 0.0065 - acc: 0.9979 - precision: 0.9980 - recall: 0.99 - ETA: 3s - loss: 0.0064 - acc: 0.9980 - precision: 0.9980 - recall: 0.99 - ETA: 2s - loss: 0.0063 - acc: 0.9980 - precision: 0.9981 - recall: 0.99 - ETA: 2s - loss: 0.0066 - acc: 0.9977 - precision: 0.9981 - recall: 0.99 - ETA: 2s - loss: 0.0079 - acc: 0.9974 - precision: 0.9978 - recall: 0.99 - ETA: 2s - loss: 0.0078 - acc: 0.9975 - precision: 0.9978 - recall: 0.99 - ETA: 2s - loss: 0.0082 - acc: 0.9972 - precision: 0.9975 - recall: 0.99 - ETA: 2s - loss: 0.0081 - acc: 0.9973 - precision: 0.9976 - recall: 0.99 - ETA: 1s - loss: 0.0080 - acc: 0.9973 - precision: 0.9976 - recall: 0.99 - ETA: 1s - loss: 0.0080 - acc: 0.9974 - precision: 0.9977 - recall: 0.99 - ETA: 1s - loss: 0.0079 - acc: 0.9974 - precision: 0.9977 - recall: 0.99 - ETA: 1s - loss: 0.0079 - acc: 0.9975 - precision: 0.9977 - recall: 0.99 - ETA: 1s - loss: 0.0078 - acc: 0.9975 - precision: 0.9978 - recall: 0.99 - ETA: 1s - loss: 0.0076 - acc: 0.9976 - precision: 0.9978 - recall: 0.99 - ETA: 0s - loss: 0.0075 - acc: 0.9976 - precision: 0.9979 - recall: 0.99 - ETA: 0s - loss: 0.0074 - acc: 0.9977 - precision: 0.9979 - recall: 0.99 - ETA: 0s - loss: 0.0076 - acc: 0.9974 - precision: 0.9976 - recall: 0.99 - ETA: 0s - loss: 0.0075 - acc: 0.9975 - precision: 0.9977 - recall: 0.99 - ETA: 0s - loss: 0.0074 - acc: 0.9975 - precision: 0.9977 - recall: 0.99 - ETA: 0s - loss: 0.0076 - acc: 0.9976 - precision: 0.9978 - recall: 0.99 - ETA: 0s - loss: 0.0075 - acc: 0.9976 - precision: 0.9978 - recall: 0.99 - 11s 3ms/step - loss: 0.0075 - acc: 0.9976 - precision: 0.9978 - recall: 0.9994 - val_loss: 0.0998 - val_acc: 0.9806 - val_precision: 0.9811 - val_recall: 0.9967\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.05557\n",
      "Epoch 20/100\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 0.0696 - acc: 0.9531 - precision: 0.9455 - recall: 1.000 - ETA: 10s - loss: 0.0372 - acc: 0.9766 - precision: 0.9712 - recall: 1.000 - ETA: 9s - loss: 0.0256 - acc: 0.9844 - precision: 0.9811 - recall: 1.000 - ETA: 9s - loss: 0.0197 - acc: 0.9883 - precision: 0.9861 - recall: 1.00 - ETA: 8s - loss: 0.0166 - acc: 0.9906 - precision: 0.9890 - recall: 1.00 - ETA: 8s - loss: 0.0182 - acc: 0.9896 - precision: 0.9909 - recall: 0.99 - ETA: 8s - loss: 0.0157 - acc: 0.9911 - precision: 0.9922 - recall: 0.99 - ETA: 8s - loss: 0.0177 - acc: 0.9902 - precision: 0.9909 - recall: 0.99 - ETA: 8s - loss: 0.0160 - acc: 0.9913 - precision: 0.9919 - recall: 0.99 - ETA: 7s - loss: 0.0145 - acc: 0.9922 - precision: 0.9928 - recall: 0.99 - ETA: 7s - loss: 0.0146 - acc: 0.9929 - precision: 0.9935 - recall: 0.99 - ETA: 7s - loss: 0.0206 - acc: 0.9922 - precision: 0.9940 - recall: 0.99 - ETA: 7s - loss: 0.0190 - acc: 0.9928 - precision: 0.9945 - recall: 0.99 - ETA: 7s - loss: 0.0177 - acc: 0.9933 - precision: 0.9949 - recall: 0.99 - ETA: 7s - loss: 0.0167 - acc: 0.9938 - precision: 0.9953 - recall: 0.99 - ETA: 7s - loss: 0.0157 - acc: 0.9941 - precision: 0.9956 - recall: 0.99 - ETA: 7s - loss: 0.0153 - acc: 0.9945 - precision: 0.9958 - recall: 0.99 - ETA: 6s - loss: 0.0145 - acc: 0.9948 - precision: 0.9961 - recall: 0.99 - ETA: 6s - loss: 0.0139 - acc: 0.9951 - precision: 0.9963 - recall: 0.99 - ETA: 6s - loss: 0.0133 - acc: 0.9953 - precision: 0.9965 - recall: 0.99 - ETA: 6s - loss: 0.0129 - acc: 0.9955 - precision: 0.9967 - recall: 0.99 - ETA: 6s - loss: 0.0142 - acc: 0.9950 - precision: 0.9960 - recall: 0.99 - ETA: 6s - loss: 0.0136 - acc: 0.9952 - precision: 0.9962 - recall: 0.99 - ETA: 5s - loss: 0.0132 - acc: 0.9954 - precision: 0.9963 - recall: 0.99 - ETA: 5s - loss: 0.0127 - acc: 0.9956 - precision: 0.9965 - recall: 0.99 - ETA: 5s - loss: 0.0128 - acc: 0.9958 - precision: 0.9966 - recall: 0.99 - ETA: 5s - loss: 0.0124 - acc: 0.9959 - precision: 0.9967 - recall: 0.99 - ETA: 5s - loss: 0.0120 - acc: 0.9961 - precision: 0.9969 - recall: 0.99 - ETA: 5s - loss: 0.0117 - acc: 0.9962 - precision: 0.9970 - recall: 0.99 - ETA: 5s - loss: 0.0113 - acc: 0.9964 - precision: 0.9971 - recall: 0.99 - ETA: 4s - loss: 0.0110 - acc: 0.9965 - precision: 0.9971 - recall: 0.99 - ETA: 4s - loss: 0.0109 - acc: 0.9966 - precision: 0.9972 - recall: 0.99 - ETA: 4s - loss: 0.0108 - acc: 0.9967 - precision: 0.9973 - recall: 0.99 - ETA: 4s - loss: 0.0105 - acc: 0.9968 - precision: 0.9974 - recall: 0.99 - ETA: 4s - loss: 0.0104 - acc: 0.9969 - precision: 0.9975 - recall: 0.99 - ETA: 4s - loss: 0.0102 - acc: 0.9970 - precision: 0.9975 - recall: 0.99 - ETA: 4s - loss: 0.0099 - acc: 0.9970 - precision: 0.9976 - recall: 0.99 - ETA: 3s - loss: 0.0097 - acc: 0.9971 - precision: 0.9976 - recall: 0.99 - ETA: 3s - loss: 0.0095 - acc: 0.9972 - precision: 0.9977 - recall: 0.99 - ETA: 3s - loss: 0.0093 - acc: 0.9973 - precision: 0.9978 - recall: 0.99 - ETA: 3s - loss: 0.0095 - acc: 0.9970 - precision: 0.9974 - recall: 0.99 - ETA: 3s - loss: 0.0094 - acc: 0.9970 - precision: 0.9974 - recall: 0.99 - ETA: 3s - loss: 0.0093 - acc: 0.9971 - precision: 0.9975 - recall: 0.99 - ETA: 3s - loss: 0.0091 - acc: 0.9972 - precision: 0.9975 - recall: 0.99 - ETA: 2s - loss: 0.0089 - acc: 0.9972 - precision: 0.9976 - recall: 0.99 - ETA: 2s - loss: 0.0088 - acc: 0.9973 - precision: 0.9977 - recall: 0.99 - ETA: 2s - loss: 0.0087 - acc: 0.9973 - precision: 0.9977 - recall: 0.99 - ETA: 2s - loss: 0.0085 - acc: 0.9974 - precision: 0.9978 - recall: 0.99 - ETA: 2s - loss: 0.0084 - acc: 0.9974 - precision: 0.9978 - recall: 0.99 - ETA: 2s - loss: 0.0082 - acc: 0.9975 - precision: 0.9978 - recall: 0.99 - ETA: 2s - loss: 0.0081 - acc: 0.9975 - precision: 0.9979 - recall: 0.99 - ETA: 1s - loss: 0.0080 - acc: 0.9976 - precision: 0.9979 - recall: 0.99 - ETA: 1s - loss: 0.0078 - acc: 0.9976 - precision: 0.9980 - recall: 0.99 - ETA: 1s - loss: 0.0077 - acc: 0.9977 - precision: 0.9980 - recall: 0.99 - ETA: 1s - loss: 0.0076 - acc: 0.9977 - precision: 0.9980 - recall: 0.99 - ETA: 1s - loss: 0.0074 - acc: 0.9978 - precision: 0.9981 - recall: 0.99 - ETA: 1s - loss: 0.0076 - acc: 0.9975 - precision: 0.9978 - recall: 0.99 - ETA: 1s - loss: 0.0076 - acc: 0.9976 - precision: 0.9978 - recall: 0.99 - ETA: 0s - loss: 0.0075 - acc: 0.9976 - precision: 0.9979 - recall: 0.99 - ETA: 0s - loss: 0.0074 - acc: 0.9977 - precision: 0.9979 - recall: 0.99 - ETA: 0s - loss: 0.0075 - acc: 0.9977 - precision: 0.9979 - recall: 0.99 - ETA: 0s - loss: 0.0075 - acc: 0.9977 - precision: 0.9980 - recall: 0.99 - ETA: 0s - loss: 0.0075 - acc: 0.9978 - precision: 0.9980 - recall: 0.99 - ETA: 0s - loss: 0.0074 - acc: 0.9978 - precision: 0.9980 - recall: 0.99 - ETA: 0s - loss: 0.0081 - acc: 0.9976 - precision: 0.9981 - recall: 0.99 - 11s 3ms/step - loss: 0.0081 - acc: 0.9976 - precision: 0.9981 - recall: 0.9992 - val_loss: 0.0936 - val_acc: 0.9835 - val_precision: 0.9827 - val_recall: 0.9983\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.05557\n",
      "Epoch 21/100\n",
      "4180/4180 [==============================] - ETA: 11s - loss: 9.7243e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 6.7254e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0051 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000   - ETA: 9s - loss: 0.0041 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0053 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0045 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0040 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0036 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0037 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0047 - acc: 0.9984 - precision: 0.9982 - recall: 1.00 - ETA: 8s - loss: 0.0071 - acc: 0.9972 - precision: 0.9967 - recall: 1.00 - ETA: 8s - loss: 0.0089 - acc: 0.9961 - precision: 0.9970 - recall: 0.99 - ETA: 8s - loss: 0.0089 - acc: 0.9964 - precision: 0.9972 - recall: 0.99 - ETA: 7s - loss: 0.0083 - acc: 0.9967 - precision: 0.9974 - recall: 0.99 - ETA: 7s - loss: 0.0078 - acc: 0.9969 - precision: 0.9976 - recall: 0.99 - ETA: 7s - loss: 0.0074 - acc: 0.9971 - precision: 0.9977 - recall: 0.99 - ETA: 7s - loss: 0.0086 - acc: 0.9963 - precision: 0.9968 - recall: 0.99 - ETA: 7s - loss: 0.0082 - acc: 0.9965 - precision: 0.9970 - recall: 0.99 - ETA: 7s - loss: 0.0079 - acc: 0.9967 - precision: 0.9971 - recall: 0.99 - ETA: 7s - loss: 0.0075 - acc: 0.9969 - precision: 0.9973 - recall: 0.99 - ETA: 7s - loss: 0.0073 - acc: 0.9970 - precision: 0.9974 - recall: 0.99 - ETA: 7s - loss: 0.0070 - acc: 0.9972 - precision: 0.9975 - recall: 0.99 - ETA: 6s - loss: 0.0067 - acc: 0.9973 - precision: 0.9976 - recall: 0.99 - ETA: 6s - loss: 0.0065 - acc: 0.9974 - precision: 0.9977 - recall: 0.99 - ETA: 6s - loss: 0.0062 - acc: 0.9975 - precision: 0.9978 - recall: 0.99 - ETA: 6s - loss: 0.0061 - acc: 0.9976 - precision: 0.9979 - recall: 0.99 - ETA: 6s - loss: 0.0059 - acc: 0.9977 - precision: 0.9980 - recall: 0.99 - ETA: 6s - loss: 0.0057 - acc: 0.9978 - precision: 0.9981 - recall: 0.99 - ETA: 6s - loss: 0.0058 - acc: 0.9978 - precision: 0.9981 - recall: 0.99 - ETA: 6s - loss: 0.0057 - acc: 0.9979 - precision: 0.9982 - recall: 0.99 - ETA: 6s - loss: 0.0056 - acc: 0.9980 - precision: 0.9982 - recall: 0.99 - ETA: 5s - loss: 0.0057 - acc: 0.9980 - precision: 0.9983 - recall: 0.99 - ETA: 5s - loss: 0.0059 - acc: 0.9981 - precision: 0.9984 - recall: 0.99 - ETA: 5s - loss: 0.0058 - acc: 0.9982 - precision: 0.9984 - recall: 0.99 - ETA: 5s - loss: 0.0056 - acc: 0.9982 - precision: 0.9984 - recall: 0.99 - ETA: 5s - loss: 0.0055 - acc: 0.9983 - precision: 0.9985 - recall: 0.99 - ETA: 5s - loss: 0.0054 - acc: 0.9983 - precision: 0.9985 - recall: 0.99 - ETA: 5s - loss: 0.0060 - acc: 0.9979 - precision: 0.9981 - recall: 0.99 - ETA: 4s - loss: 0.0059 - acc: 0.9980 - precision: 0.9981 - recall: 0.99 - ETA: 4s - loss: 0.0058 - acc: 0.9980 - precision: 0.9982 - recall: 0.99 - ETA: 4s - loss: 0.0057 - acc: 0.9981 - precision: 0.9982 - recall: 0.99 - ETA: 4s - loss: 0.0056 - acc: 0.9981 - precision: 0.9983 - recall: 0.99 - ETA: 4s - loss: 0.0055 - acc: 0.9982 - precision: 0.9983 - recall: 0.99 - ETA: 4s - loss: 0.0054 - acc: 0.9982 - precision: 0.9984 - recall: 0.99 - ETA: 3s - loss: 0.0053 - acc: 0.9983 - precision: 0.9984 - recall: 0.99 - ETA: 3s - loss: 0.0052 - acc: 0.9983 - precision: 0.9984 - recall: 0.99 - ETA: 3s - loss: 0.0051 - acc: 0.9983 - precision: 0.9985 - recall: 0.99 - ETA: 3s - loss: 0.0050 - acc: 0.9984 - precision: 0.9985 - recall: 0.99 - ETA: 3s - loss: 0.0049 - acc: 0.9984 - precision: 0.9985 - recall: 0.99 - ETA: 2s - loss: 0.0048 - acc: 0.9984 - precision: 0.9986 - recall: 0.99 - ETA: 2s - loss: 0.0050 - acc: 0.9982 - precision: 0.9982 - recall: 0.99 - ETA: 2s - loss: 0.0049 - acc: 0.9982 - precision: 0.9983 - recall: 0.99 - ETA: 2s - loss: 0.0049 - acc: 0.9982 - precision: 0.9983 - recall: 0.99 - ETA: 2s - loss: 0.0048 - acc: 0.9983 - precision: 0.9983 - recall: 0.99 - ETA: 2s - loss: 0.0047 - acc: 0.9983 - precision: 0.9984 - recall: 0.99 - ETA: 1s - loss: 0.0046 - acc: 0.9983 - precision: 0.9984 - recall: 0.99 - ETA: 1s - loss: 0.0045 - acc: 0.9984 - precision: 0.9984 - recall: 0.99 - ETA: 1s - loss: 0.0046 - acc: 0.9984 - precision: 0.9984 - recall: 0.99 - ETA: 1s - loss: 0.0046 - acc: 0.9984 - precision: 0.9985 - recall: 0.99 - ETA: 1s - loss: 0.0045 - acc: 0.9984 - precision: 0.9985 - recall: 0.99 - ETA: 0s - loss: 0.0045 - acc: 0.9985 - precision: 0.9985 - recall: 0.99 - ETA: 0s - loss: 0.0045 - acc: 0.9985 - precision: 0.9985 - recall: 0.99 - ETA: 0s - loss: 0.0044 - acc: 0.9985 - precision: 0.9986 - recall: 0.99 - ETA: 0s - loss: 0.0043 - acc: 0.9985 - precision: 0.9986 - recall: 0.99 - ETA: 0s - loss: 0.0043 - acc: 0.9986 - precision: 0.9986 - recall: 0.99 - 13s 3ms/step - loss: 0.0043 - acc: 0.9986 - precision: 0.9986 - recall: 0.9997 - val_loss: 0.1231 - val_acc: 0.9828 - val_precision: 0.9867 - val_recall: 0.9933\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.05557\n",
      "Epoch 22/100\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 1.7433e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 1.6915e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 1.3848e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 5.1569e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 4.4152e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 4.5197e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0045 - acc: 0.9978 - precision: 0.9975 - recall: 1.0000   - ETA: 9s - loss: 0.0055 - acc: 0.9980 - precision: 0.9978 - recall: 1.00 - ETA: 9s - loss: 0.0056 - acc: 0.9983 - precision: 0.9981 - recall: 1.00 - ETA: 8s - loss: 0.0051 - acc: 0.9984 - precision: 0.9982 - recall: 1.00 - ETA: 8s - loss: 0.0048 - acc: 0.9986 - precision: 0.9984 - recall: 1.00 - ETA: 8s - loss: 0.0045 - acc: 0.9987 - precision: 0.9985 - recall: 1.00 - ETA: 8s - loss: 0.0042 - acc: 0.9988 - precision: 0.9986 - recall: 1.00 - ETA: 8s - loss: 0.0041 - acc: 0.9989 - precision: 0.9987 - recall: 1.00 - ETA: 7s - loss: 0.0038 - acc: 0.9990 - precision: 0.9988 - recall: 1.00 - ETA: 7s - loss: 0.0039 - acc: 0.9990 - precision: 0.9989 - recall: 1.00 - ETA: 7s - loss: 0.0117 - acc: 0.9982 - precision: 0.9979 - recall: 1.00 - ETA: 7s - loss: 0.0111 - acc: 0.9983 - precision: 0.9980 - recall: 1.00 - ETA: 7s - loss: 0.0106 - acc: 0.9984 - precision: 0.9981 - recall: 1.00 - ETA: 7s - loss: 0.0101 - acc: 0.9984 - precision: 0.9982 - recall: 1.00 - ETA: 7s - loss: 0.0096 - acc: 0.9985 - precision: 0.9983 - recall: 1.00 - ETA: 6s - loss: 0.0092 - acc: 0.9986 - precision: 0.9984 - recall: 1.00 - ETA: 6s - loss: 0.0089 - acc: 0.9986 - precision: 0.9985 - recall: 1.00 - ETA: 6s - loss: 0.0085 - acc: 0.9987 - precision: 0.9985 - recall: 1.00 - ETA: 6s - loss: 0.0085 - acc: 0.9988 - precision: 0.9986 - recall: 1.00 - ETA: 6s - loss: 0.0082 - acc: 0.9988 - precision: 0.9986 - recall: 1.00 - ETA: 5s - loss: 0.0085 - acc: 0.9983 - precision: 0.9987 - recall: 0.99 - ETA: 5s - loss: 0.0082 - acc: 0.9983 - precision: 0.9987 - recall: 0.99 - ETA: 5s - loss: 0.0080 - acc: 0.9984 - precision: 0.9988 - recall: 0.99 - ETA: 5s - loss: 0.0077 - acc: 0.9984 - precision: 0.9988 - recall: 0.99 - ETA: 5s - loss: 0.0075 - acc: 0.9985 - precision: 0.9988 - recall: 0.99 - ETA: 5s - loss: 0.0072 - acc: 0.9985 - precision: 0.9989 - recall: 0.99 - ETA: 4s - loss: 0.0071 - acc: 0.9986 - precision: 0.9989 - recall: 0.99 - ETA: 4s - loss: 0.0069 - acc: 0.9986 - precision: 0.9989 - recall: 0.99 - ETA: 4s - loss: 0.0067 - acc: 0.9987 - precision: 0.9990 - recall: 0.99 - ETA: 4s - loss: 0.0066 - acc: 0.9987 - precision: 0.9990 - recall: 0.99 - ETA: 4s - loss: 0.0064 - acc: 0.9987 - precision: 0.9990 - recall: 0.99 - ETA: 4s - loss: 0.0064 - acc: 0.9988 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0063 - acc: 0.9988 - precision: 0.9991 - recall: 0.99 - ETA: 3s - loss: 0.0063 - acc: 0.9988 - precision: 0.9991 - recall: 0.99 - ETA: 3s - loss: 0.0063 - acc: 0.9989 - precision: 0.9991 - recall: 0.99 - ETA: 3s - loss: 0.0062 - acc: 0.9989 - precision: 0.9991 - recall: 0.99 - ETA: 3s - loss: 0.0061 - acc: 0.9989 - precision: 0.9992 - recall: 0.99 - ETA: 3s - loss: 0.0059 - acc: 0.9989 - precision: 0.9992 - recall: 0.99 - ETA: 3s - loss: 0.0058 - acc: 0.9990 - precision: 0.9992 - recall: 0.99 - ETA: 3s - loss: 0.0057 - acc: 0.9990 - precision: 0.9992 - recall: 0.99 - ETA: 2s - loss: 0.0056 - acc: 0.9990 - precision: 0.9992 - recall: 0.99 - ETA: 2s - loss: 0.0072 - acc: 0.9987 - precision: 0.9989 - recall: 0.99 - ETA: 2s - loss: 0.0076 - acc: 0.9984 - precision: 0.9989 - recall: 0.99 - ETA: 2s - loss: 0.0076 - acc: 0.9984 - precision: 0.9989 - recall: 0.99 - ETA: 2s - loss: 0.0075 - acc: 0.9985 - precision: 0.9989 - recall: 0.99 - ETA: 2s - loss: 0.0074 - acc: 0.9985 - precision: 0.9990 - recall: 0.99 - ETA: 1s - loss: 0.0073 - acc: 0.9985 - precision: 0.9990 - recall: 0.99 - ETA: 1s - loss: 0.0071 - acc: 0.9986 - precision: 0.9990 - recall: 0.99 - ETA: 1s - loss: 0.0070 - acc: 0.9986 - precision: 0.9990 - recall: 0.99 - ETA: 1s - loss: 0.0069 - acc: 0.9986 - precision: 0.9990 - recall: 0.99 - ETA: 1s - loss: 0.0068 - acc: 0.9986 - precision: 0.9991 - recall: 0.99 - ETA: 1s - loss: 0.0067 - acc: 0.9987 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0067 - acc: 0.9987 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0066 - acc: 0.9987 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0067 - acc: 0.9987 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0066 - acc: 0.9987 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0073 - acc: 0.9985 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0073 - acc: 0.9985 - precision: 0.9992 - recall: 0.99 - ETA: 0s - loss: 0.0072 - acc: 0.9986 - precision: 0.9992 - recall: 0.99 - 11s 3ms/step - loss: 0.0072 - acc: 0.9986 - precision: 0.9992 - recall: 0.9992 - val_loss: 0.0763 - val_acc: 0.9878 - val_precision: 0.9892 - val_recall: 0.9967\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.05557\n",
      "Epoch 23/100\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 0.0015 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0026 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0027 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0023 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0031 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0034 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0029 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0026 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0029 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0028 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0026 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0025 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0026 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0029 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0027 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0026 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0025 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 0.0024 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 0.0023 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 0.0023 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 0.0022 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 0.0022 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 0.0022 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 0.0021 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 0.0020 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 5s - loss: 0.0020 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 5s - loss: 0.0025 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 5s - loss: 0.0056 - acc: 0.9989 - precision: 0.9994 - recall: 0.99 - ETA: 5s - loss: 0.0054 - acc: 0.9989 - precision: 0.9994 - recall: 0.99 - ETA: 5s - loss: 0.0053 - acc: 0.9990 - precision: 0.9994 - recall: 0.99 - ETA: 5s - loss: 0.0052 - acc: 0.9990 - precision: 0.9994 - recall: 0.99 - ETA: 5s - loss: 0.0052 - acc: 0.9990 - precision: 0.9994 - recall: 0.99 - ETA: 4s - loss: 0.0053 - acc: 0.9991 - precision: 0.9995 - recall: 0.99 - ETA: 4s - loss: 0.0052 - acc: 0.9991 - precision: 0.9995 - recall: 0.99 - ETA: 4s - loss: 0.0051 - acc: 0.9991 - precision: 0.9995 - recall: 0.99 - ETA: 4s - loss: 0.0050 - acc: 0.9991 - precision: 0.9995 - recall: 0.99 - ETA: 4s - loss: 0.0048 - acc: 0.9992 - precision: 0.9995 - recall: 0.99 - ETA: 4s - loss: 0.0047 - acc: 0.9992 - precision: 0.9995 - recall: 0.99 - ETA: 3s - loss: 0.0046 - acc: 0.9992 - precision: 0.9995 - recall: 0.99 - ETA: 3s - loss: 0.0047 - acc: 0.9992 - precision: 0.9996 - recall: 0.99 - ETA: 3s - loss: 0.0053 - acc: 0.9985 - precision: 0.9987 - recall: 0.99 - ETA: 3s - loss: 0.0053 - acc: 0.9985 - precision: 0.9987 - recall: 0.99 - ETA: 3s - loss: 0.0053 - acc: 0.9985 - precision: 0.9987 - recall: 0.99 - ETA: 3s - loss: 0.0052 - acc: 0.9986 - precision: 0.9988 - recall: 0.99 - ETA: 3s - loss: 0.0051 - acc: 0.9986 - precision: 0.9988 - recall: 0.99 - ETA: 2s - loss: 0.0050 - acc: 0.9986 - precision: 0.9988 - recall: 0.99 - ETA: 2s - loss: 0.0049 - acc: 0.9987 - precision: 0.9989 - recall: 0.99 - ETA: 2s - loss: 0.0049 - acc: 0.9987 - precision: 0.9989 - recall: 0.99 - ETA: 2s - loss: 0.0048 - acc: 0.9987 - precision: 0.9989 - recall: 0.99 - ETA: 2s - loss: 0.0047 - acc: 0.9988 - precision: 0.9989 - recall: 0.99 - ETA: 2s - loss: 0.0049 - acc: 0.9985 - precision: 0.9986 - recall: 0.99 - ETA: 1s - loss: 0.0051 - acc: 0.9982 - precision: 0.9986 - recall: 0.99 - ETA: 1s - loss: 0.0050 - acc: 0.9982 - precision: 0.9986 - recall: 0.99 - ETA: 1s - loss: 0.0051 - acc: 0.9980 - precision: 0.9983 - recall: 0.99 - ETA: 1s - loss: 0.0050 - acc: 0.9980 - precision: 0.9984 - recall: 0.99 - ETA: 1s - loss: 0.0049 - acc: 0.9980 - precision: 0.9984 - recall: 0.99 - ETA: 1s - loss: 0.0049 - acc: 0.9981 - precision: 0.9984 - recall: 0.99 - ETA: 1s - loss: 0.0048 - acc: 0.9981 - precision: 0.9984 - recall: 0.99 - ETA: 0s - loss: 0.0047 - acc: 0.9981 - precision: 0.9985 - recall: 0.99 - ETA: 0s - loss: 0.0046 - acc: 0.9982 - precision: 0.9985 - recall: 0.99 - ETA: 0s - loss: 0.0047 - acc: 0.9982 - precision: 0.9985 - recall: 0.99 - ETA: 0s - loss: 0.0046 - acc: 0.9982 - precision: 0.9986 - recall: 0.99 - ETA: 0s - loss: 0.0045 - acc: 0.9983 - precision: 0.9986 - recall: 0.99 - ETA: 0s - loss: 0.0044 - acc: 0.9983 - precision: 0.9986 - recall: 0.99 - ETA: 0s - loss: 0.0044 - acc: 0.9983 - precision: 0.9986 - recall: 0.99 - 11s 3ms/step - loss: 0.0044 - acc: 0.9983 - precision: 0.9986 - recall: 0.9994 - val_loss: 0.0870 - val_acc: 0.9864 - val_precision: 0.9908 - val_recall: 0.9933\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.05557\n",
      "Epoch 24/100\n",
      "4180/4180 [==============================] - ETA: 10s - loss: 0.0013 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0022 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0018 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0035 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0033 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0028 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0026 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0023 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0024 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 0.0022 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0032 - acc: 0.9986 - precision: 0.9984 - recall: 1.00 - ETA: 9s - loss: 0.0032 - acc: 0.9987 - precision: 0.9985 - recall: 1.00 - ETA: 9s - loss: 0.0030 - acc: 0.9988 - precision: 0.9986 - recall: 1.00 - ETA: 8s - loss: 0.0028 - acc: 0.9989 - precision: 0.9987 - recall: 1.00 - ETA: 8s - loss: 0.0028 - acc: 0.9990 - precision: 0.9988 - recall: 1.00 - ETA: 8s - loss: 0.0043 - acc: 0.9980 - precision: 0.9989 - recall: 0.99 - ETA: 8s - loss: 0.0040 - acc: 0.9982 - precision: 0.9989 - recall: 0.99 - ETA: 8s - loss: 0.0039 - acc: 0.9983 - precision: 0.9990 - recall: 0.99 - ETA: 8s - loss: 0.0037 - acc: 0.9984 - precision: 0.9990 - recall: 0.99 - ETA: 7s - loss: 0.0035 - acc: 0.9984 - precision: 0.9991 - recall: 0.99 - ETA: 7s - loss: 0.0060 - acc: 0.9978 - precision: 0.9983 - recall: 0.99 - ETA: 7s - loss: 0.0079 - acc: 0.9972 - precision: 0.9984 - recall: 0.99 - ETA: 7s - loss: 0.0077 - acc: 0.9973 - precision: 0.9984 - recall: 0.99 - ETA: 7s - loss: 0.0075 - acc: 0.9974 - precision: 0.9985 - recall: 0.99 - ETA: 7s - loss: 0.0076 - acc: 0.9975 - precision: 0.9986 - recall: 0.99 - ETA: 6s - loss: 0.0074 - acc: 0.9976 - precision: 0.9986 - recall: 0.99 - ETA: 6s - loss: 0.0072 - acc: 0.9977 - precision: 0.9987 - recall: 0.99 - ETA: 6s - loss: 0.0070 - acc: 0.9978 - precision: 0.9987 - recall: 0.99 - ETA: 6s - loss: 0.0069 - acc: 0.9978 - precision: 0.9987 - recall: 0.99 - ETA: 6s - loss: 0.0068 - acc: 0.9979 - precision: 0.9988 - recall: 0.99 - ETA: 6s - loss: 0.0066 - acc: 0.9980 - precision: 0.9988 - recall: 0.99 - ETA: 5s - loss: 0.0065 - acc: 0.9980 - precision: 0.9989 - recall: 0.99 - ETA: 5s - loss: 0.0063 - acc: 0.9981 - precision: 0.9989 - recall: 0.99 - ETA: 5s - loss: 0.0062 - acc: 0.9982 - precision: 0.9989 - recall: 0.99 - ETA: 5s - loss: 0.0061 - acc: 0.9982 - precision: 0.9990 - recall: 0.99 - ETA: 5s - loss: 0.0059 - acc: 0.9983 - precision: 0.9990 - recall: 0.99 - ETA: 4s - loss: 0.0058 - acc: 0.9983 - precision: 0.9990 - recall: 0.99 - ETA: 4s - loss: 0.0057 - acc: 0.9984 - precision: 0.9990 - recall: 0.99 - ETA: 4s - loss: 0.0055 - acc: 0.9984 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0054 - acc: 0.9984 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0053 - acc: 0.9985 - precision: 0.9991 - recall: 0.99 - ETA: 4s - loss: 0.0052 - acc: 0.9985 - precision: 0.9991 - recall: 0.99 - ETA: 3s - loss: 0.0051 - acc: 0.9985 - precision: 0.9992 - recall: 0.99 - ETA: 3s - loss: 0.0050 - acc: 0.9986 - precision: 0.9992 - recall: 0.99 - ETA: 3s - loss: 0.0049 - acc: 0.9986 - precision: 0.9992 - recall: 0.99 - ETA: 3s - loss: 0.0048 - acc: 0.9986 - precision: 0.9992 - recall: 0.99 - ETA: 3s - loss: 0.0047 - acc: 0.9987 - precision: 0.9992 - recall: 0.99 - ETA: 2s - loss: 0.0046 - acc: 0.9987 - precision: 0.9992 - recall: 0.99 - ETA: 2s - loss: 0.0047 - acc: 0.9987 - precision: 0.9993 - recall: 0.99 - ETA: 2s - loss: 0.0047 - acc: 0.9988 - precision: 0.9993 - recall: 0.99 - ETA: 2s - loss: 0.0046 - acc: 0.9988 - precision: 0.9993 - recall: 0.99 - ETA: 2s - loss: 0.0045 - acc: 0.9988 - precision: 0.9993 - recall: 0.99 - ETA: 2s - loss: 0.0045 - acc: 0.9988 - precision: 0.9993 - recall: 0.99 - ETA: 1s - loss: 0.0044 - acc: 0.9988 - precision: 0.9993 - recall: 0.99 - ETA: 1s - loss: 0.0043 - acc: 0.9989 - precision: 0.9993 - recall: 0.99 - ETA: 1s - loss: 0.0042 - acc: 0.9989 - precision: 0.9994 - recall: 0.99 - ETA: 1s - loss: 0.0042 - acc: 0.9989 - precision: 0.9994 - recall: 0.99 - ETA: 1s - loss: 0.0042 - acc: 0.9989 - precision: 0.9994 - recall: 0.99 - ETA: 1s - loss: 0.0041 - acc: 0.9989 - precision: 0.9994 - recall: 0.99 - ETA: 0s - loss: 0.0047 - acc: 0.9987 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0048 - acc: 0.9987 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0047 - acc: 0.9987 - precision: 0.9991 - recall: 0.99 - ETA: 0s - loss: 0.0051 - acc: 0.9985 - precision: 0.9989 - recall: 0.99 - ETA: 0s - loss: 0.0050 - acc: 0.9985 - precision: 0.9989 - recall: 0.99 - ETA: 0s - loss: 0.0050 - acc: 0.9986 - precision: 0.9989 - recall: 0.99 - 12s 3ms/step - loss: 0.0050 - acc: 0.9986 - precision: 0.9989 - recall: 0.9994 - val_loss: 0.0903 - val_acc: 0.9857 - val_precision: 0.9917 - val_recall: 0.9917\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.05557\n",
      "Epoch 25/100\n",
      "3712/4180 [=========================>....] - ETA: 12s - loss: 0.0011 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0015 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 10s - loss: 0.0011 - acc: 1.0000 - precision: 1.0000 - recall: 1.000 - ETA: 9s - loss: 8.6215e-04 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 9s - loss: 0.0020 - acc: 1.0000 - precision: 1.0000 - recall: 1.0000   - ETA: 9s - loss: 0.0017 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0015 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0014 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0014 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0013 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 8s - loss: 0.0013 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0012 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0018 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0019 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0018 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0018 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 7s - loss: 0.0017 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 0.0016 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 0.0021 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 0.0020 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 0.0019 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 0.0019 - acc: 1.0000 - precision: 1.0000 - recall: 1.00 - ETA: 6s - loss: 0.0025 - acc: 0.9993 - precision: 0.9992 - recall: 1.00 - ETA: 5s - loss: 0.0028 - acc: 0.9993 - precision: 0.9992 - recall: 1.00 - ETA: 5s - loss: 0.0027 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 5s - loss: 0.0028 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 5s - loss: 0.0027 - acc: 0.9994 - precision: 0.9993 - recall: 1.00 - ETA: 5s - loss: 0.0027 - acc: 0.9994 - precision: 0.9994 - recall: 1.00 - ETA: 5s - loss: 0.0026 - acc: 0.9995 - precision: 0.9994 - recall: 1.00 - ETA: 5s - loss: 0.0025 - acc: 0.9995 - precision: 0.9994 - recall: 1.00 - ETA: 4s - loss: 0.0025 - acc: 0.9995 - precision: 0.9994 - recall: 1.00 - ETA: 4s - loss: 0.0024 - acc: 0.9995 - precision: 0.9994 - recall: 1.00 - ETA: 4s - loss: 0.0024 - acc: 0.9995 - precision: 0.9995 - recall: 1.00 - ETA: 4s - loss: 0.0023 - acc: 0.9995 - precision: 0.9995 - recall: 1.00 - ETA: 4s - loss: 0.0023 - acc: 0.9996 - precision: 0.9995 - recall: 1.00 - ETA: 4s - loss: 0.0022 - acc: 0.9996 - precision: 0.9995 - recall: 1.00 - ETA: 4s - loss: 0.0022 - acc: 0.9996 - precision: 0.9995 - recall: 1.00 - ETA: 3s - loss: 0.0022 - acc: 0.9996 - precision: 0.9995 - recall: 1.00 - ETA: 3s - loss: 0.0021 - acc: 0.9996 - precision: 0.9995 - recall: 1.00 - ETA: 3s - loss: 0.0021 - acc: 0.9996 - precision: 0.9996 - recall: 1.00 - ETA: 3s - loss: 0.0020 - acc: 0.9996 - precision: 0.9996 - recall: 1.00 - ETA: 3s - loss: 0.0020 - acc: 0.9996 - precision: 0.9996 - recall: 1.00 - ETA: 3s - loss: 0.0019 - acc: 0.9996 - precision: 0.9996 - recall: 1.00 - ETA: 3s - loss: 0.0019 - acc: 0.9996 - precision: 0.9996 - recall: 1.00 - ETA: 2s - loss: 0.0019 - acc: 0.9997 - precision: 0.9996 - recall: 1.00 - ETA: 2s - loss: 0.0018 - acc: 0.9997 - precision: 0.9996 - recall: 1.00 - ETA: 2s - loss: 0.0018 - acc: 0.9997 - precision: 0.9996 - recall: 1.00 - ETA: 2s - loss: 0.0024 - acc: 0.9993 - precision: 0.9993 - recall: 1.00 - ETA: 2s - loss: 0.0027 - acc: 0.9990 - precision: 0.9993 - recall: 0.99 - ETA: 2s - loss: 0.0027 - acc: 0.9991 - precision: 0.9993 - recall: 0.99 - ETA: 2s - loss: 0.0027 - acc: 0.9991 - precision: 0.9993 - recall: 0.99 - ETA: 1s - loss: 0.0027 - acc: 0.9991 - precision: 0.9993 - recall: 0.99 - ETA: 1s - loss: 0.0026 - acc: 0.9991 - precision: 0.9993 - recall: 0.99 - ETA: 1s - loss: 0.0027 - acc: 0.9991 - precision: 0.9993 - recall: 0.99 - ETA: 1s - loss: 0.0026 - acc: 0.9991 - precision: 0.9993 - recall: 0.99 - ETA: 1s - loss: 0.0026 - acc: 0.9992 - precision: 0.9994 - recall: 0.99 - ETA: 1s - loss: 0.0026 - acc: 0.9992 - precision: 0.9994 - recall: 0.99 - ETA: 1s - loss: 0.0026 - acc: 0.9992 - precision: 0.9994 - recall: 0.9997"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-fb5b3258c9c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m           verbose=1)\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2977\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize our ModelCheckpoint and TensorBoard callbacks\n",
    "# model checkpoint for saving best weights\n",
    "model_checkpoint = ModelCheckpoint(\"results/spam_classifier_{val_loss:.2f}\", save_best_only=True,\n",
    "                                    verbose=1)\n",
    "# for better visualization\n",
    "tensorboard = TensorBoard(f\"logs/spam_classifier_{time.time()}\")\n",
    "# print our data shapes\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)\n",
    "# train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "          batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "          callbacks=[tensorboard, model_checkpoint],\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1394/1394 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 817us/step\n",
      "[+] Accuracy: 98.06%\n",
      "[+] Precision:   99.41%\n",
      "[+] Recall:   98.33%\n",
      "[+] F-1: 98.87%\n"
     ]
    }
   ],
   "source": [
    "# get the loss and metrics\n",
    "result = model.evaluate(X_test, y_test)\n",
    "# extract those\n",
    "loss = result[0]\n",
    "accuracy = result[1]\n",
    "precision = result[2]\n",
    "recall = result[3]\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f\"[+] Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"[+] Precision:   {precision*100:.2f}%\")\n",
    "print(f\"[+] Recall:   {recall*100:.2f}%\")\n",
    "print(f\"[+] F-1: {f1*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    # pad the sequence\n",
    "    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
    "    # get the prediction\n",
    "    prediction = model.predict(sequence)[0]\n",
    "    # one-hot encoded vector, revert using np.argmax\n",
    "    return int2label[np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "text = \"Congratulations! you have won 100,000$ this week, click here to claim fast\"\n",
    "print(get_predictions(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi man, I was wondering if we can meet tomorrow.\"\n",
    "print(get_predictions(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
