{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "# Show first 5 in dataset\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop soem unwanted columns that do not provide insight\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":\"label\", \"v2\":\"text\"})\n",
    "# Show last 5 in dataset\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  label_tag\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1   ham                      Ok lar... Joking wif u oni...          0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1\n",
       "3   ham  U dun say so early hor... U c already then say...          0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label_tag\"] = data.label.map({'ham':0, 'spam':1})\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label        5572\n",
      "text         5572\n",
      "label_tag    5572\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the size of our dataset\n",
    "print(data.count())\n",
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  label_tag\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1   ham                      Ok lar... Joking wif u oni...          0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1\n",
       "3   ham  U dun say so early hor... U c already then say...          0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 4572/5572 emails\n",
    "training_data = data[0:4572]\n",
    "training_data_length = len(training_data.label)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4572</th>\n",
       "      <td>spam</td>\n",
       "      <td>\\URGENT! This is the 2nd attempt to contact U!...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4573</th>\n",
       "      <td>ham</td>\n",
       "      <td>:( but your not here....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4574</th>\n",
       "      <td>ham</td>\n",
       "      <td>Not directly behind... Abt 4 rows behind Ì_...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4575</th>\n",
       "      <td>spam</td>\n",
       "      <td>Congratulations ur awarded 500 of CD vouchers ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4576</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your contract mobile 11 Mnths? Latest Moto...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text  label_tag\n",
       "4572  spam  \\URGENT! This is the 2nd attempt to contact U!...          1\n",
       "4573   ham                           :( but your not here....          0\n",
       "4574   ham     Not directly behind... Abt 4 rows behind Ì_...          0\n",
       "4575  spam  Congratulations ur awarded 500 of CD vouchers ...          1\n",
       "4576  spam  Had your contract mobile 11 Mnths? Latest Moto...          1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last 1000/5572\n",
    "test_data = data[-1000:]\n",
    "test_data_length = len(test_data.label)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the shape of our input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4572, 3)\n",
      "(4572,)\n"
     ]
    }
   ],
   "source": [
    "print(training_data.shape)\n",
    "print(training_data.label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 features and 4572 samples in our trtaining set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape)\n",
    "print(test_data.label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop a Predictive Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_text_and_label(i):\n",
    "    print(training_data.label[i] + \"\\t:\\t\" + training_data.text[i][:80] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels \t : \t texts\n",
      "\n",
      "ham\t:\tRight on brah, see you later...\n",
      "ham\t:\tAs if i wasn't having enough trouble sleeping....\n",
      "spam\t:\tLast Chance! Claim ur å£150 worth of discount vouchers today! Text SHOP to 85023...\n",
      "ham\t:\tSo wat's da decision?...\n",
      "ham\t:\tMind blastin.. No more Tsunamis will occur from now on.. Rajnikant stopped swimm...\n",
      "ham\t:\tRemember to ask alex about his pizza...\n",
      "ham\t:\tHi! This is Roger from CL. How are you?...\n",
      "ham\t:\tNo..but heard abt tat.....\n"
     ]
    }
   ],
   "source": [
    "print(\"labels \\t : \\t texts\\n\")\n",
    "# choose  a random spam set to analyse\n",
    "# random.randrange(start, stop, step)\n",
    "pretty_print_text_and_label(random.randrange(0,4572))\n",
    "pretty_print_text_and_label(random.randrange(0,4572,4))\n",
    "pretty_print_text_and_label(random.randrange(0,4572,50))\n",
    "pretty_print_text_and_label(random.randrange(0,4572,100))\n",
    "pretty_print_text_and_label(random.randrange(0,4572,200))\n",
    "pretty_print_text_and_label(random.randrange(0,4572,500))\n",
    "pretty_print_text_and_label(random.randrange(0,4572,800))\n",
    "pretty_print_text_and_label(random.randrange(0,4572,1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very easy to distinguish a spam text from a non-spam text (in this case ham) . Spam text occasionaly contain words like **free**, **sell**, **promotion**, **deal**, **offer**, **discount**, **lucky** e.t.c. This way we can let our network learn some of the words assocaiated with spams and based on such criteria we can classify a text as a spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theory Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pprint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_counts = Counter()\n",
    "ham_counts = Counter()\n",
    "total_counts = Counter()\n",
    "spam_ham_ratios = Counter()\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(training_data_length):\n",
    "    if(training_data.label[i] == 0):\n",
    "        for word in training_data.text[i].split(\" \"):\n",
    "            ham_counts[word] += 1\n",
    "            total_counts[word] += 1\n",
    "    else:\n",
    "        for word in training_data.text[i].split(\" \"):\n",
    "            spam_counts[word] += 1\n",
    "            total_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('to', 1758),\n",
      "    ('you', 1368),\n",
      "    ('I', 1204),\n",
      "    ('a', 1094),\n",
      "    ('the', 989),\n",
      "    ('and', 736),\n",
      "    ('in', 652),\n",
      "    ('is', 648),\n",
      "    ('i', 612),\n",
      "    ('u', 567),\n",
      "    ('for', 529),\n",
      "    ('my', 522),\n",
      "    ('', 521),\n",
      "    ('of', 498),\n",
      "    ('me', 465),\n",
      "    ('your', 447),\n",
      "    ('on', 410),\n",
      "    ('have', 402),\n",
      "    ('2', 371),\n",
      "    ('that', 358),\n",
      "    ('are', 327),\n",
      "    ('it', 313),\n",
      "    ('or', 304),\n",
      "    ('call', 303),\n",
      "    ('at', 300),\n",
      "    ('be', 299),\n",
      "    ('not', 292),\n",
      "    ('with', 281),\n",
      "    ('get', 270),\n",
      "    ('will', 266)]\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(spam_counts.most_common()[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,count in list(total_counts.most_common()):\n",
    "    if(count > 100):\n",
    "        spam_ham_ratio = spam_counts[word] / float(ham_counts[word]+1)\n",
    "        spam_ham_ratios[word] = spam_ham_ratio\n",
    "\n",
    "for word,ratio in spam_ham_ratios.most_common():\n",
    "    if(ratio > 1):\n",
    "        spam_ham_ratios[word] = np.log(ratio)\n",
    "    else:\n",
    "        spam_ham_ratios[word] = -np.log((1 / (ratio+0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('to', 7.471932078245122),\n",
      "    ('you', 7.221105098182496),\n",
      "    ('I', 7.093404625868766),\n",
      "    ('a', 6.9975959829819265),\n",
      "    ('the', 6.8966943316227125),\n",
      "    ('and', 6.601230118728877),\n",
      "    ('in', 6.480044561926653),\n",
      "    ('is', 6.473890696352274),\n",
      "    ('i', 6.416732282512326),\n",
      "    ('u', 6.340359303727752),\n",
      "    ('for', 6.270988431858299),\n",
      "    ('my', 6.257667587882639),\n",
      "    ('', 6.255750041753367),\n",
      "    ('of', 6.210600077024653),\n",
      "    ('me', 6.142037405587356),\n",
      "    ('your', 6.102558594613569),\n",
      "    ('on', 6.016157159698354),\n",
      "    ('have', 5.996452088619021),\n",
      "    ('2', 5.916202062607435),\n",
      "    ('that', 5.8805329864007),\n",
      "    ('are', 5.7899601708972535),\n",
      "    ('it', 5.746203190540153),\n",
      "    ('or', 5.717027701406222),\n",
      "    ('call', 5.713732805509369),\n",
      "    ('at', 5.703782474656201),\n",
      "    ('be', 5.700443573390687),\n",
      "    ('not', 5.676753802268282),\n",
      "    ('with', 5.638354669333745),\n",
      "    ('get', 5.598421958998375),\n",
      "    ('will', 5.583496308781699)]\n"
     ]
    }
   ],
   "source": [
    "# words most frequently seen in a text with a \"spam\" label\n",
    "pp.pprint(spam_ham_ratios.most_common()[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('he', 4.61512051684126),\n",
      "    ('there', 4.61512051684126),\n",
      "    ('no', 4.624972813284271),\n",
      "    ('one', 4.6443908991413725),\n",
      "    ('our', 4.6443908991413725),\n",
      "    ('been', 4.6443908991413725),\n",
      "    ('If', 4.663439094112067),\n",
      "    ('No', 4.672828834461906),\n",
      "    ('But', 4.700480365792417),\n",
      "    ('still', 4.709530201312334),\n",
      "    ('text', 4.718498871295094),\n",
      "    ('need', 4.736198448394496),\n",
      "    ('as', 4.74493212836325),\n",
      "    ('only', 4.74493212836325),\n",
      "    ('n', 4.762173934797756),\n",
      "    ('How', 4.770684624465665),\n",
      "    ('what', 4.770684624465665),\n",
      "    (\"I'll\", 4.770684624465665),\n",
      "    ('going', 4.787491742782046),\n",
      "    ('then', 4.787491742782046),\n",
      "    ('Call', 4.795790545596741),\n",
      "    ('...', 4.804021044733257),\n",
      "    ('time', 4.820281565605037),\n",
      "    ('want', 4.844187086458591),\n",
      "    ('about', 4.852030263919617),\n",
      "    ('send', 4.867534450455582),\n",
      "    ('by', 4.897839799950911),\n",
      "    ('was', 5.030437921392435),\n",
      "    ('?', 5.049856007249537),\n",
      "    ('now', 5.049856007249537)]\n"
     ]
    }
   ],
   "source": [
    "# words most frequently seen in a text with a \"ham\" label\n",
    "pp.pprint(list(reversed(spam_ham_ratios.most_common()))[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Text into Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks only understand numbers hence we have to find a way to represent our text inputs in a way it can understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13874\n"
     ]
    }
   ],
   "source": [
    "vocab = set(total_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that from all our dataset, we have a total of **13874** unique words. Use this to build up our vocabulary vector containing columns of all these words.\n",
    "\n",
    "Because, **13874**, can be a large size in memory (a matrix of size **13874 by 4572**), lets allocate its memory once with default zeros and will only change its contents accordingly later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13874)\n",
      "array([[0., 0., 0., ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "vocab_vector = np.zeros((1, vocab_size))\n",
    "pp.pprint(vocab_vector.shape)\n",
    "pp.pprint(vocab_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a dictionary that allows us to look at every word in our vocabulary and map it to the `vocab_vector` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Maps a word to its column in the vocab_vector\n",
    "word_column_dict = {}\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    # {key: value} is {word: column}\n",
    "    word_column_dict[word] = i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the count of words as the input to our neural network. The `vocab_vector` will have columns for all the words in our training data in the form of `{key: value}` i.e `{word: count}` as held by the `word_column_dict`  python `Dictionary`. The individual word counts in any particular text is updated from 0 to a number based on a word's total count in any single text.\n",
    "\n",
    "This means that the words with a higher count might have a higher weight in determining whether a text is a spam or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Nothing really, just making sure everybody's up to speed\"\n"
     ]
    }
   ],
   "source": [
    "def update_input_layer(text):\n",
    "    pp.pprint(text)\n",
    "    global vocab_vector\n",
    "    \n",
    "    # clear out previous state, reset the vector to be all 0s\n",
    "    vocab_vector *= 0\n",
    "    for word in text.split(\" \"):\n",
    "        vocab_vector[0][word_column_dict[word]] += 1\n",
    "\n",
    "update_input_layer(training_data[\"text\"][random.randrange(0,4572,4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the SpamClassificationNeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tweak our network from before to model these phenomena\n",
    "class SpamClassificationNeuralNetwork(object):\n",
    "    def __init__(self, training_data, num_hidden_nodes = 10, num_epochs = 10, learning_rate = 0.1):\n",
    "        # set our random number generator \n",
    "        np.random.seed(1)\n",
    "        # pre-process data\n",
    "        self.pre_process_data(training_data)\n",
    "        \n",
    "        self.num_features = len(self.vocab)\n",
    "        self.vocab_vector = np.zeros((1, len(self.vocab)))\n",
    "        self.num_input_nodes = self.num_features\n",
    "        self.num_hidden_nodes = num_hidden_nodes\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_output_nodes = 1\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_i_h = np.random.randn(self.num_input_nodes, self.num_hidden_nodes)\n",
    "        self.weights_h_o = np.random.randn(self.num_hidden_nodes, self.num_output_nodes)\n",
    "        \n",
    "    def forward_backward_propagate(self, text, label):\n",
    "        ### Forward pass ###\n",
    "        # Input Layer\n",
    "        self.update_input_layer(text)\n",
    "        # Hidden layer\n",
    "        hidden_layer = self.vocab_vector.dot(self.weights_i_h)\n",
    "        # Output layer\n",
    "        output_layer = self.sigmoid(hidden_layer.dot(self.weights_h_o))\n",
    "        \n",
    "        ### Backward pass ###\n",
    "        # Output error\n",
    "        output_layer_error = output_layer - label \n",
    "        output_layer_delta = output_layer_error * self.sigmoid_derivative(output_layer)\n",
    "\n",
    "        # Backpropagated error - to the hidden layer\n",
    "        hidden_layer_error = output_layer_delta.dot(self.weights_h_o.T)\n",
    "        # hidden layer gradients - no nonlinearity so it's the same as the error\n",
    "        hidden_layer_delta = output_layer_error \n",
    "\n",
    "        # update the weights - with grdient descent\n",
    "        self.weights_h_o -= hidden_layer.T.dot(output_layer_delta) * self.learning_rate \n",
    "        self.weights_i_h -= self.vocab_vector.T.dot(hidden_layer_delta) * self.learning_rate \n",
    "        \n",
    "        if(np.abs(output_layer_error) < 0.5):\n",
    "                self.correct_so_far += 1\n",
    "        \n",
    "        \n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def sigmoid_derivative(self,x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.correct_so_far = 0\n",
    "            start = time.time()\n",
    "\n",
    "            for i in range(len(training_data)):\n",
    "                # Forward and Back Propagation\n",
    "                self.forward_backward_propagate(training_data[\"text\"][i], training_data[\"label_tag\"][i])\n",
    "\n",
    "                samples_per_second = i / float(time.time() - start + 0.001)\n",
    "\n",
    "                sys.stdout.write(\"\\rEpoch: \"+ str(epoch)\n",
    "                                 +\" Progress: \" + str(100 * i/float(len(training_data)))[:4] \n",
    "                                 + \" % Speed(samples/sec): \" + str(samples_per_second)[0:5] \n",
    "                                 + \" #Correct: \" + str(self.correct_so_far) \n",
    "                                 + \" #Trained: \" + str(i+1) \n",
    "                                 + \" Training Accuracy: \" + str(self.correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            print(\"\")\n",
    "        \n",
    "    def pre_process_data(self, training_data):\n",
    "        vocab = set()\n",
    "        \n",
    "        for review in training_data[\"text\"]:\n",
    "            for word in review.split(\" \"):\n",
    "                vocab.add(word)\n",
    "                \n",
    "        self.vocab = list(vocab)\n",
    "        \n",
    "        self.word_to_column = {}\n",
    "        for i, word in enumerate(self.vocab):\n",
    "            self.word_to_column[word] = i\n",
    "            \n",
    "    def update_input_layer(self, text):\n",
    "        global vocab_vector\n",
    "\n",
    "        # clear out previous state, reset the vector to be all 0s\n",
    "        self.vocab_vector *= 0\n",
    "        for word in text.split(\" \"):\n",
    "            self.vocab_vector[0][word_column_dict[word]] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = SpamClassificationNeuralNetwork(training_data, num_epochs = 50, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Progress: 99.9 % Speed(samples/sec): 759.0 #Correct: 3200 #Trained: 4572 Training Accuracy: 69.9%\n",
      "Epoch: 1 Progress: 99.9 % Speed(samples/sec): 793.5 #Correct: 3813 #Trained: 4572 Training Accuracy: 83.3%\n",
      "Epoch: 2 Progress: 99.9 % Speed(samples/sec): 737.4 #Correct: 3999 #Trained: 4572 Training Accuracy: 87.4%\n",
      "Epoch: 3 Progress: 99.9 % Speed(samples/sec): 705.8 #Correct: 4086 #Trained: 4572 Training Accuracy: 89.3%\n",
      "Epoch: 4 Progress: 99.9 % Speed(samples/sec): 698.7 #Correct: 4179 #Trained: 4572 Training Accuracy: 91.4%\n",
      "Epoch: 5 Progress: 99.9 % Speed(samples/sec): 791.2 #Correct: 4237 #Trained: 4572 Training Accuracy: 92.6%\n",
      "Epoch: 6 Progress: 99.9 % Speed(samples/sec): 690.6 #Correct: 4287 #Trained: 4572 Training Accuracy: 93.7%\n",
      "Epoch: 7 Progress: 99.9 % Speed(samples/sec): 621.9 #Correct: 4329 #Trained: 4572 Training Accuracy: 94.6%\n",
      "Epoch: 8 Progress: 99.9 % Speed(samples/sec): 618.4 #Correct: 4355 #Trained: 4572 Training Accuracy: 95.2%\n",
      "Epoch: 9 Progress: 99.9 % Speed(samples/sec): 772.2 #Correct: 4387 #Trained: 4572 Training Accuracy: 95.9%\n",
      "Epoch: 10 Progress: 99.9 % Speed(samples/sec): 853.3 #Correct: 4406 #Trained: 4572 Training Accuracy: 96.3%\n",
      "Epoch: 11 Progress: 99.9 % Speed(samples/sec): 828.7 #Correct: 4423 #Trained: 4572 Training Accuracy: 96.7%\n",
      "Epoch: 12 Progress: 99.9 % Speed(samples/sec): 819.5 #Correct: 4443 #Trained: 4572 Training Accuracy: 97.1%\n",
      "Epoch: 13 Progress: 99.9 % Speed(samples/sec): 823.0 #Correct: 4459 #Trained: 4572 Training Accuracy: 97.5%\n",
      "Epoch: 14 Progress: 99.9 % Speed(samples/sec): 819.2 #Correct: 4472 #Trained: 4572 Training Accuracy: 97.8%\n",
      "Epoch: 15 Progress: 99.9 % Speed(samples/sec): 812.8 #Correct: 4482 #Trained: 4572 Training Accuracy: 98.0%\n",
      "Epoch: 16 Progress: 99.9 % Speed(samples/sec): 824.7 #Correct: 4492 #Trained: 4572 Training Accuracy: 98.2%\n",
      "Epoch: 17 Progress: 99.9 % Speed(samples/sec): 794.2 #Correct: 4502 #Trained: 4572 Training Accuracy: 98.4%\n",
      "Epoch: 18 Progress: 99.9 % Speed(samples/sec): 795.3 #Correct: 4509 #Trained: 4572 Training Accuracy: 98.6%\n",
      "Epoch: 19 Progress: 99.9 % Speed(samples/sec): 826.0 #Correct: 4517 #Trained: 4572 Training Accuracy: 98.7%\n",
      "Epoch: 20 Progress: 99.9 % Speed(samples/sec): 804.2 #Correct: 4525 #Trained: 4572 Training Accuracy: 98.9%\n",
      "Epoch: 21 Progress: 99.9 % Speed(samples/sec): 816.4 #Correct: 4532 #Trained: 4572 Training Accuracy: 99.1%\n",
      "Epoch: 22 Progress: 99.9 % Speed(samples/sec): 809.4 #Correct: 4538 #Trained: 4572 Training Accuracy: 99.2%\n",
      "Epoch: 23 Progress: 99.9 % Speed(samples/sec): 803.0 #Correct: 4539 #Trained: 4572 Training Accuracy: 99.2%\n",
      "Epoch: 24 Progress: 99.9 % Speed(samples/sec): 828.2 #Correct: 4541 #Trained: 4572 Training Accuracy: 99.3%\n",
      "Epoch: 25 Progress: 99.9 % Speed(samples/sec): 801.2 #Correct: 4544 #Trained: 4572 Training Accuracy: 99.3%\n",
      "Epoch: 26 Progress: 99.9 % Speed(samples/sec): 827.2 #Correct: 4546 #Trained: 4572 Training Accuracy: 99.4%\n",
      "Epoch: 27 Progress: 99.9 % Speed(samples/sec): 815.1 #Correct: 4552 #Trained: 4572 Training Accuracy: 99.5%\n",
      "Epoch: 28 Progress: 99.9 % Speed(samples/sec): 830.4 #Correct: 4554 #Trained: 4572 Training Accuracy: 99.6%\n",
      "Epoch: 29 Progress: 99.9 % Speed(samples/sec): 827.1 #Correct: 4558 #Trained: 4572 Training Accuracy: 99.6%\n",
      "Epoch: 30 Progress: 99.9 % Speed(samples/sec): 814.5 #Correct: 4559 #Trained: 4572 Training Accuracy: 99.7%\n",
      "Epoch: 31 Progress: 99.9 % Speed(samples/sec): 813.3 #Correct: 4559 #Trained: 4572 Training Accuracy: 99.7%\n",
      "Epoch: 32 Progress: 99.9 % Speed(samples/sec): 827.2 #Correct: 4560 #Trained: 4572 Training Accuracy: 99.7%\n",
      "Epoch: 33 Progress: 99.9 % Speed(samples/sec): 836.8 #Correct: 4561 #Trained: 4572 Training Accuracy: 99.7%\n",
      "Epoch: 34 Progress: 99.9 % Speed(samples/sec): 810.7 #Correct: 4561 #Trained: 4572 Training Accuracy: 99.7%\n",
      "Epoch: 35 Progress: 99.9 % Speed(samples/sec): 799.1 #Correct: 4562 #Trained: 4572 Training Accuracy: 99.7%\n",
      "Epoch: 36 Progress: 99.9 % Speed(samples/sec): 789.0 #Correct: 4564 #Trained: 4572 Training Accuracy: 99.8%\n",
      "Epoch: 37 Progress: 99.9 % Speed(samples/sec): 810.8 #Correct: 4564 #Trained: 4572 Training Accuracy: 99.8%\n",
      "Epoch: 38 Progress: 99.9 % Speed(samples/sec): 820.9 #Correct: 4564 #Trained: 4572 Training Accuracy: 99.8%\n",
      "Epoch: 39 Progress: 99.9 % Speed(samples/sec): 818.9 #Correct: 4565 #Trained: 4572 Training Accuracy: 99.8%\n",
      "Epoch: 40 Progress: 99.9 % Speed(samples/sec): 823.6 #Correct: 4565 #Trained: 4572 Training Accuracy: 99.8%\n",
      "Epoch: 41 Progress: 99.9 % Speed(samples/sec): 821.6 #Correct: 4566 #Trained: 4572 Training Accuracy: 99.8%\n",
      "Epoch: 42 Progress: 99.9 % Speed(samples/sec): 842.2 #Correct: 4567 #Trained: 4572 Training Accuracy: 99.8%\n",
      "Epoch: 43 Progress: 99.9 % Speed(samples/sec): 841.6 #Correct: 4568 #Trained: 4572 Training Accuracy: 99.9%\n",
      "Epoch: 44 Progress: 99.9 % Speed(samples/sec): 846.1 #Correct: 4569 #Trained: 4572 Training Accuracy: 99.9%\n",
      "Epoch: 45 Progress: 99.9 % Speed(samples/sec): 831.7 #Correct: 4571 #Trained: 4572 Training Accuracy: 99.9%\n",
      "Epoch: 46 Progress: 99.9 % Speed(samples/sec): 836.7 #Correct: 4571 #Trained: 4572 Training Accuracy: 99.9%\n",
      "Epoch: 47 Progress: 99.9 % Speed(samples/sec): 810.1 #Correct: 4571 #Trained: 4572 Training Accuracy: 99.9%\n",
      "Epoch: 48 Progress: 99.9 % Speed(samples/sec): 834.2 #Correct: 4571 #Trained: 4572 Training Accuracy: 99.9%\n",
      "Epoch: 49 Progress: 99.9 % Speed(samples/sec): 835.9 #Correct: 4571 #Trained: 4572 Training Accuracy: 99.9%\n"
     ]
    }
   ],
   "source": [
    "nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
